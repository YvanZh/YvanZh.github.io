<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YvanZh&#39;s Blog</title>
  
  <subtitle>Goketsu Monogatari</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yvanzh.top/"/>
  <updated>2020-01-01T07:07:31.797Z</updated>
  <id>http://yvanzh.top/</id>
  
  <author>
    <name>YvanZh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Unsupervised Domain Adaptation via Structured Prediction Based Selective Pseudo-Labeling</title>
    <link href="http://yvanzh.top/2019/12/09/Paper-Notes-8/"/>
    <id>http://yvanzh.top/2019/12/09/Paper-Notes-8/</id>
    <published>2019-12-09T05:32:39.000Z</published>
    <updated>2020-01-01T07:07:31.797Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://tvax2.sinaimg.cn/mw690/007v1rTBgy1g9qfpff2axj31hc0zj4qp.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><a href="https://dl.acm.org/citation.cfm?id=2623726" target="_blank" rel="noopener">文章来源</a>：2020-AAAI</p><p><a href="https://github.com/hellowangqian/domain-adaptation-capls" target="_blank" rel="noopener">文章代码</a></p><p>文章主旨：本文利用LPP保留原始空间的局部信息来学习投影子空间，每一次投影后采用NCP与SP结合的SPL方法对目标域样本计算软标签，并以软标签为基准在下一次迭代时选择性地添加目标域样本到特征矩阵中。通过指定次数的迭代，实现无监督的领域适应。</p><h1 id="2-详情"><a href="#2-详情" class="headerlink" title="2 详情"></a>2 详情</h1><p>论文的方法框架如图所示：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://tvax3.sinaimg.cn/mw690/007v1rTBgy1g9qfdf336rj30is0h440n.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></p><h2 id="2-1-Dimensionality-Reduction"><a href="#2-1-Dimensionality-Reduction" class="headerlink" title="2.1 Dimensionality Reduction"></a>2.1 Dimensionality Reduction</h2><p>利用PCA对原始特征矩阵$X\in \mathbb{R}^{m\times n}$进行降维。由于PCA的降维是线性的，所以对降维后的$\tilde{X}$每一列向量实行$L_2$归一化，即$\tilde{x} \leftarrow \tilde{x}/ \Vert \tilde{x} \Vert_2$。</p><h2 id="2-2-Domain-Alignment"><a href="#2-2-Domain-Alignment" class="headerlink" title="2.2 Domain Alignment"></a>2.2 Domain Alignment</h2><p>应用Supervised LPP学习一个保持原始空间局部结构的投影子空间：</p><script type="math/tex; mode=display">\min_{P} \sum_{i,j} \Vert P^T\tilde{x}_i -P^T\tilde{x}_j \Vert_2^2 M_{ij}\tag{1}</script><p>其中$M_{ij}$是根据标签和伪标签生成的相似矩阵。当$y_i=y_j$时，$M_{ij}$值为1；否则其值为0。</p><p>在原LPP中有一段：“度矩阵$D$提供了一个对数据点的天然测度，即$D_{ii}$的值越大，那么对应的点$P^T\tilde{x}_i$就越重要”。这段话需要这样来理解，我们知道$D_{ii} = \sum_{j}M_{ij}$。其值越大说明它与其他点的关联越密切，也就越重要。为了凸显这个重要性，LPP施加了一个对$P$的列向量$p$约束$p^T\tilde{X}D\tilde{X}^Tp = 1$，其转化为矩阵形式即为：</p><script type="math/tex; mode=display">P^T \tilde{X} D \tilde{X}^T P = I\tag{2}</script><p>然后考虑对投影矩阵$P$中的极大值添加正则化项：</p><script type="math/tex; mode=display">\Vert P \Vert_F^2\tag{3}</script><p>那么结合(1)(2)(3)式，我们得到：</p><script type="math/tex; mode=display">\min_{P} Tr(P^T\tilde{X}L\tilde{X}^TP) + Tr(P^TP) \quad s.t \, P^T\tilde{X}D\tilde{X}^TP = I\tag{4}</script><p>其中$L=D-M$。进一步，(4)式可化简为：</p><script type="math/tex; mode=display">\min_{P} \frac{Tr(P^T(\tilde{X}L\tilde{X}^T+I)P) }  {P^T\tilde{X}D\tilde{X}^TP}\tag{5}</script><p>通过上式(5)求解$P$需要运用广义特征值分解：$Ap=\lambda B p$，其中A为n阶实对称矩阵，B为n阶实对称正定矩阵，$p_0,\dotsb,p_{m-1}$为P的列向量。在这里与(5)式对应的广义特征值分解为：</p><script type="math/tex; mode=display">\tilde{X}D\tilde{X}^Tp=\lambda (\tilde{X}L\tilde{X}^T+I)p\tag{6}</script><p>最后把按照对应特征值从大到小的顺序将特征向量$p$排列成$P$，这里所选择维度也就是所学习的子空间的维度。</p><h2 id="2-3-Pseudo-Labeling"><a href="#2-3-Pseudo-Labeling" class="headerlink" title="2.3 Pseudo-Labeling"></a>2.3 Pseudo-Labeling</h2><p>为目标域的样本打上的伪标签一般采用软标签的形式，因为错误的标签有时会导致模型在优化时<br>被错误引导。</p><p>下文简单比较了NCP、SP和本文提出的SPL方法。</p><h3 id="2-3-1-Pseudo-Labeling-via-Nearest-Class-Prototype-NCP"><a href="#2-3-1-Pseudo-Labeling-via-Nearest-Class-Prototype-NCP" class="headerlink" title="2.3.1 Pseudo-Labeling via Nearest Class Prototype (NCP)"></a>2.3.1 Pseudo-Labeling via Nearest Class Prototype (NCP)</h3><p>经过投影后，源域和目标域样本在子空间中的表示为：$z^s =P^T\tilde{x}^s$，$z^t =P^T\tilde{x}^t$。</p><p>然后对数据进行中心化：$z\leftarrow z -\bar{z}$，其中$\bar{z}$表示均值。</p><p>接着再次利用$L_2$正则化来提高不同类之间的分离性：$z\leftarrow z/\Vert z \Vert_2$。</p><p>之后对源域的每个类$y \in \mathcal{Y}$，以求均值的方式求出其原型的位置：</p><script type="math/tex; mode=display">\bar{z}_{y}^s = \frac{\sum_{i=1}^{n_s}z_i^s\delta(y,y_i^s)}{\sum_{i=1}^{n_s}\delta(y,y_i^s)}\tag{7}</script><p>其中$\delta(y,y_i)=1$当且仅当$y=y_i$,否则为0。然后对类原型$\bar{z}_{y}^s$也实施一次$L_2$正则化。</p><p>最后通过计算高斯核函数作为条件概率，为目标域样本打上伪标签：</p><script type="math/tex; mode=display">P_1(y|x^t) = \frac{\exp(-\Vert z^t- \bar{z}_y^s\Vert)}{\sum_{y=1}^{\vert \mathcal{Y} \vert} \exp(-\Vert z^t - \bar{z}_y^s \Vert)}\tag{8}</script><p>其中$\vert \mathcal{Y} \vert$表示类的个数。显然这里的伪标签是以软标签的形式存在的。</p><h3 id="2-4-2-Pseudo-Labeling-via-Structured-Prediction-SP"><a href="#2-4-2-Pseudo-Labeling-via-Structured-Prediction-SP" class="headerlink" title="2.4.2 Pseudo-Labeling via Structured Prediction (SP)"></a>2.4.2 Pseudo-Labeling via Structured Prediction (SP)</h3><p>NCP只考虑了源域样本的信息（以源域样本中心为类的原型），而没有利用目标域样本的内蕴结构。于是SP方法考虑对投影后的目标域样本$z^t$使用K-means来分离出$\vert \mathcal{Y} \vert$个类，其中初始化的中心点由(7)式来计算得出。之后利用最终得到的中心点$\bar{z}^t$与投影后的源域样本的均值点$\bar{z}^s$依次进行最近距离匹配，从而确定每个中心点$\bar{z}^t_y$的类别。最后通过计算条件概率给目标域样本打上软标签：</p><script type="math/tex; mode=display">P_1(y|x^t) = \frac{\exp(-\Vert z^t- \bar{z}_y^t\Vert)}{\sum_{y=1}^{\vert \mathcal{Y} \vert} \exp(-\Vert z^t - \bar{z}_y^t \Vert)}\tag{9}</script><h3 id="2-4-3-Iterative-Learning-with-Selective-Pseudo-Labeling-SPL"><a href="#2-4-3-Iterative-Learning-with-Selective-Pseudo-Labeling-SPL" class="headerlink" title="2.4.3 Iterative Learning with Selective Pseudo-Labeling (SPL)"></a>2.4.3 Iterative Learning with Selective Pseudo-Labeling (SPL)</h3><p>可以看到NCP和SP分别以源域信息和目标域信息为基准来计算条件概率。于是SPL主张简单结合二者：</p><script type="math/tex; mode=display">p(y|x^t)=\max\{p_1(y|x^t),p_2(y|x^t)\}\tag{10}</script><p>最终，$x^t$的伪标签被预测为：</p><script type="math/tex; mode=display">\hat{y}^t = \arg \max_{y\in \mathcal{Y}} p(y|x^t)\tag{11}</script><p>此外，在学习投影矩阵的时候本文运用了一个技巧，即逐步添加目标域样本到特征矩阵$X$中。给定最大迭代次数$T$，第$k$次迭代时$X$中包含$kn_t/T$个目标域样本。为了避免某高概率类选择的样本太多，对每个伪标签$c\in \mathcal{Y}$样本按照最大概率排序选择前$kn^c_t/T$个。</p><p>算法流程如下图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://tvax4.sinaimg.cn/bmiddle/007v1rTBgy1g9svl0bprbj30ir0gr76c.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>该模型实验效果比较显著，甚至超过了TADA、MEDA、Symnet等深度模型。</p><p>此外，作者进行了Ablation Study，即对pseudo-labeling (PL)，sample selection (S) for pseudo-labeling，nearest class prototype (NCP) 和 structured prediction (SP)这四个部分进行不同的组合。最后得出四个部分都使用的时候性能是最好的。</p><h1 id="3-启发"><a href="#3-启发" class="headerlink" title="3 启发"></a>3 启发</h1><ol><li>$L_2$归一化提高样本分离度。</li><li>本文的SPL依然把目标域和源域的信息割裂开来了。能否把对所有样本实施K-means，得到然后在每一个簇中源域样本哪一类多，则这一簇中的目标域样本就为哪一类。</li><li>或可使用其他聚类方法，分层聚类，原型聚类，密度聚类等.</li><li>能否利用低秩表示来计算相似度矩阵。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://tvax2.sinaimg.cn/mw690/007v1rTBgy1g9qfpff2axj31hc0zj4qp.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Domain Adaptation" scheme="http://yvanzh.top/tags/Domain-Adaptation/"/>
    
  </entry>
  
  <entry>
    <title>My First Paper</title>
    <link href="http://yvanzh.top/2019/08/08/My-First-Paper/"/>
    <id>http://yvanzh.top/2019/08/08/My-First-Paper/</id>
    <published>2019-08-08T10:01:33.000Z</published>
    <updated>2020-01-01T07:00:28.550Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>5.1~8.7日，笔者在耕耘学术生涯中的第一篇论文，所以暂时搁置了博客。现在，盆友们，我胡汉三又回来惹！</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://tvax1.sinaimg.cn/wap690/007v1rTBgy1g2pio3mf7dj31hc0tze0y.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><p>此役历时三月有余。期间，由于实验结果达不到预期效果，于是笔者修改了模型，最后在六月下旬才做完实验。而后又马不停蹄地在参考文献中摸爬滚打，定型论文主线和框架。目前，论文的初稿已经写好了。接下来有两周的暑假时间，笔者准备回家好好休整，麻辣小龙虾等我很久了hiahiahia~🐱‍🏍</p><blockquote><p>如果后续把论文挂在arXiv，代码挂在GitHub，笔者将会在此篇博文中添加相关链接。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;5.1~8.7日，笔者在耕耘学术生涯中的第一篇论文，所以暂时搁置了博客。现在，盆友们，我胡汉三又回来惹！&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://tvax1.sinaimg.cn/wap690/007v1rTBgy1g2pio3mf7dj31hc0tze0y.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Researching" scheme="http://yvanzh.top/categories/Researching/"/>
    
    
      <category term="Researching" scheme="http://yvanzh.top/tags/Researching/"/>
    
  </entry>
  
  <entry>
    <title>二三年华</title>
    <link href="http://yvanzh.top/2019/04/30/23-years-old/"/>
    <id>http://yvanzh.top/2019/04/30/23-years-old/</id>
    <published>2019-04-30T03:11:11.000Z</published>
    <updated>2020-08-29T02:28:17.106Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/bmiddle/007v1rTBgy1g2ua64x7uaj30zm1hcgvg.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><blockquote><p><strong><em>淡墨青衫，躬植荆棘。</em></strong><br><strong><em>心囿象牙，身陷囹圄。</em></strong><br><strong><em>毋宁刺猝以醒迷痴，</em></strong><br><strong><em>不信宿命，仍抱愿景。</em></strong><br><strong><em>尝求思愆而量得失，</em></strong><br><strong><em>既怀逸兴，何惧天青。</em></strong><br><strong><em>隔世恍恍，明月悠悠，</em></strong><br><strong><em>旦辰只道，五载情愁。</em></strong><br><strong><em>离人，走马。</em></strong><br><strong><em>归客，行舟。</em></strong><br><strong><em>乘风路，沐雨途。</em></strong><br><strong><em>何处相逢，知音如故。</em></strong><br><strong><em>望尽长安花，</em></strong><br><strong><em>饮罢珠江水，</em></strong><br><strong><em>梦回洞庭湖。</em></strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/bmiddle/007v1rTBgy1g2ua64x7uaj30zm1hcgvg.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Life" scheme="http://yvanzh.top/categories/Life/"/>
    
    
      <category term="Life" scheme="http://yvanzh.top/tags/Life/"/>
    
  </entry>
  
  <entry>
    <title>Deep Self-Evolution Clustering</title>
    <link href="http://yvanzh.top/2019/04/25/Paper-Notes-7/"/>
    <id>http://yvanzh.top/2019/04/25/Paper-Notes-7/</id>
    <published>2019-04-25T12:32:39.000Z</published>
    <updated>2020-08-29T02:28:17.121Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1g2culm08exj31hc0zkkem.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>To be continue</p><ol><li>如今的聚类任务的难点并不是在于方法，而是在于确定任务数据的预设聚类模式。</li><li>传统的聚类方法依然需要分步进行聚类。比如将特征提取得到的特征来做聚类，在做聚类之前，数据的特征就是已经固定好了，也就是说在做聚类的过程中，数据特征是一成不变，也就无法使得聚类产生更好的效果。</li></ol><h1 id="2-详情"><a href="#2-详情" class="headerlink" title="2 详情"></a>2 详情</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://tvax2.sinaimg.cn/wap690/007v1rTBly1g6d6353xvlj31910kadtr.jpg" alt="DSEC" title="">                </div>                <div class="image-caption">DSEC</div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1g2culm08exj31hc0zkkem.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Clustering" scheme="http://yvanzh.top/tags/Clustering/"/>
    
  </entry>
  
  <entry>
    <title>Unsupervised and Semi-Supervised Learning via ℓ1-Norm Graph</title>
    <link href="http://yvanzh.top/2019/04/21/Paper-Notes-6/"/>
    <id>http://yvanzh.top/2019/04/21/Paper-Notes-6/</id>
    <published>2019-04-21T13:15:54.000Z</published>
    <updated>2020-08-29T02:28:17.121Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx4.sinaimg.cn/wap690/007v1rTBgy1g23ounx6imj31hc12c1aw.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/6126506/" target="_blank" rel="noopener">文章来源</a>：2011-ICCV</p><p>文章主旨:</p><p>由于图拉普拉斯无法直接得到好的聚类结构，后续需要利用$K$-means来进行聚类。然而$K$-means聚类容易局部收敛，且得到非唯一的聚类结果。</p><p>对于半监督学习来说，图拉普拉斯一般使用的都是采用的<strong>二次型的图嵌入</strong>（TODO），然而这种方式对噪声和异常值敏感。</p><p>作者提出一种$\ell_1$范数约束的谱聚类框架，且扩展了一个半监督学习模型，并证明了其收敛性。</p><h1 id="2-基本框架"><a href="#2-基本框架" class="headerlink" title="2 基本框架"></a>2 基本框架</h1><h2 id="2-1-谱聚类"><a href="#2-1-谱聚类" class="headerlink" title="2.1 谱聚类"></a>2.1 谱聚类</h2><p>对于$Q = \left[q_{1}, q_{2}, \cdots, q_{c}\right] \in \mathbb{R}^{n \times c}$，$q_{k} \in \mathbb{R}^{n \times 1}$为$Q$的第$k$列。</p><p>Graph Ratio Cut：</p><script type="math/tex; mode=display">\begin{gathered}  \min _{Q^{T} Q=I} \operatorname{Tr}\left(Q^{T} L Q\right) \\  q_k = (0, \cdots, 0, \overbrace{\frac{1}{\sqrt{n_{k}}}, \cdots, \frac{1}{\sqrt{n_{k}}}}^{n_k}, 0, \cdots, 0)^{T}\end{gathered} \tag{1}</script><p>Graph Normalized Cut：</p><script type="math/tex; mode=display">\begin{gathered}  \min _{Q^{T}DQ=I} \operatorname{Tr}\left(Q^{T} L Q\right)  \\  q_{k}=(0, \cdots, 0, \overbrace{\frac{1}{\sqrt{q_{k}^{T} D q_{k}}}, \cdots, \frac{1}{\sqrt{q_{k}^{T} D q_{k}}}}^{n_k}, 0, \cdots, 0)^{T} \end{gathered} \tag{2}</script><p>详情可参见<a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">谱聚类（spectral clustering）原理总结</a>一文。</p><h2 id="2-2-基本模型"><a href="#2-2-基本模型" class="headerlink" title="2.2 基本模型"></a>2.2 基本模型</h2><p>将$(2)$式写为：</p><script type="math/tex; mode=display">\min _{Q^{T} D Q=I} \sum_{i, j=1}^{n} W_{i j}\left\|q^{i}-q^{j}\right\|_{2}^{2} \tag{3}</script><p>$Q$的理想解就是当$x_i,x_j$属于同一类时，使得$q^i = q^j$。也就是说$Q$的许多行都是相等的，这样就有了很强的聚类结构。那么我们想要很多对$(i,j)$有$\left|q^{i}-q^{j}\right|_{2}=0$，那么用$\ell_1$范数来解决这个问题也是等价的：</p><script type="math/tex; mode=display">\min _{Q^{T} D Q=I} \sum_{i, j=1}^{n} W_{i j}\left\|q^{i}-q^{j}\right\|_{2} \tag{4}</script><p>设一个$n^2$维向量$p$，其$((i-1)*n+j)$个元素为$W_{i j}\left|q^{i}-q^{j}\right|_{2}$。那么我们将$(4)$式写为：</p><script type="math/tex; mode=display">\min _{Q^{T} D Q=I}\|p\|_{1} \tag{5}</script><p>这样我们可以直观地知道$p$的元素会由于$\ell_1$范数的约束而变得稀疏，也就为$Q$提供了一个理想的聚类结果。</p><h2 id="2-3-优化算法"><a href="#2-3-优化算法" class="headerlink" title="2.3 优化算法"></a>2.3 优化算法</h2><p>$(4)$式的拉格朗格日函数为：</p><script type="math/tex; mode=display">\mathcal{L}(Q)=\sum_{i, j=1}^{n} W_{i j}\left\|q^{i}-q^{j}\right\|_{2}-\operatorname{Tr}\left(\Lambda\left(Q^{T} D Q-I\right)\right) \tag{6}</script><p>设拉普拉斯矩阵$\widetilde{L} = \widetilde{D} -\widetilde{W}$，$\widetilde{D}$为第$i$个元素为$\sum_j\widetilde{W}_{ij}$的对角阵，$\widetilde{W}$为：</p><script type="math/tex; mode=display">\widetilde{W}_{i j}=\frac{W_{i j}}{2\left\|q^{i}-q^{j}\right\|_{2}} \tag{7}</script><p>那么$\mathcal{L}(Q)$对$Q$的偏导为零时，有：</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(Q)}{\partial Q}=\widetilde{L} Q-D Q \Lambda=\mathbf{0} \tag{8}</script><p>也就是说解$Q$为$D^{-1}\widetilde{L}$的特征值。注意到$D^{-1}\widetilde{L}$又依赖于$Q$，于是可以通过迭代来得到$Q$的局部最优解。</p><h2 id="2-4-收敛性分析"><a href="#2-4-收敛性分析" class="headerlink" title="2.4 收敛性分析"></a>2.4 收敛性分析</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx4.sinaimg.cn/bmiddle/007v1rTBgy1g2aj0qrgh5j30q70myjui.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><blockquote><p>引理1<br>对任意非零向量$q$，$q_t \in \mathcal{R}^c$，有：</p><script type="math/tex; mode=display">\|q\|_{2}-\|q\|_{2}^{2} / 2\left\|q_{t}\right\|_{2} \leq\left\|q_{t}\right\|_{2}-\left\|q_{t}\right\|_{2}^{2} / 2\left\|q_{t}\right\|_{2} \tag{9}</script></blockquote><p>我们需要利用引理1来证明算法可以收敛。</p><blockquote><p>定理1<br>算法1会在每次迭代中单调地降低问题$(4)$的目标，并接近于问题的局部最优。</p></blockquote><p>证明：</p><p>根据算法1的第二步，可知：</p><script type="math/tex; mode=display">Q_{t+1}=\arg \min _{Q^{T} D Q=I} \sum_{i, j=1}^{n}(\widetilde{W}_{t})_{i j}\left\|q^{i}-q^{j}\right\|_{2}^{2} \tag{10}</script><p>又$\left(\widetilde{W}_{t}\right)_{i j}=\frac{W_{i j}}{2\left|q_{t}^{i}-q_{t}^{j}\right|_{2}}$，那么：</p><script type="math/tex; mode=display">\sum_{i, j=1}^{n} \frac{W_{i j}\left\|q_{t+1}^{i}-q_{t+1}^{j}\right\|_{2}^{2}}{2\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}} \leq \sum_{i, j=1}^{n} \frac{W_{i j}\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}^{2}}{2\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}} \tag{11}</script><p>根据引理1，可得：</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{i, j=1}^{n} W_{i j}\left(\left\|q_{t+1}^{i}-q_{t+1}^{j}\right\|_{2}-\frac{\left\|q_{t+1}^{i}-q_{t+1}^{j}\right\|_{2}^{2}}{2\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}}\right) \\ & \leq \sum_{i, j=1}^{n} W_{i j}\left(\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}-\frac{\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}^{2}}{2\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}}\right) \end{aligned} \tag{12}</script><p>结合$(11)(12)$式，可得：</p><script type="math/tex; mode=display">\sum_{i, j=1}^{n} W_{i j}\left\|q_{t+1}^{i}-q_{t+1}^{j}\right\|_{2} \leq \sum_{i, j=1}^{n} W_{i j}\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2} \tag{13}</script><p>因此，算法1将在每次迭代$t$中单调地降低问题$(4)$的目标，直到算法收敛。当达到收敛时，$(13)$式等号成立，则$Q_t$和$\widetilde{L}_t$将满足$(8)$式，也就是问题$(4)$的$\text{KKT}$条件。定理1得证。</p><h1 id="3-半监督框架"><a href="#3-半监督框架" class="headerlink" title="3 半监督框架"></a>3 半监督框架</h1><h2 id="3-1-问题描述"><a href="#3-1-问题描述" class="headerlink" title="3.1 问题描述"></a>3.1 问题描述</h2><p>设$Y=[\left(y^{1}\right)^{T},\left(y^{2}\right)^{T}, \cdots,\left(y^{n}\right)^{T}] \in \mathbb{R}^{n \times c}$为初始的标签矩阵。若$x_i$为无标签数据，则$y^i=0$。若$x_i$为$k$类，则$y^i$的第$k$个元素为1，否则为0。</p><p>传统的半监督学习需要解决的问题如下：</p><script type="math/tex; mode=display">\min _{Q} \operatorname{Tr}\left(Q^{T} L Q\right)+\operatorname{Tr}(Q-Y)^{T} U(Q-Y) \tag{14}</script><p>其中$L$为拉普拉斯矩阵，$U$为控制$x_i$初始标签$y^i$的影响程度的对角阵（相当于超参），$Q \in \mathcal{R}^{n \times c}$为需要求解的标签矩阵。</p><p>类似的$(14)$式可写为：</p><script type="math/tex; mode=display">\min _{Q} \sum_{i, j=1}^{n} W_{i j}\left\|q^{i}-q^{j}\right\|_{2}^{2}+\operatorname{Tr}(Q-Y)^{T} U(Q-Y) \tag{15}</script><p>为了得到最优解$Q$，我们需要解决以下半监督分类问题（注意这个用的是$\ell_1$范数）：</p><script type="math/tex; mode=display">\min _{Q} \sum_{i, j=1}^{n} W_{i j}\left\|q^{i}-q^{j}\right\|_{2}+\operatorname{Tr}(Q-Y)^{T} U(Q-Y) \tag{16}</script><h2 id="3-2-优化算法"><a href="#3-2-优化算法" class="headerlink" title="3.2 优化算法"></a>3.2 优化算法</h2><p>$(16)$式对$Q$求偏导为零时有：</p><script type="math/tex; mode=display">\widetilde{L} Q+U(Q-Y)=\mathbf{0} \Rightarrow Q=(\widetilde{L}+U)^{-1} U Y \tag{17}</script><h2 id="3-3-收敛性分析"><a href="#3-3-收敛性分析" class="headerlink" title="3.3 收敛性分析"></a>3.3 收敛性分析</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx4.sinaimg.cn/bmiddle/007v1rTBgy1g2bdhhoz27j30q60ixq5j.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><blockquote><p>定理1<br>算法2会在每次迭代中单调地降低问题$(16)$的目标，并接近于问题的局部最优。</p></blockquote><p>证明：</p><p>设$f(Q)=\operatorname{Tr}(Q-Y)^{T} U(Q-Y)$，据算法2的第二步可知：</p><script type="math/tex; mode=display">Q_{t+1}=\arg \min _{Q} \sum_{i, j=1}^{n}\left(\tilde{W}_{t}\right)_{i j}\left\|q^{i}-q^{j}\right\|_{2}^{2}+f(Q) \tag{18}</script><p>注意到$(\tilde{W}_{t})_{i j}=\frac{W_{i j}}{2\left|q_{t}^{i}-q_{t}^{j}\right|_{2}}$，则：</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{i, j=1}^{n} \frac{W_{i j}\left\|q_{t+1}^{i}-q_{t+1}^{j}\right\|_{2}^{2}}{2\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}}+f\left(Q_{t+1}\right) \\ & \leq \sum_{i, j=1}^{n} \frac{W_{i j}\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}^{2}}{2\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}}+f\left(Q_{t}\right) \end{aligned} \tag{19}</script><p>将$(19)$式和$(12)$式两边求和，可得：</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{i, j=1}^{n} W_{i j}\left\|q_{t+1}^{i}-q_{t+1}^{j}\right\|_{2}+f\left(Q_{t+1}\right) \\ & \leq \sum_{i, j=1}^{n} W_{i j}\left\|q_{t}^{i}-q_{t}^{j}\right\|_{2}+f\left(Q_{t}\right) \end{aligned} \tag{20}</script><p>因此，算法2在每次迭代$t$中单调地降低问题$(16)$的目标，收敛时$Q_t$和$L_t$满足$(17)$式。由于问题$(16)$是一个凸优化问题，满足式$(17)$表明$Q_t$是问题$(16)$的全局最优解。因此，算法2收敛到问题$(16)$的全局最优。定理2得证。</p><h1 id="4-思考"><a href="#4-思考" class="headerlink" title="4 思考"></a>4 思考</h1><ol><li><p>本文模型简单来说就是在Normalized Cut谱聚类的基础上，将$\ell_2$范数约束项的平方等价为了一个$\ell_1$范数，目的是为了构造出一个与$Q$相关的$\widetilde{W}$，从而联系了$Q$和$L$并使得二者交替得以更新。相较于固定$L$的原始Normalized Cut算法来说，$L$的更新无疑带来了算法的提升。说穿了感觉就像是一个优化的小trick造就了一篇顶会。</p></li><li><p>从直观层面谈一谈聚类算法：（TODO）</p><ul><li><p>$K$-means。其一般采用欧氏距离所以只能局限于球形簇。距离度量的不确定性是算是聚类算法的通病。我们需要通过多次尝试，或者精细地分析所研究数据的特点来确定选取何种距离。$K$-means最大的问题是初始化过程对结果的影响非常大，对此有$K$-means++和二分$K$-means改进了初始化过程。而对噪点和异常值十分敏感的问题，出现了抛弃均值而转投中值怀抱的尝试（$K$-mediods）。</p></li><li><p>DESCAN。</p></li><li><p>分层聚类。</p></li><li><p>谱聚类。主要的两个问题，</p></li><li><p>流形聚类。</p></li><li><p>子空间聚类。</p></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx4.sinaimg.cn/wap690/007v1rTBgy1g23ounx6imj31hc12c1aw.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Clustering" scheme="http://yvanzh.top/tags/Clustering/"/>
    
      <category term="Spectral Clustering" scheme="http://yvanzh.top/tags/Spectral-Clustering/"/>
    
  </entry>
  
  <entry>
    <title>引子</title>
    <link href="http://yvanzh.top/2019/04/15/Journal-0/"/>
    <id>http://yvanzh.top/2019/04/15/Journal-0/</id>
    <published>2019-04-15T12:21:16.000Z</published>
    <updated>2020-08-29T02:28:17.118Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, please enter password to read." />    <label for="pass">Welcome to my blog, please enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      The article has been encrypted, please enter your password to view.&lt;br&gt;
    
    </summary>
    
      <category term="Journal" scheme="http://yvanzh.top/categories/Journal/"/>
    
    
      <category term="Journal" scheme="http://yvanzh.top/tags/Journal/"/>
    
  </entry>
  
  <entry>
    <title>Flexible Manifold Embedding:A Framework for Semi-Supervised and Unsupervised Dimension Reduction</title>
    <link href="http://yvanzh.top/2019/04/13/Paper-Notes-5/"/>
    <id>http://yvanzh.top/2019/04/13/Paper-Notes-5/</id>
    <published>2019-04-13T07:52:52.000Z</published>
    <updated>2020-08-29T02:28:17.120Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx4.sinaimg.cn/wap690/007v1rTBgy1g1vekpd0kij31hc0qo7er.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><a href="https://ieeexplore.ieee.org/document/5427147" target="_blank" rel="noopener">文章来源</a>：2010-TPAMI</p><p>文章主旨：</p><p>很多降维方法都是使用线性映射$F=X^TW$（比如PCA，LDA，LPP，SDA）。它们简单高效，但是在实际应用中预测标签$F$位于训练样本所张成的低维空间中未免太多严格。</p><p>提出一个新的框架，同时优化预测标签$F$，线性回归函数$h(X)$和回归残差$F_0$。其结合了标签适配度和流形平滑度（其实指的就是局部信息或局部一致性）有关的两项，以及一个灵活的惩罚项$\Vert F_0 \Vert^2$。</p><p>后知后觉：</p><p>本文的模型依然是线性的，保留了线性映射的简单高效的特点，同时利用残差放宽了线性约束，很好避免了过拟合。在深度学习还没有兴起时，这无疑是个很好的方法。感觉何凯明也是吸取了残差的思想才创造了Res-Net，本文的影响可见一斑。</p><h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h1><h2 id="2-1-LGC-and-GFHF"><a href="#2-1-LGC-and-GFHF" class="headerlink" title="2.1 LGC and GFHF"></a>2.1 LGC and GFHF</h2><p>Local and global consistency (LGC)：</p><script type="math/tex; mode=display">g_L(F) = \frac{1}{2}\sum_{i,j=1}^m \Big\Vert \frac{F_{i.}}{\sqrt{D_{ii}}}- \frac{F_{i.}}{\sqrt{D_{jj}}} \Big \Vert^2 S_{ij} +\lambda\sum _{i=1}^n \Vert F_{i.} -Y_{i.} \Vert^2  \tag{1}</script><p>Gaussian fields and harmonic functions (GFHF):</p><script type="math/tex; mode=display">g_G(F) = \frac{1}{2} \sum_{i,j=1}^m \Vert F_{i.} - F_{j.} \Vert^2 S_{ij} +\lambda_{\infty} \sum_{i =1} ^n \Vert F_{i.} -Y{i.} \Vert^2 \tag{2}</script><p>其中配分系数$\lambda$用来平衡流形平滑度和标签适配。$\lambda_{\infty}$是个非常大的数。</p><p>注意到$(1)(2)$式共享一个方程：</p><script type="math/tex; mode=display">\operatorname{Tr}(F^TMF) +\operatorname{Tr}(F-Y)^T U (F-Y) \tag{3}</script><p>其中$M \in \mathcal{R}^{m\times m}$代表拉普拉斯矩阵，$U \in \mathcal{R}^{m \times m}$为对角矩阵。</p><p>在LGC中，$M$表示一个归一化的拉普拉斯矩阵$\hat{L}$，$U$表示元素为$\lambda$的对角矩阵。</p><p>在GFHF中，$M$表示普通的拉普拉斯矩阵，$U$表示前n个元素和后$m-n$个元素分别为$\lambda_\infty$和$0$的对角矩阵。</p><h2 id="2-2-MR"><a href="#2-2-MR" class="headerlink" title="2.2 MR"></a>2.2 MR</h2><p>Manifold Regularization (MR) 扩展了许多现有的算法。比如岭回归和SVM，使得它们可以通过加入一个基于几何的正则化项来进行半监督学习。</p><p>来简单看一下LapRLS/L，它就是将MR对岭回归进行扩展，同时计算岭回归的误差并保留流形平滑度。其表示为：</p><script type="math/tex; mode=display">g_M(W,b) = \lambda_A\Vert W \Vert^2 + \lambda_I\operatorname{Tr}r(W^TXLX^TW) + \frac{1}{n} \sum_{i =1}^n \Vert W^Tx_i +b -Y_{i.}^T \Vert^2 \tag{4}</script><h2 id="2-3-SDA"><a href="#2-3-SDA" class="headerlink" title="2.3 SDA"></a>2.3 SDA</h2><p>Semi-Supervised Discriminant Analysis (SDA) 的核心假设依然是流形平滑，也就是在低维空间中相近的点具有相似的特征表示。</p><p>定义$X_l = [x_1,x_2,\dots,x_n]$为有标签数据的矩阵。第$i$类的样本数为$n_i$。图相似矩阵$\tilde{S}^w,\tilde{S}^b \in \mathcal{R}^{n\times n}$，其中$\tilde{S}^w_{ij}= \delta_{y_i,y_j}/n_{y_i}$，$\tilde{S}^b_{ij}= (\frac{1}{n})-\tilde{S}^w_{ij}$。它们分别对应的拉普拉斯矩阵为$\tilde{L}_w$和$\tilde{L}_b$。</p><p>类内散度:</p><script type="math/tex; mode=display">S_w = \sum_{i =1}^n(x_i - \bar{x}_{y_i})(x_i - \bar{x}_{y_i})^T = X_l\tilde{L}_wX_l^T \tag{5}</script><p>类间散度：</p><script type="math/tex; mode=display">S_b = \sum_{l =1}^c n_c ( \bar{x}_l- \bar{x})(\bar{x}_l - \bar{x})^T = X_l \tilde{L}_b X_l^T \tag{6}</script><p>SDA：</p><script type="math/tex; mode=display">g_S(W) = \frac{\vert W^TX_l \tilde{L}_bX^T_l W \vert } {\vert W^T(X_l(\tilde{L}_w +\tilde{L}_b)X_l^T +\alpha XLX^T + \beta I) W \vert}  \tag{7}</script><h1 id="3-模型框架"><a href="#3-模型框架" class="headerlink" title="3 模型框架"></a>3 模型框架</h1><h2 id="3-1-联系-LGC-GFHF-和-LapRLSL-L"><a href="#3-1-联系-LGC-GFHF-和-LapRLSL-L" class="headerlink" title="3.1 联系 LGC/GFHF 和 LapRLSL/L"></a>3.1 联系 LGC/GFHF 和 LapRLSL/L</h2><p>LGC/GFHF基于标签传播和随机游走而被提出，LapRLSL/L的提出为了对岭回归的进行半监督扩展。<br>LGC/GFHF只能为现有的数据点找到一个合适的映射关系，而LapRLSL/L可以通过线性函数$h(x)$为新数据点提供一个映射。</p><blockquote><p>命题1<br>当拉普拉斯矩阵$M\in \mathcal{R}^{m\times m}$满足$M\mathbf{1} = 0$与$\mathbf{1}^TM = 0^T，$LapRLSL/L是扩展到样本外的LGC/GFHF。</p></blockquote><p>证明：</p><p>假设LGC/GFHF的解$F$位于由$X$张成的线性子空间中，比如$F= h(X)= X^TW + \mathbf{1}b^T$。其中$W \in \mathcal{R}^{f\times c}$为投影矩阵，$b\in \mathcal{R}^{c \times 1}$为偏置项。那么LGC/GFHF的目标函数$(3)$式可被写为：</p><script type="math/tex; mode=display">\begin{gathered}Tr[(X^TW + \mathbf{1}b^T)^T M (X^TW + \mathbf{1}b^T)] \\+ Tr[(X^TW + \mathbf{1}b^T - Y )^T U(X^TW + \mathbf{1}b^T -Y)] \end{gathered} \tag{8}</script><p>接着添加一个正则化项$(\lambda_A)/(\lambda_I) \Vert W \Vert^2$，并设$M = L$，对角阵$U$的前$n$个元素和后$m-n$个元素分别为$(1)/(n\lambda_A)$和$0$。则$(8)$式变为：</p><script type="math/tex; mode=display">\frac{\lambda_A}{\lambda_I}\Vert W \Vert^2 \operatorname{Tr}r(W^TXLX^TW) + \frac{1}{n\lambda_I}\sum_{i =1}^n \Vert W^Tx_i + b - Y_i^T \Vert^2\tag{9}</script><p>于是$(9)$式就等于$(1)/(\lambda_I)g_M(W,b)$。命题1得证。</p><h2 id="3-2-FME"><a href="#3-2-FME" class="headerlink" title="3.2 FME"></a>3.2 FME</h2><p>从命题1我们知道LapRLSL/L中的预测标签$F$也是被限制在由所有训练样本$X$所张成的空间中。尽管我们学得的线性函数可以映射新的数据点，但是$W$中的参数的个数并不依赖于样本的个数。因此，这个线性函数可能会过拟合来自非线性流形的训练样本。于是作者提出FME(Flexible Manifold Embedding)框架来解决这个问题。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws4.sinaimg.cn/bmiddle/007v1rTBgy1g1z0g7zhtvj30rn0fdjs9.jpg" alt="Fig.1" title="">                </div>                <div class="image-caption">Fig.1</div>            </figure><p>如Fig.1所示，设$F = h(X) +F_0 = X^TW + \mathbf{1}b^T + F_0$，作者通过使用回归残差来放宽约束。其中$F_0 \in \mathcal{R}^{m \times c}$就是用来建模$F$与$h(X)$之间失配的回归残差。FME就是为了同时寻找最优的预测标签$F$，回归残差$F_0$，和线性回归函数$h(X)$。即：</p><script type="math/tex; mode=display">\begin{aligned}(F^*,F^*_0,W^*,b^*) = \arg \underset{F,F_0,W,b}{\min} &\operatorname{Tr}(F-Y)^TU(F-Y) +\operatorname{Tr}(F^TMF) \\&+ \mu(\Vert W \Vert^2 + \gamma \Vert F_0 \Vert^2)    \end{aligned} \tag{10}</script><p>其中$M \in \mathcal{R}^{m\times m}$为拉普拉斯矩阵，$U \in \mathcal{R}^{m \times m}$为对角阵。前人也做过一些类似工作，不过都是聚焦于二分类任务。在这里，作者扩展到多类别的降维任务，且类别的独立性可以从提取的特征中捕捉到。</p><p>与LGC、GFHF和LapRLS/L类似，$(10)$式的前两项分别表示标签适配度和流形平滑度。考虑到不同样本（比如$j \neq i$）的预测标签$F_i$和给定标签$Y_j$之间的近似是无意义的，因此设定$U$为前$n$个元素和后$m-n$个元素分别为$1$和$0$的对角阵。此外，为了保持流形结构（比如，$F$应该尽可能地在整个图中保持平滑），在半监督学习中$M$应该被设为图的拉普拉斯矩阵。用高斯核函数来计算$M = D - S$，其中$D_{ii} = \sum_{j}S_{ij}$，若$x_i$为$x_j$的$k$近邻，$S_{ij} = exp(- \Vert x_i - x_j \Vert^2/t)$；否则$S_{ij} = 0$。</p><p>$(10)$式中的后两项用来控制投影矩阵$W$和回归残差$F_0$。相较于LapRLS/L，我们不强制$F$位于训练样本$X$所张成的空间中。因此，我们的框架更灵活，同时也能更好地处理驻留在非线性流形的样本。</p><p>用$F-X^TW -\mathbf{1}b^T$替换$F_0$，那么得到：</p><script type="math/tex; mode=display">\begin{aligned}(F^*,W^*,b^*) = \arg \underset{F,W,b}{\min} &\operatorname{Tr}(F-Y)^T U (F-Y) + \operatorname{Tr}(F^TMF) \\&+\mu(\vert W \Vert^2 +\gamma \Vert X^TW + \mathbf{1}b^T - F \Vert^2)\end{aligned}  \tag{11}</script><blockquote><p>定理1<br>设$U,M \in \mathcal{R}^{m\times m}$，$F,Y\in \mathcal{R}^{m\times c}$，$W\in \mathcal{R}^{f \times c}$，$b \in\mathcal{R}^{c \times 1}$。若$U$和$M$为半正定矩阵，且$\mu ,\gamma \ge 0$，则</p><script type="math/tex; mode=display">g(F,W,b) = \operatorname{Tr}(F-Y)^TU(F-Y) + \operatorname{Tr}(F^TMF) + \mu(\Vert W \Vert^2 + \gamma \Vert X^TW + \mathbf{1}b^T - F \Vert^2)</script><p>对于$F,W,b$为联合凸的。</p></blockquote><p>证明：</p><p>在$g(F,W,b)$中，我们移除常数项$\operatorname{Tr}(Y^TUY)$，那么可以写为：</p><script type="math/tex; mode=display">g(F,W,b) = Tr  \begin{bmatrix}    F \\    W \\    b^T\end{bmatrix}^T P \begin{bmatrix}    F \\    W \\    b^T    \end{bmatrix} - Tr \begin{bmatrix}    F \\    W \\    b^T\end{bmatrix}^T \begin{bmatrix}    2UY \\    0 \\    0   \end{bmatrix}</script><p>其中：</p><script type="math/tex; mode=display">P = \begin{bmatrix}    \mu\gamma I+M+U & -\mu \gamma X^T & -\mu \gamma \mathbf{1}    \\    - \mu\gamma X & \mu I +\mu \gamma XX^T & \mu \gamma X\mathbf{1} \\    - \mu \gamma \mathbf{1}^T & \mu \gamma \mathbf{1}^TX^T & \mu \gamma m\end{bmatrix}</script><p>因此我们要证$g(F,W,b)$对$F,W,b$为联合凸的，只需要证$P$为半正定矩阵即可。</p><p>对任意向量$z = [z_1^T,z_2^T,z_3^T]\in \mathcal{R}^{(m+f+1)\times 1}$，其中$z_1 \in \mathcal{R}^{m \times 1},z_2 \in \mathcal{R}^{f\times 1},z_3$是个标量，则有：</p><script type="math/tex; mode=display">\begin{aligned} z^{T} P z=& z_{1}^{T}(\mu \gamma I+M+U) z_{1}-2 \mu \gamma z_{1}^{T} X^{T} z_{2}-2 \mu \gamma z_{1}^{T} 1 z_{3} \\ &+z_{2}^{T}\left(\mu I+\mu \gamma X X^{T}\right) z_{2}+2 \mu \gamma z_{2}^{T} X \mathbf{1} z_{3}+\mu \gamma m z_{3}^{T} z_{3} \\= &z_{1}^{T}(M+U) z_{1}+\mu z_{2}^{T} z_{2}+\mu \gamma\left(z_{1}^{T} z_{1}-2 z_{1}^{T} X^{T} z_{2}\right.\\ &-2 z_{1}^{T} 1 z_{3}+z_{2}^{T} X X^{T} z_{2}+2 z_{2}^{T} X \mathbf{1} z_{3}+m z_{3}^{T} z_{3} ) \\=& z_{1}^{T}(M+U) z_{1}+\mu z_{2}^{T} z_{2}+\mu \gamma\left(z_{1}-X^{T} z_{2}-\mathbf{1} z_{3}\right)^{T} \\ & \times\left(z_{1}-X^{T} z_{2}-\mathbf{1} z_{3}\right)\end{aligned}</script><p>也就是若$U,M$为半正定矩阵，且$\mu,\gamma \ge 0$，那么$\forall z,z^TPz \ge 0$。因此$P$为正定矩阵，也就是说$g(F,W,b)$是凸函数。定理1得证。</p><p>为了得到最优解，首先我们设$(11)$式对$b,W$的偏导为$0$。可得：</p><script type="math/tex; mode=display">\begin{gathered}    b = \frac{1}{m}(F^T\mathbf{1} - W^TX\mathbf{1}) \\    W = \gamma(\gamma XH_cX^T + I)^{-1}XH_cF = AF \end{gathered} \tag{12}</script><p>其中$A = \gamma(\gamma XH_cX^T + I)^{-1}XH_c,H_c = I - (1/m)\mathbf{1}\mathbf{1}^T$用来对数据进行中心化。利用$(12)$式重新表示$(11)$式中回归函数 $X^TW+\mathbf{1}b^T$为：</p><script type="math/tex; mode=display">\begin{aligned} X^{T} W+\mathbf{1} b^{T} &=X^{T} A F+\frac{1}{m} \mathbf{1 1}^{T} F-\frac{1}{m} \mathbf{1 1}^{T} X^{T} A F \\ &=H_{c} X^{T} A F+\frac{1}{m} \mathbf{1 1}^{T} F=B F \end{aligned} \tag{13}</script><p>其中$B=H_{c} X^{T} A+(1 / m) \mathbf{1} \mathbf{1}^{T}$。代入$(11)$是，得到：</p><script type="math/tex; mode=display">\begin{aligned} F^{*}=& \arg \min _{F} \operatorname{Tr}(F-Y)^{T} U(F-Y) \\ &+\operatorname{Tr}\left(F^{T} M F\right)+\mu\left(\operatorname{Tr}\left(F^{T} A^{T} A F\right)\right.\\ &+\gamma \operatorname{Tr}(B F-F)^{T}(B F-F) ) \end{aligned} \tag{14}</script><p>然后设$(14)$式对$F$偏导为$0$，可得：</p><script type="math/tex; mode=display">F=\left(U+M+\mu \gamma(B-I)^{T}(B-I)+\mu A^{T} A\right)^{-1} U Y \tag{15}</script><p>根据$H_cH_c = H_c = H_c^T$和$\gamma\mu  A^TXH_cX^TA+\mu A^TA = \gamma \mu A^TXH_c = \gamma\mu H_cX^TA$，$(15)$式中的$\mu \gamma(B-I)^{T}(B-I)+ \mu A^TA$可写为$\mu \gamma\left(A^{T} X-I\right) H_{c}\left(X^{T} A-I\right)+ \mu A^TA$。因此可得：</p><script type="math/tex; mode=display">\begin{aligned} \mu \gamma(B-I)^{T}(B-I)+\mu A^{T} A=& \mu \gamma H_{c}-\mu \gamma^{2} H_{c} X^{T} \\ & \times\left(\gamma X H_{c} X^{T}+I\right)^{-1} X H_{c} \end{aligned} \tag{16}</script><p>根据定义$X_c = XH_c$，我们可以计算预测标签$F$：</p><script type="math/tex; mode=display">F=\left(U+M+\mu \gamma H_{c}-\mu \gamma^{2} N\right)^{-1} U Y \tag{17}</script><p>其中$N=X_{c}^{T}\left(\gamma X_{c} X_{c}^{T}+I\right)^{-1} X_{c}=X_{c}^{T} X_{c}\left(\gamma X_{c}^{T} X_{c}+I\right)^{-1}$。</p><p>综上，首先利用$(17)$式得到最优解$F$，然后根据$(12)$式得到最优解$W,b$。</p><h2 id="3-3-FME-U"><a href="#3-3-FME-U" class="headerlink" title="3.3 FME/U"></a>3.3 FME/U</h2><p>通过设定$(11)$式中的矩阵$U$为$0$可以简单得到FME的无监督版本：</p><script type="math/tex; mode=display">\begin{aligned}\left(F^{*}, W^{*}, b^{*}\right)=& \arg \min_{F, W, b, F^{T} V F=I} \operatorname{Tr}\left(F^{T} M F\right) \\ &+\mu\left(\|W\|^{2}+\gamma\left\|X^{T} W+1 b^{T}-F\right\|^{2}\right) \end{aligned} \tag{18}</script><p>其中$V$被设为$H_c$，$I$为单位矩阵。</p><p>在无监督学习中，变量$F$可以看作低维表征的隐变量。我们限制$F$经过中心化操作后位于一个球面上，以避免$F=0$。同时$(18)$式是一个一般形式的等式，其可以通过使用不同的矩阵$M$和$V$来进行监督学习（TODO）。FME/U很自然地提供了一个对新数据映射方法，即$h(X) = X^TW+ \mathbf{1}b^T$，且由于增加了一个残差惩罚项（$\Vert h(X) - F \Vert^2$），所以较于前人研究的“硬核”映射$F = X^TW$更显灵活性。</p><p>同样类似的优化步骤，首先设$(18)$式对于$W,b$的偏导为$0$，根据$(12)$式计算$W,b$，然后将$W,b$代入$(18)$式中，得到：</p><script type="math/tex; mode=display">\begin{aligned} F^{*}= \arg \min _{F, F^{T} H_{c} F=I} &\operatorname{Tr}\left(F^{T} M F\right)+\mu\left(\operatorname{Tr}\left(F^{T} A^{T} A F\right)\right.\\ &+\gamma \operatorname{Tr}(B F-F)^{T}(B F-F) ) \end{aligned} \tag{19}</script><p>最后根据$(16)$式，可以将$(19)$式写为：</p><script type="math/tex; mode=display">\begin{aligned} F^{*} &=\arg \min _{F, F^{T} H_{c} F=I} \operatorname{Tr} F^{T}\left(M+\mu \gamma H_{c}-\mu \gamma^{2} N\right) F \\ &=\arg \min _{F, F^{T} H_{c} F=I} \operatorname{Tr} F^{T}\left(M-\mu \gamma^{2} N\right) F \end{aligned} \tag{20}</script><p>其中$N=X_{c}^{T}\left(\gamma X_{c} X_{c}^{T}+I\right)^{-1} X_{c}=X_{c}^{T} X_{c}\left(\gamma X_{c}^{T} X_{c}+I\right)^{-1}$。</p><p>综上，我们先通过$(20)$式得到最优解$F$，然后通过$(12)$式得到最优解$W,b$。</p><h1 id="4-模型比较"><a href="#4-模型比较" class="headerlink" title="4 模型比较"></a>4 模型比较</h1><p>模型比较分三个部分：</p><ol><li>比较FME与其他的半监督学习算法LGC,GFHF,LapRLS/L。</li><li>比较FME/U与Graph Embedding Framwork。</li><li>比较FME/U与Spectral Regression。</li></ol><h2 id="4-1-比较FME"><a href="#4-1-比较FME" class="headerlink" title="4.1 比较FME"></a>4.1 比较FME</h2><blockquote><p>示例1<br>LGC与GFHF为FME的两种特殊情形。</p></blockquote><p>证明：</p><p>若我们设$\mu = 0$，那么FME的目标函数$(11)$式就退化成了<br>$(3)$式，也就是LGC和GFHF的一般形式。示例1得证。</p><blockquote><p>示例2<br>LapRLS/L也是FME的一种特殊情形。</p></blockquote><p>证明：</p><p>若我们设$(11)$式中$\mu (\lambda_A)/(\lambda_I)$且$\gamma \rightarrow \infty$，那么可得$F = X^TW+\mathbf{1}b^T$，代入$(11)$式有：</p><script type="math/tex; mode=display">\begin{aligned} g(W, b)=& \operatorname{Tr}\left(X^{T} W+\mathbf{1} b^{T}\right)^{T} M\left(X^{T} W+1 b^{T}\right) \\ &+\mu\|W\|^{2}+\operatorname{Tr}\left(X^{T} W+1 b^{T}-Y\right)^{T} \\ & U\left(X^{T} W+1 b^{T}-Y\right) \end{aligned} \tag{21}</script><p>进一步设$M = L$且$U$为前$n$个元素和后$m-n$个元素分别为$(1)/(n\lambda_I)$和$0$的对角阵。那么$g(W,b)$就等于$(4)$式中的$(1)/(\lambda_I)g_M(W,b)$。示例2得证。</p><h2 id="4-2-比较FME-U与GE"><a href="#4-2-比较FME-U与GE" class="headerlink" title="4.2 比较FME/U与GE"></a>4.2 比较FME/U与GE</h2><p>此前有一篇很厉害的<a href="https://ieeexplore.ieee.org/document/4016549" target="_blank" rel="noopener">文章</a>提出了广义的图嵌入框架整合了一大堆降维算法（如PCA, LDA, ISOMAP, LLE, LE）。文章把各个算法给定的统计和几何性质编码为图形关系，并且每个算法都可以被看作是直接图嵌入，线性图嵌入，或其他扩展。直接图嵌入的目标函数为：</p><script type="math/tex; mode=display">F^{*}=\arg \min _{F, F^{T} V F=I} \operatorname{Tr}\left(F^{T} M F\right) \tag{22}</script><p>其中$V$为另一个图拉普拉斯矩阵（比如中心矩阵$H_c$），那么有$V\mathbf{1} = \mathbf{0},\mathbf{1}^TV = \mathbf{0}^T$。</p><p>然而直接的图嵌入计算得到的对于训练样本低维表征$F$并不能为新的数据点提供映射。那篇文章中也给出了解决方法，线性化，核化，向量化等。假设一个硬线性映射为$F = X^TW +\mathbf{1}b^T$，那么目标函数在线性图嵌入中表示为：</p><script type="math/tex; mode=display">\begin{aligned} W^{*}=& \arg \min_{W,\left(X^{T} W+\mathbf{1} b^{T}\right)^{T} V\left(X^{T} W+\mathbf{1} b^{T}\right)=I} \\ & \operatorname{Tr}\left(X^{T} W+\mathbf{1} b^{T}\right)^{T} M\left(X^{T} W+\mathbf{1} b^{T}\right) \\ =&\arg \min_{W, W^{T} X V X^{T} W=I} \operatorname{Tr}\left(W^{T} X M X^{T} W\right) \end{aligned} \tag{23}</script><blockquote><p>示例3<br>直接图嵌入和它的线性化是$FME/U$的一种特殊情形。</p></blockquote><p>证明：</p><p>若我们设$\mu = 0$，那么$FME/U$的目标函数退化为$(22)$式中的直接图嵌入。</p><p>当$(18)$式中$\mu \rightarrow 0,\mu \gamma \rightarrow \infty$时，有$F = X^TW +\mathbf{1}b^T$。替换$(18)$式中的的$F$后$FME/U$的目标函数退化为$(23)$式中的线性图嵌入。示例3得证。</p><h2 id="4-3-比较FME-U与SR"><a href="#4-3-比较FME-U与SR" class="headerlink" title="4.3 比较FME/U与SR"></a>4.3 比较FME/U与SR</h2><p>SR的提出是为了解决投影矩阵$W$来映射新数据点的问题。分为两个步骤，首先$(18)$式已经得到最优解$F$，然后计算最优解$W,b$：</p><script type="math/tex; mode=display">\left[W^{*}, b^{*}\right]=\arg \min _{W, b}\left\|X^{T} W+1 b^{T}-F\right\|^{2}+\lambda\|W\|^{2} \tag{24}</script><blockquote><p>示例4<br>SR是$FME/U$的一种特殊情形。</p></blockquote><p>证明：</p><p>当$\mu \rightarrow 0, \gamma = 1/\lambda$，那么$(18)$式会退化为$(22)$式，也就是说我们先解$F$。然后目标函数从$(18)$式转化到$(24)$式来解$W$，此时注意到SR目标函数为$W^{*}=\left(X H_{c} X^{T}+\lambda I\right)^{-1} X H_{c} F$，且与FME/U中的一致。示例4得证。</p><h2 id="4-4-汇总"><a href="#4-4-汇总" class="headerlink" title="4.4 汇总"></a>4.4 汇总</h2><p>直接上图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws4.sinaimg.cn/bmiddle/007v1rTBgy1g21ahtrfepj30n80p7766.jpg" alt="Fig.2" title="">                </div>                <div class="image-caption">Fig.2</div>            </figure><h1 id="5-思考"><a href="#5-思考" class="headerlink" title="5 思考"></a>5 思考</h1><ol><li><p>文章的创新点在于加入了一个回归残差作为惩罚项，并整合了多个降维算法。看这篇文章最大的收获，其一就是学会了一种利用残差来放宽约束的方法，可以避免过拟合非线性流形的训练数据。其二就是了解了各大知名的降维算法。论文有些地方的还没有读通透，需要再仔细研读一下。</p></li><li><p>对于迁移学习的研究的思考。</p><p> 我们一般假设源域和源域数据采样一个高维流形之中，那么一般的做法大多是对源域和目标域数据进行本文中所描述的“硬核”线性投影（$W^TX+b$），得到一个利于最终任务的子空间（当然其中需要有各种图拉普拉斯约束，MMD约束等）。也就是说我们得到的子空间其实是一个线性的空间（见Fig.1），对于投影后不在这个空间的新数据而言，约束太过严格了。那么我们能否加一个残差项来补足呢？也就是说我们最后学的映射是$F =W^TX+b+ F_0$觉得这一点很值得一试。</p></li><li><p>对于深度学习的思考。</p><p> 按照作者的理论，其实普通的全连接神经网络的每一层的输出相当于对输入做了一次投影$W^TX+b$，那么为了让输出有更好的表现力，利于最终任务，我们能否在每一层中加入一个残差项呢？这个残差项跟何凯明的Res-Net不一样。我们的目的是使得输出有更好非线性表现，也就是相当于一个激活函数的作用，重要的是这个激活函数不需人为指定。至于是否会出现过拟合，梯度爆炸和梯度消失，结合BN层会产生哪些“化学反应”等问题，还需仔细琢磨一下。这也不失为一种创新的方法。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx4.sinaimg.cn/wap690/007v1rTBgy1g1vekpd0kij31hc0qo7er.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://yvanzh.top/tags/Machine-Learning/"/>
    
      <category term="Manifold Learning" scheme="http://yvanzh.top/tags/Manifold-Learning/"/>
    
      <category term="Dimension Reduction" scheme="http://yvanzh.top/tags/Dimension-Reduction/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 之 Indigo 主题博客置顶</title>
    <link href="http://yvanzh.top/2019/04/08/Hexo-Top/"/>
    <id>http://yvanzh.top/2019/04/08/Hexo-Top/</id>
    <published>2019-04-08T06:11:11.000Z</published>
    <updated>2020-08-29T02:28:17.117Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>如何让文章C位出道~</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx2.sinaimg.cn/wap690/007v1rTBgy1g1v8lcaa7ij31hc123wrk.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="详情"><a href="#详情" class="headerlink" title="详情"></a>详情</h1><p>首先修改<code>/node_modules/hexo-generator-index/lib/generator.js</code>文件，添加置顶功能：</p><pre><code class="lang-js">&#39;use strict&#39;;var pagination = require(&#39;hexo-pagination&#39;);module.exports = function(locals){  var config = this.config;  var posts = locals.posts;    posts.data = posts.data.sort(function(a, b) {        if(a.top &amp;&amp; b.top) { // 两篇文章top都有定义            if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排            else return b.top - a.top; // 否则按照top值降序排        }        else if(a.top &amp;&amp; !b.top) { // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）            return -1;        }        else if(!a.top &amp;&amp; b.top) {            return 1;        }        else return b.date - a.date; // 都没定义按照文章日期降序排    });  var paginationDir = config.pagination_dir || &#39;page&#39;;  return pagination(&#39;&#39;, posts, {    perPage: config.index_generator.per_page,    layout: [&#39;index&#39;, &#39;archive&#39;],    format: paginationDir + &#39;/%d/&#39;,    data: {      __index: true    }  });};</code></pre><p>然后修改<code>/themes/indigo/layout/_partial/post.ejs</code>文件，给索引页面增加置顶样式：</p><pre><code class="lang-ejs">&lt;%- partial(&#39;post/toc&#39;, { post: post}) %&gt;&lt;article id=&quot;&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;&quot;  class=&quot;post-article article-type-&lt;%= post.layout %&gt; fade&quot; itemprop=&quot;blogPost&quot;&gt;    &lt;div class=&quot;post-card&quot;&gt;        &lt;h1 class=&quot;post-card-title&quot;&gt;&lt;%- post.title %&gt;&lt;/h1&gt;        &lt;div class=&quot;post-meta&quot;&gt;            &lt;%- partial(&#39;post/date&#39;, {date_format: config.date_format}) %&gt;            &lt;%- partial(&#39;post/category&#39;) %&gt;            &lt;% if (post.top&gt;0) { %&gt;                &lt;span&gt;&lt;icon class=&quot;icon icon-thumb-tack icon-pr&quot;&gt;&lt;/icon&gt;&lt;font color=&quot;ff4081&quot;&gt;置顶&lt;/font&gt;&lt;/span&gt;            &lt;% } %&gt;                        &lt;%- partial(&#39;plugins/page-visit&#39;) %&gt;        &lt;/div&gt;        &lt;div class=&quot;post-content&quot; id=&quot;post-content&quot; itemprop=&quot;postContent&quot;&gt;            &lt;%- post.content %&gt;        &lt;/div&gt;        &lt;%- partial(&#39;post/copyright&#39;) %&gt;        &lt;%- partial(&#39;post/reward-btn&#39;) %&gt;        &lt;div class=&quot;post-footer&quot;&gt;            &lt;%- partial(&#39;post/tag&#39;) %&gt;            &lt;%- partial(&#39;post/share-fab&#39;) %&gt;        &lt;/div&gt;    &lt;/div&gt;    &lt;%- partial(&#39;post/nav&#39;) %&gt;    &lt;%- partial(&#39;post/comment&#39;) %&gt;&lt;/article&gt;&lt;%- partial(&#39;post/reward&#39;) %&gt;</code></pre><p>之后修改<code>/themes/indigo/layout/_partial/index-item.ejs</code>文件，给正文页面增加置顶样式：</p><pre><code class="lang-ejs">&lt;article id=&quot;&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;&quot;  class=&quot;article-card article-type-&lt;%= post.layout %&gt;&quot; itemprop=&quot;blogPost&quot;&gt;    &lt;div class=&quot;post-meta&quot;&gt;        &lt;%- partial(&#39;post/date&#39;, {date_format: config.date_format}) %&gt;        &lt;%- partial(&#39;post/category&#39;) %&gt;        &lt;% if (post.top&gt;0) { %&gt;            &lt;span&gt;&lt;icon class=&quot;icon icon-thumb-tack icon-pr&quot;&gt;&lt;/icon&gt;&lt;font color=&quot;ff4081&quot;&gt;置顶&lt;/font&gt;&lt;/span&gt;        &lt;% } %&gt;    &lt;/div&gt;    &lt;%- partial(&#39;post/title&#39;, { hasLink: true }) %&gt;    &lt;div class=&quot;post-content&quot; id=&quot;post-content&quot; itemprop=&quot;postContent&quot;&gt;    &lt;% if(theme.excerpt_render) { %&gt;        &lt;%- post.excerpt || post.content %&gt;    &lt;% } else { %&gt;        &lt;%- post.excerpt ? strip_html(post.excerpt) : truncate(strip_html(post.content), {            length: theme.excerpt_length        }) %&gt;    &lt;% } %&gt;        &lt;a href=&quot;&lt;%- url_for(post.path) %&gt;&quot; class=&quot;post-more waves-effect waves-button&quot;&gt;            &lt;%= __(&#39;post.continue_reading&#39;) %&gt;        &lt;/a&gt;    &lt;/div&gt;    &lt;% if(post.tags &amp;&amp; post.tags.length){ %&gt;    &lt;div class=&quot;post-footer&quot;&gt;        &lt;%- partial(&#39;post/tag&#39;) %&gt;    &lt;/div&gt;    &lt;% } %&gt;&lt;/article&gt;</code></pre><p>最后大功告成，只需要在<code>.md</code>文件中加入<code>top</code>初始化参数，比如：</p><pre><code class="lang-markdown">title: Hexo置顶文章date: 1111-11-11 11:11:11tags: Hexocategories: Hexotop: 1</code></pre><ul><li>当top的值取0的时候，表示默认排序，即是按照时间顺序来排序。</li><li>当top的值取1到无穷大（最好取整数）的时候，值越高越靠前。</li></ul><blockquote><p>参考：<br><a href="https://chankin.tech/2018/09/23/sticky-post-sticky-problem/" target="_blank" rel="noopener">解决Hexo-theme-indigo的置顶问题</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;如何让文章C位出道~&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx2.sinaimg.cn/wap690/007v1rTBgy1g1v8lcaa7ij31hc123wrk.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yvanzh.top/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yvanzh.top/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Clustering and Projected Clustering with Adaptive Neighbors</title>
    <link href="http://yvanzh.top/2019/04/07/Paper-Notes-4/"/>
    <id>http://yvanzh.top/2019/04/07/Paper-Notes-4/</id>
    <published>2019-04-07T09:01:33.000Z</published>
    <updated>2020-08-29T02:28:17.120Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1g1nf4za90zj31hc0zkdn6.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><a href="https://dl.acm.org/citation.cfm?id=2623726" target="_blank" rel="noopener">文章来源</a>：2014-KDD</p><p>文章主旨：</p><ol><li>由于相似性度量和数据聚类分为两个步骤进行，可能会导致学习到的数据相似度不是最佳的。本文提出一个新框架让两者同时进行。</li><li>对拉普拉斯矩阵中的相似度矩阵进行秩约束，从而诱导聚类结构。</li><li>把模型扩展到投影聚类来处理高维数据。</li></ol><p>吐槽与膜拜：</p><p>这篇文章的符号设定简直反人类，且还有几处小错误。但是瑕不掩瑜，论文字字珠玑，最牛批的是直接让$K$-means和谱聚类沦为了跟班小弟。当时就WC了，小脑袋瓜都怎么长的，咋这好使。传统的机器学习确实是宝刀未老，14年的文章现在来看也毫不过时（也可能是我涉猎太少）。反观现在的深度学习研究感觉处在一个病态阶段，大都是在用体力劳动的结果说话，反复修改参数，堆砌部件，结果好就是老大，丝毫不管为什么好，为什么不好，模型的可解释性如何。不得不感叹，我们离真正的人工智能时代之间确实还是道阻路且长，在让机器学会如何学习之前，只能姑且被称为人力驱动的劳动密集型AI。在此之前，传统的机器学习还是应该受到重视的，因为深度学习的诱导和和驱动的源泉还是这些传统机器学习知识。一通认知浅薄的大话，不知不觉就扯远了，总之本文绝对是值得反复读的经典没错！</p><h1 id="2-模型框架"><a href="#2-模型框架" class="headerlink" title="2 模型框架"></a>2 模型框架</h1><ol><li><p>更小的距离应该被分配更大的邻近概率：</p><script type="math/tex; mode=display">\underset{\forall i,s_i^T \mathbf{1} = 1,0\le s_i \le 1}{\mathrm{min}} \sum_{i,j=1}^{n} (\Vert x_i - x_j \Vert_2^2 s_{ij} + \gamma s_{ij}^2) \tag{1}</script><p> 其中第二项相当于正则化项，表示在不考虑数据的距离信息情况下，每个数据点$x_i$与其他点的邻近概率趋向于$\frac{1}{n}$（算术平均数≤平方平均数），以此作为一个先验的邻近分配。</p><p> 进一步，将$d_{ij}^x = \Vert x_i - x_j \Vert_2^2$代入，则$(1)$式可转化为：</p><script type="math/tex; mode=display">\underset{\forall i,s_i^T \mathbf{1} = 1,0\le s_i \le 1}{\mathrm{min}} \Big\Vert s_i + \frac{1}{2\gamma}d_i^x \Big\Vert_2^2 \tag{2}</script></li><li><p>诱导连通分量为$c$个（即将数据聚为$c$簇）。</p><p> 将$(1)$式中得到的$S\in \mathbb{R}^{n\times n}$看作图节点的相似矩阵，每个节点$i$被赋值为$f_i \in \mathbb{R}^{1\times c}$，有：</p><script type="math/tex; mode=display">\sum^n_{i,j=1} \Vert f_i - f_j \Vert_2^2s_{ij} = 2Tr(F^TL_SF) \tag{3}</script><p> 若相似矩阵$S$为非负的，那么拉普拉斯矩阵满足一个重要性质：</p><blockquote><p>定理1<br>在拉普拉斯矩阵$L_S$中，特征根0的重数等于图的相似矩阵$S$的连通分量个数。</p></blockquote><p> 也就是说，我们可以通过约束$rank(L_S) = n-c$来得到很好的聚类结构。而不需要进行$K$-means或者其他离散化程序。因此，结合(1)式，得到：</p><script type="math/tex; mode=display"> \begin{gathered} J_{opt} = \underset{S}{\mathrm{min}} \sum_{i,j =1}^n(\Vert x_i - x_j\Vert_2^2s_{ij} + \gamma s_{ij}^2) \\ s.t. \ \ \ \ \forall i,s_i^T\mathbf{1}=1,0\le s_i \le 1,rank(L_S) = n - c \end{gathered} \tag{4}</script></li><li><p>对于$(4)$式的优化算法。</p><p> 设$\sigma_i(L_S)$是$L_S$最小的第$i$个特征值，且因为$L_S$为半正定的，$\sigma_i(L_S)\ge 0$。则可引入一个足够大的$\lambda$，将$(4)$式转化为：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S}{\mathrm{min}} \sum_{i,j =1}^n(\Vert x_i - x_j\Vert_2^2s_{ij} + \gamma s_{ij}^2) + 2\lambda \sum \sigma_i(L_S) \\ s.t. \ \ \ \ \forall i,s_i^T\mathbf{1}=1,0\le s_i \le 1 \end{gathered} \tag{5}</script><p> 根据<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1063126/?page=4" target="_blank" rel="noopener">Ky Fan定理</a>，我们可以得到：</p><script type="math/tex; mode=display"> \sum_{i=1}^c \sigma_i(L_S) = \underset{F \in \mathbb{R}^{n \times c},F^T F = I} {\min} Tr(F^T L_S F) \tag{6}</script><p> 将$(6)$式代入$(5)$式，有：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S,F}{\mathrm{min}} \sum_{i,j =1}^n(\Vert x_i - x_j\Vert_2^2s_{ij} + \gamma s_{ij}^2) + 2\lambda Tr(F^TL_SF) \\ s.t. \ \ \ \ \forall i,s_i^T\mathbf{1}=1,0\le s_i \le 1 ,F\in\mathbb{R}^{n\times c},F^TF =I  \end{gathered}\tag{7}</script><p> 这样一来，问题就简单了很多。我们可以先固定$S$，那么$(7)$式变成了：</p><script type="math/tex; mode=display">\underset{F \in \mathbb{R}^{n \times c},F^T F = I} {\min} Tr(F^T L_S F) \tag{8}</script><p> 我们对$(8)$式求的$F$其实就是$L_S$最小的$c$个特征值所对应的特征向量。然后固定$F$，那么$(7)$式就变成了：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S}{\mathrm{min}} \sum_{i,j =1}^n(\Vert x_i - x_j\Vert_2^2s_{ij} + \gamma s_{ij}^2) + 2\lambda Tr(F^TL_SF) \\ s.t. \ \ \ \ \forall i,s_i^T\mathbf{1}=1,0\le s_i \le 1     \end{gathered} \tag{9}</script><p> 根据$(3)$式，有：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S}{\mathrm{min}} \sum_{i,j =1}^n(\Vert x_i - x_j\Vert_2^2s_{ij} + \gamma s_{ij}^2 + 2\lambda \Vert f_i - f_j\Vert_2^2 s_{ij}) \\ s.t. \ \ \ \ \forall i,s_i^T\mathbf{1}=1,0\le s_i \le 1      \end{gathered}\tag{10}</script><p> 注意$d_{ij}= d^x_{ij} + \lambda d^f_{ij}$，其中$d_{ij}^x = \Vert x_i - x_j \Vert_2^2$，$d_{ij}^f = \Vert f_i - f_j \Vert_2^2$。根据$(2)$式，有：</p><script type="math/tex; mode=display">\underset{\forall i,s_i^T \mathbf{1} = 1,0\le s_i \le 1}{\mathrm{min}} \Big\Vert s_i + \frac{1}{2\gamma}d_i \Big\Vert_2^2 \tag{11}</script><p> 其中$s_i$和$d_i$分别为$S$和$d_{ij}$的第$i$列向量。</p><p> 综上，交替迭代$(8)$和$(11)$式更新$F$和$S$即可。</p></li></ol><h1 id="3-联系-K-means"><a href="#3-联系-K-means" class="headerlink" title="3 联系$K$-means"></a>3 联系$K$-means</h1><blockquote><p>引理1<br>$HD^xH =-2HXX^TH$</p></blockquote><p>证明：</p><p>中心矩阵定义为：</p><script type="math/tex; mode=display">H = I - \frac{1}{n}\mathbf{1}\mathbf{1}^T \tag{12}</script><p>由$d^x_{ij} =\Vert x_i -x_j \Vert_2^2 = x_i^Tx_i +x_j^Tx_j -2x_i^Tx_j$，可得：</p><script type="math/tex; mode=display">D^x = Diag(XX^T) \mathbf{1} \mathbf{1}^T + \mathbf{1}\mathbf{1}^TDiag(XX^T) - 2XX^T \tag{13}</script><p>同时注意$H\mathbf{1} = \mathbf{1}^TH = 0$，则对$(13)$式左乘右乘$H$，得证。</p><blockquote><p>定理2<br>当$\gamma \rightarrow \infty$时，$(4)式$等价于$K$-means问题。</p></blockquote><p>证明：</p><p>将$(4)$式写成矩阵形式：</p><script type="math/tex; mode=display">\underset{S\mathbf{1},S\ge0,rank(L_S)=n-c}{\mathrm{min}} Tr(S^TD^x) + \gamma \Vert S \Vert^2_F \tag{14}</script><p>由于$rank(L_S) = n- c$，那么解$S$有确切的$c$个连通分量，也就是$S$经过适当变换之后可以写成块对角的形式，例如：</p><script type="math/tex; mode=display">\left(\begin{matrix}    S_1 & 0 & 0\\    0 & S_2 & 0\\    0 & 0 & S_3\end{matrix}\right)</script><p>第$i$个连通分量$S_i \in \mathbb R^{n_i\times n_i}$，其中$n_i$为该连通分量中的数据个数。那么$(14)$式可转化为对每个连通分量$i$：</p><script type="math/tex; mode=display">\underset{S_i\mathbf{1},S_i\ge0}{\mathrm{min}} Tr(S_i^TD_i^x) + \gamma \Vert S_i \Vert^2_F \tag{15}</script><p>当$\gamma \rightarrow \infty$时，$(15)$式就转化为：</p><script type="math/tex; mode=display">\underset{S_i\mathbf{1},S_i\ge0}{\mathrm{min}} \gamma \Vert S_i \Vert^2_F \tag{16}</script><p>那么$(16)$的最优解就是$S_i$的所有元素都等于$\frac{1}{n_i}$。</p><p>因此，当$\gamma \rightarrow \infty$时，$(14)$式的最优解$S$应该是这样的：</p><script type="math/tex; mode=display">s_{ij}  =  \begin{cases}    \frac{1}{n_k} \ \ \ \ x_i,x_j \ \text{are in the same component }k\\    \ 0 \ \ \ \ \  \text{otherwise}\end{cases} \tag{17}</script><p>也就是说对于每一个最优的划分$\mathcal{V}$来说，总有$\Vert S \Vert_F^2 = c$。到了这里发现$(14)$式的第二项在后续的优化过程中是个常数，$(14)$式可写为：</p><script type="math/tex; mode=display">\underset{S\in \mathcal{V}}{\mathrm{min}} \ Tr(S^TD^x) \tag{18}</script><p>其中$S$为对称阵，且$\mathbf{1}^T S = \mathbf{1}^T$，再结合$(12)$式。可知：</p><script type="math/tex; mode=display">Tr(HD^xHS) = Tr(D^xS)- \frac{1}{n}\mathbf{1}^TD^x\mathbf{1} \tag{19}</script><p>那么根据$(19)$式，$(18)$式可写为：</p><script type="math/tex; mode=display">\underset{S\in \mathcal{V}}{\mathrm{min}} \ Tr(HD^xHS) \tag{20}</script><p>定义一个标签矩阵$Y \in \mathbb{R}^{n\times c}$，其中：</p><script type="math/tex; mode=display">y_{ij}  =  \begin{cases}    \frac{1}{\sqrt {n_k}} \ \ \ \ x_i \ \text{belongs to the }k\text{-th component}\\    \ \ 0 \ \ \ \ \ \ \text{otherwise}\end{cases} \tag{21}</script><p>结合$(20)(21)$式和引理1，可得：</p><script type="math/tex; mode=display">\begin{aligned}    &\underset{S\in \mathcal{V}}{\mathrm{min}} \ Tr(HD^xHS)  \\    &\Leftrightarrow \underset{S\in \mathcal{V}}{\mathrm{max}} \ Tr(HXX^THS) \\    &\Leftrightarrow \underset{S\in \mathcal{V}}{\mathrm{max}} \ Tr(X^THSHX) \\    &\Leftrightarrow \underset{S\in \mathcal{V}}{\mathrm{min}} \ Tr(X^TH(I-S)HX) \\    &\Leftrightarrow \underset{Y}{\mathrm{min}} \ Tr(X^TH(I-YY^T)HX) \\    &\Leftrightarrow \underset{Y}{\mathrm{min}} \ Tr(S_w) \end{aligned}\tag{22}</script><p>终于得证了当$\gamma \rightarrow \infty$时，这就是一个实打实的$K$-means问题。当标签$Y$已知时，$S_w$其实就是线性判别分析(LDA)中的类内散度矩阵。而$K$-means就是要找到一个最优的$Y$来使得$S_w$的迹最小。</p><p>值得注意的是，尽管$\gamma \rightarrow\infty$时，算法只能用来解决$K$-means问题（对数据进行球形分区），但是当$\gamma$不是很大的时候，还是能解决任意形状的分区问题的。牛批！</p><h1 id="4-联系谱聚类"><a href="#4-联系谱聚类" class="headerlink" title="4 联系谱聚类"></a>4 联系谱聚类</h1><p>当给定图的相似矩阵$S$时，谱聚类要解决的问题是：</p><script type="math/tex; mode=display">\underset{F \in \mathbb{R}^{n \times c},F^T F = I} {\min} Tr(F^T L_S F) \tag{23}</script><p>与$(8)$式一样，最优解$F$就是由拉普拉斯矩阵$L_S$中最小的$c$个特征值所对应的特征向量组成的。</p><p>通常，在给定$S$后求得的$F$并不能直接用来聚类，因为$S$没有给出明确的连通分量个数$c$。也就是需要对$F$进行$K$-means或其他离散化过程才能得到最终的聚类结果。</p><p>但是在本文的模型中，$F$和$S$是交替优化的。当最终收敛时，$(23)$式求出来$F$，$S$同时也被求得。并且得益于$rank(L_S) = n -c$这一约束，学得的$S$有明确的连通分量个数$c$。所以$F$直接就是聚类的结果，不需要像传统的谱聚类一样再对$F$聚类。</p><p>因此，最优解$F$可以被写为：</p><script type="math/tex; mode=display">F =YQ \tag{24}</script><p>其中$Y\in \mathbb{R}^{n \times c}$就是$(21)$式中定义的标签矩阵；$Q \in \mathbb{R}^{c \times c}$是任意的正交矩阵。意思是$rank(F) = c$，也就是说本文模型得到的$F$直接就是聚类的结果。</p><p>可以看到，传统的谱聚类只能在给定相似矩阵$S$时来求$F$，并且还需要对$F$再聚类才能得到结果。而本文算法不仅不需要给定$S$，还能生成一个自适应的$S$。厉害！</p><h1 id="5-确定-gamma-的值"><a href="#5-确定-gamma-的值" class="headerlink" title="5 确定$\gamma$的值"></a>5 确定$\gamma$的值</h1><p>众所周知，调好超参就等于成功了一半。在本文的模型中，$\gamma$恐怕是最难缠的了，其跨度是$[0,\infty)$，想想都头疼。于是作者做了下面的工作大大减少了死于调参的脑细胞。</p><p>对于$(7)$式中的$\gamma$，可以将$(7)$式等价于$(2)$式。$(2)$式的拉格朗日函数为：</p><script type="math/tex; mode=display">\mathcal{L}(s_i,\eta,\beta_i) = \frac{1}{2}\Big\Vert s_i + \frac{d_i^x}{2\gamma_i} \Big\Vert_2^2- \eta (s_i^T \mathbf{1} - 1) - \beta_i^Ts_i \tag{25}</script><p>其中$\eta,\beta_i \ge 0$为拉格朗日乘子 。根据$\mathrm{KKT}$条件，最优解$s_i$可被表示为：</p><script type="math/tex; mode=display">s_{ij}= \frac{-d_{ij}^x}{2\gamma_i}+\eta+ \beta_{ij} \tag{26}</script><p>通常关注数据的局部性可以得到更好的效果，最好就是学的一个稀疏的$s_i$，也就是只有$x_i$的$k$个最临近的点才有机会与$x_i$相连。稀疏的相似矩阵$S$另一个好处当然是可以缓解后续的计算压力。</p><p>不失一般性地假设$d_{i1}^x,d_{i2}^x,\dots,d_{in}^x$从小到大排列。如果最优解$s_i$仅有$k$个非零元素，那么根据$(26)$式，我们知道$s_{ik}&gt; 0$且$s_{i,k+1} = 0$。因此，可得到：</p><script type="math/tex; mode=display">\begin{cases}    -\frac{d_{ik}^x}{2\gamma_i} + \eta + \beta_{ij}> 0\\    -\frac{d_{i,k+1}^x}{2\gamma_i} +\eta + \beta_{ij}\le 0\end{cases} \tag{27}</script><p>再根据$(26)$式和$s_i^T\mathbf{1} = 1$，我们有：</p><script type="math/tex; mode=display">\begin{aligned}    &\sum_{j=1}^k (-\frac{d_{ij}^x}{2\gamma_i}+\eta+\beta_{ij}) = 1\\    &\Leftrightarrow \eta+\beta_{ij} = \frac{1}{k} +\frac{1}{2k\gamma_i}\sum_{j=1}^k d_{ij}^x \end{aligned}\tag{28}</script><p>因此对于$(2)$式，为了得到有$k$个非零元素的最优解$s_i$，根据$(27)(28)$式我们可以将$\gamma_i$设为：</p><script type="math/tex; mode=display">\begin{gathered}    \frac{k}{2} d_{i k}^{x}-\frac{1}{2} \sum_{j=1}^{k} d_{i j}^{x}<\gamma_{i} \leq \frac{k}{2} d_{i, k+1}^{x}-\frac{1}{2} \sum_{j=1}^{k} d_{i j}^{x}    \\    \gamma_i = \frac{k}{2}d_{i,k+1}^x -\frac{1}{2}\sum_{j=1}^k d_{ij}^x \end{gathered}\tag{29}</script><p>那么对于所有的$\gamma$，即$\gamma_1,\gamma_2,\dots,\gamma_n$，有：</p><script type="math/tex; mode=display">\gamma = \frac{1}{n}\sum_{i=1}^n \Big( \frac{k}{2}d_{i,k+1}^x - \frac{1}{2} \sum_{j=1}^k d_{ij}^x\Big) \tag{30}</script><p>这样对$\gamma$的调参就转化为了对邻近数$k$的调参，而$k$属于整数集且有直观的含义，效率无疑得到大幅提升。</p><h1 id="6-投影聚类"><a href="#6-投影聚类" class="headerlink" title="6 投影聚类"></a>6 投影聚类</h1><h2 id="6-1-模型框架"><a href="#6-1-模型框架" class="headerlink" title="6.1 模型框架"></a>6.1 模型框架</h2><ol><li><p>为了解决高维数据的聚类问题，作者想找到一个最优的投影子空间，使得利用本文的模型能让数据点在其中可以得到准确的$c$个连通分量结构。</p><p> 注意到总散度矩阵$S_t = X^THX$，其中$H$为$(12)$式定义的中心矩阵。假设我们要学得一个投影矩阵$W\in \mathbb{R}^{d\times m}$。首先我们需要约束子空间$W^TS_tW=I$，实际上就是保留了原空间的协方差，即保证在子空间中数据都是统计意义上不相关的（协方差矩阵其实就等于散度矩阵除以$n-1$）。其次，需要根据$(1)$式来分配近邻。所以要解决的问题可表示为：</p><script type="math/tex; mode=display"> \begin{gathered}     {\underset{S,W}{\min} \sum_{i,j=1}^n \left(\left\|W^{T} x_{i}-W^{T} x_{j}\right\|_{2}^{2} s_{i j}+\gamma s_{i j}^{2}\right)} \\     {s.t. \quad \forall i, s_{i}^{T} \mathbf{1}=1,0 \leq s_{i} \leq 1, W^{T} S_{t} W=I} \end{gathered} \tag{31}</script><p> 同样的，为了诱导$c$个连通分量，我们通过$rank(L_S) =n -c$来约束$S$，于是学习投影矩阵$W$和学习聚类可以同时进行：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S,W}{\min} \sum_{i,j=1}^n \left(\left\|W^{T} x_{i}-W^{T} x_{j}\right\|_{2}^{2} s_{i j}+\gamma s_{i j}^{2}\right) \\ s.t. \quad \forall i, s_{i}^{T} \mathbf{1}=1,0 \leq s_{i} \leq 1, W^{T} S_{t} W=I,rank(L_S)= n-c       \end{gathered} \tag{32}</script></li><li><p>对于$(32)$式的优化算法。</p><p> 采用$(5)(6)$式同样的trick将$(32)$式转化为：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S,W,F}{\min} \sum_{i,j=1}^n \left(\left\|W^{T} x_{i}-W^{T} x_{j}\right\|_{2}^{2} s_{i j}+\gamma s_{i j}^{2} + 2 \lambda Tr(F^TL_{S}F)\right) \\ s.t. \quad \forall i, s_{i}^{T} \mathbf{1}=1,0 \leq s_{i} \leq 1, W^{T} S_{t} W=I,F\in \mathbb{R}^{n \times c},F^TF = I     \end{gathered} \tag{33}</script><p> 首先固定$S$和$W$，则$F$由拉普拉斯矩阵$L_S$中$c$个最小特征值对应特征向量组成。</p><p> 然后就是固定$F$，$(33)$式变为：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S,W}{\min} \sum_{i,j=1}^n \left(\left\|W^{T} x_{i}-W^{T} x_{j}\right\|_{2}^{2} s_{i j}+\gamma s_{i j}^{2} + 2 \lambda Tr(F^TL_{S}F)\right) \\ s.t. \quad \forall i, s_{i}^{T} \mathbf{1}=1,0 \leq s_{i} \leq 1, W^{T} S_{t} W=I     \end{gathered} \tag{34}</script><p> 接着固定$S$，$(34)$式变为：</p><script type="math/tex; mode=display"> \underset{W^TS_tW=I}{\min} \sum_{i,j=1}^n \left\|W^{T} x_{i}-W^{T} x_{j}\right\|_{2}^{2} s_{i j}\tag{35}</script><p> 再根据$(3)$式，$(35)$式变为：</p><script type="math/tex; mode=display"> \underset{W^TS_tW=I}{\min} Tr(W^TX^TL_SXW)\tag{36}</script><p> 那么最优解$W$由$S_t^{-1}X^TL_SX$中最小的$m$个特征值所对应的特征向量组成。</p><p> 最后固定$W$，对于$(34)$式有：</p><script type="math/tex; mode=display"> \begin{gathered} \underset{S}{\min} \sum_{i,j=1}^n \left(\left\|W^{T} x_{i}-W^{T} x_{j}\right\|_{2}^{2} s_{i j}+\gamma s_{i j}^{2} \right)+ \lambda \sum_{i,j=1}^n \Vert f_i - f_j \Vert_2^2 s_{ij}\\ s.t. \quad \forall i, s_{i}^{T} \mathbf{1}=1,0 \leq s_{i} \leq 1     \end{gathered} \tag{37}</script><p> 注意到$(37)$式中所有$i$都是独立的。且有$d_{ij}^{w}= d^{wx}_{ij} + \lambda d^{f}_{ij}$，其中$d_{ij}^{wx} = \Vert W^Tx_i - W^Tx_j \Vert_2^2$，$d_{ij}^f = \Vert f_i - f_j \Vert_2^2$。$(37)$式可写为向量形式：</p><script type="math/tex; mode=display">\underset{s^T_i=1,0\le s_i \le 1}{\min}\Big\Vert s_i+\frac{1}{2\lambda d^w_i}\Big\Vert_2^2 \tag{38}</script><p> 综上，首先固定$S$和$W$更新$F$，然后固定$F$和$S$根据$(36)$式更新$W$，最后固定$F$和$W$根据$(38)$式更新$S$。</p></li></ol><h2 id="6-2-联系LDA"><a href="#6-2-联系LDA" class="headerlink" title="6.2 联系LDA"></a>6.2 联系LDA</h2><blockquote><p>定理3<br>当$\lambda \rightarrow \infty$时，$(32)$式等价于LDA问题，其中标签也是需要被优化的变量。</p></blockquote><p>证明：</p><p>$(32)$式写成矩阵的形式：</p><script type="math/tex; mode=display">\underset{S \mathbf{1} = \mathbf{1},S\ge0,W^TS_tW=I,rank(L_S)=n-c}{\min}Tr(S^TD^{wx}) +\gamma\Vert S \Vert_F^2 \tag{39}</script><p>同样最优解$S$由于$rank(L_S) = n -c$的缘故，可以变换后得到一个分块对角阵，对每一个连通分量$i$有：</p><script type="math/tex; mode=display">\underset{S \mathbf{1} = \mathbf{1},S\ge0,W^TS_tW=I}{\min}Tr(S_i^TD^{wx}) +\gamma\Vert S_i \Vert_F^2 \tag{40}</script><p>当$\lambda \rightarrow \infty$时，$(40)$式变成了：</p><script type="math/tex; mode=display">\underset{S_i\mathbf{1}= \mathbf{1},S_i \ge 0}{\min} \Vert S_i \Vert_F^2 \tag{41}</script><p>那么对于最优解$S$，其分块$S_i$的每一个元素都等于$\frac{1}{n_i}$。</p><p>也就说当$\lambda \rightarrow \infty$时，每一个满足最优解的划分$\mathcal{V}$，都有：</p><script type="math/tex; mode=display">\underset{S\in \mathcal{V},W^TS_tW=I}{\min} Tr(HD^wxHS) \tag{42}</script><p>最后结合$(42)$式和引理1，有：</p><script type="math/tex; mode=display">\begin{aligned}    &\underset{S\in \mathcal{V},W^TS_tW=I}{\mathrm{min}} \ Tr(HD^{wx}HS) \\    &\Leftrightarrow \underset{S\in \mathcal{V},W^TS_tW=I}{\mathrm{max}} \ Tr(HXWW^TX^THS) \\    &\Leftrightarrow \underset{S\in \mathcal{V},W^TS_tW=I}{\mathrm{max}} \ Tr(W^TX^THSHXW) \\    &\Leftrightarrow \underset{S\in \mathcal{V},W^TS_tW=I}{\mathrm{min}} \ Tr(W^TX^TH(I-S)HXW) \\    &\Leftrightarrow \underset{Y,W^TS_tW=I}{\mathrm{min}} \ Tr(W^TX^TH(I-YY^T)HXW) \\    &\Leftrightarrow \underset{Y,W^TS_tW=I}{\mathrm{min}} \ Tr(W^TS_wW) \end{aligned} \tag{43}</script><p>如果标签矩阵$Y$已知，那么$(43)$式就等价于LDA问题。因此当$\lambda \rightarrow \infty$时，本文的模型可以解决标签矩阵未知的LDA问题。</p><p>当$\gamma$不是很大时，$(36)$式中的矩阵$X^TL_SX$可以被视为局部类散度矩阵。在这种情况下，模型可以看作是基于局部散度矩阵的LDA方法的无监督版本，而LDA方法是为处理多模态非高斯数据而设计的。</p><h1 id="7-思考"><a href="#7-思考" class="headerlink" title="7 思考"></a>7 思考</h1><ol><li><p>先说一下读这篇论文的感受。一开始拿到论文，因为不熟悉的聚类领域，认真看了INTRODUCTION。然后就开始懵逼了，这么多公式，推理向的论文简直让人头大。可能是因为看西瓜书看怕了，最怕就是前面还是<code>1+1</code>，后面就变成了<code>@#￥%</code>。但是一路看下来，论文推导写的意外的流畅，看得舒爽无比。看完模型后其实没有感觉有多厉害，然而没有对比就没有伤害，后面作者把自己的模型联系到$K$-means和谱聚类，让二者直接成为了模型的一种特例时，简直激动到爆炸。膜拜！</p><p> 感慨万千，数学功底真的是无比重要，关键时刻旁征博引，各种定理引理张口就莱，打通了论文的任督二脉，狼人话不多，直接两开花。</p></li><li><p>关于均值不等式。</p><p> 如果$x_{1},x_{2},\ldots ,x_{n}$是正数，则有$H_n \le G_n \le A_n \le Q_n$，其中：</p><script type="math/tex; mode=display"> H_n = \dfrac{n}{\displaystyle \sum_{i=1}^{n} \dfrac{1}{x_i}} = \dfrac{n}{\dfrac{1}{x_1}+\dfrac{1}{x_2}+\cdots+\dfrac{1}{x_n}}</script><script type="math/tex; mode=display"> G_n=\sqrt[n]{\prod_{i=1}^nx_i}=\sqrt[n]{x_1x_2\cdots x_n}</script><script type="math/tex; mode=display"> A_n = \dfrac{\displaystyle \sum_{i=1}^{n} x_i }{n} = \dfrac{x_1+x_2+\cdots+x_n}{n}</script><script type="math/tex; mode=display"> Q_n = \sqrt{\dfrac{\displaystyle \sum_{i=1}^{n} x_i^2}{n}} = \sqrt{\dfrac{x^2_1+x^2_2+\cdots+x^2_n}{n}}</script><p> 当且仅当$x_{1}=x_{2}=\cdots =x_{n}$，等号成立。</p><p> 即对这些正数：调和平均数 ≤ 几何平均数 ≤ 算术平均数 ≤ 平方平均数</p></li><li><p>关于迹的运算。对于$A_{n\times n },B_{n\times n },C_{n\times n }$有：</p><ul><li>$Tr(AB) = Tr(BA)$</li><li>$Tr(ABC) = Tr(CAB) = Tr(BCA)$</li><li>$Tr(A) = Tr(A^T)$</li><li>$Tr(a) = a, a \in \mathcal{R}$</li><li>$\bigtriangledown_A Tr(AB) = B^T$</li><li>$\bigtriangledown_A Tr(A^TBA) = (B+B^T)A$</li><li>$\bigtriangledown_A T r(ABA^TC) = CAB + C^TAB^T$</li></ul></li><li><p>关于$\mathrm{KKT}$条件（详见李航《统计学习方法》附录C）。</p></li><li><p>可以利用算法到迁移学习中。比如谱聚类中，我们能否利用算法生成的自适应的$S$作为监督学习。</p></li><li><p>2019年4月3日 VALSE Webinar 中听了报告嘉宾的两篇论文：《Image Translation for Domain Adaptation and Generalization》、《Image Translation for Domain Adaptation and Generalization》。然后Panel嘉宾（龙明盛、段立新）的进行了一些讨论。</p><p> 简单规划一下读论文的方向：</p><ul><li>了解一下负迁移，样本迁移，参数迁移的相关论文。</li><li>了解迁移模型的选择工作、迁移模型的交叉验证。</li><li>侧重关注关系迁移，元学习的论文。</li><li>关注杨强教授的研究，读他的经典论文。</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1g1nf4za90zj31hc0zkdn6.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Clustering" scheme="http://yvanzh.top/tags/Clustering/"/>
    
      <category term="Machine Learning" scheme="http://yvanzh.top/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 之博客备份</title>
    <link href="http://yvanzh.top/2019/04/03/Hexo-Backup/"/>
    <id>http://yvanzh.top/2019/04/03/Hexo-Backup/</id>
    <published>2019-04-03T02:01:23.000Z</published>
    <updated>2020-08-29T02:28:17.109Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>再也不用担心博客迷路了！</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1g1pf4lhmajj31hc0zkk0y.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-机制与原理"><a href="#1-机制与原理" class="headerlink" title="1 机制与原理"></a>1 机制与原理</h1><p><code>Hexo</code>先在本地生成静态网页文件并暂存于<code>.deploy_git</code>文件夹中，然后部署时只把该文件夹<code>push</code>到远程仓库的<code>master</code>分支中。因此，我们可以在仓库中创建第二个分支来备份源文件。这样我们就能在利用<code>Git</code>的分支管理系统来进行多终端工作了。</p><h1 id="2-备份步骤"><a href="#2-备份步骤" class="headerlink" title="2 备份步骤"></a>2 备份步骤</h1><blockquote><p>2019年5月30日更新：由于GitHub的公开仓库才能免费使用静态网页，为避免源码泄露使用Coding仓库。</p></blockquote><ol><li>打开<code>Coding</code>中博客所在的仓库，新建一个分支，命名为<code>hexo</code>。</li><li>在仓库的<code>settings</code>的<code>Branches</code>选项中，把默认分支设为<code>hexo</code>。（因只需要对<code>hexo</code>分支进行手动操作，这样同步不需要指定分支，更加方便。）</li><li>在本地任意目录下，<code>git bash here</code>：<pre><code class="lang-bash">git clone git@git.dev.tencent.com:YvanZh/YvanZh.coding.me.git</code></pre></li><li>在克隆到本地的<code>YvanZh.coding.me</code>文件夹中，删除<code>.git</code>文件夹以外的所有文件。</li><li>将博客的源文件全部复制到<code>YvanZh.coding.me</code>文件夹中，除了<code>.deploy_git</code>。复制过来的源文件应该有一个<code>.gitignore</code>，用来忽略一些不需要的<code>git</code>的文件。如果没有的话，新建一个，内容如下：<pre><code class="lang-bash"> .DS_Store Thumbs.db db.json *.log node_modules/ public/ .deploy*/</code></pre></li><li>如果之前克隆主题文件，需要删除<code>theme</code>文件夹中的<code>.git</code>文件夹。</li><li>最后<code>git bash here</code>：<pre><code class="lang-bash"> git add . git commit -m &quot;add branch&quot; git push</code></pre></li></ol><p>这样就将博客源文件上传到了<code>hexo</code>分支，完成了备份。<br>PS：注意<code>.git</code>文件夹为隐藏文件夹，请务必开启隐藏文件可见。</p><h1 id="3-更换终端后"><a href="#3-更换终端后" class="headerlink" title="3 更换终端后"></a>3 更换终端后</h1><p><a href="https://yvanzh.top/2018/08/15/GitHub-Conding+Hexo/">搭建环境</a>的过程与之前无异：</p><ol><li>安装<code>Git</code>，并绑定全局用户名和邮箱，配置SSH密钥。</li><li>安装<code>Node.js</code>。</li><li>安装<code>Hexo</code>：<pre><code class="lang-bash"> npm install hexo-cli -g</code></pre></li><li>之后无需进行博客初始化，直接在任意文件夹下，<code>gti bash here</code>：<pre><code class="lang-bash"> git clone git@git.dev.tencent.com:YvanZh/YvanZh.coding.me.git</code></pre></li><li>然后<code>cd YvanZh.coding.me</code>，进入克隆到本地的文件夹：<pre><code class="lang-bash"> npm install npm install hexo-deployer-git --save hexo g -d</code></pre></li><li>终于可以开始写新博客了：<pre><code class="lang-bash"> hexo new newpage</code></pre></li><li>最好每次写完博客之后更新一下<code>hexo</code>分支:<pre><code class="lang-bash"> git add . git commit -m &quot;xxx&quot; git push</code></pre></li></ol><ul><li>咦~好像出现了一点不明真相的Bug（貌似是因为在第4步时没有按照SSH协议的方法来克隆仓库）导致<code>git push</code>需要输入密码，那么解决办法如下：<pre><code class="lang-bash">git remote set-url origin git@git.dev.tencent.com:YvanZh/YvanZh.coding.me.git</code></pre></li></ul><h1 id="4-结语"><a href="#4-结语" class="headerlink" title="4 结语"></a>4 结语</h1><p>颇费，美滋滋，滚去看论文了~</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;再也不用担心博客迷路了！&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1g1pf4lhmajj31hc0zkk0y.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yvanzh.top/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yvanzh.top/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>2019华为软件精英挑战赛</title>
    <link href="http://yvanzh.top/2019/04/01/HuaWei-CodeCraft2019/"/>
    <id>http://yvanzh.top/2019/04/01/HuaWei-CodeCraft2019/</id>
    <published>2019-04-01T06:01:33.000Z</published>
    <updated>2020-08-29T02:28:17.117Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>别问，问就惜败。</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1g2ckyxwhfjj31hc0zkjwm.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-总结"><a href="#1-总结" class="headerlink" title="1 总结"></a>1 总结</h1><p>研一上学期就获悉了华为比赛的信息，笔者作为本科期间多次参加数学建模竞赛的老菜鸟，其实还是有点想法的，再加上实验室的小伙伴（C++同学）的“勾搭”，直接就上了“贼船”。</p><p>2019年3月10日至29日为练习阶段，30日9:00~17:00为初赛阶段。三只小菜鸡不停啄米，走了不少弯路，最后一周通宵了两次。最后由于时间紧张，棋差一招，很遗憾，很不甘，来年再战！</p><p>接下来说说一些关键的时间点：</p><ul><li>10日，JAVA同学和C++同学分别开始构建底层数据结构。笔者开始读任务书，关注论坛。</li><li>17日，笔者写完调度系统的伪代码。</li><li>26日，JAVA同学实现了判题器，同时C++同学陷入调Bug自闭状态。</li><li>30日，最重要的优化没来得及实现，败北。</li></ul><p>下面是比赛的思路：</p><h1 id="2-调度系统"><a href="#2-调度系统" class="headerlink" title="2 调度系统"></a>2 调度系统</h1><p>话不多说，直接上伪代码：</p><pre><code class="lang-JAVA">for(T){    // 对车进行标记，终止状态为0，等待状态为1，可入库状态为2。ASC表示升序。    for(EachRoad){        1StateCarQueue_EachRoad = queue[]        0StateCarQueue_EachRoad = queue[]         // 是否把终止状态的车也顺便放在一个队列里面，这样可以省去调度时遍历的时间。        AheadCar_EachChannel(1:NumberOfChannel) = 0  //对每个Channel设一个变量        for((i = LenOfRoad):1:1){            for(EachChannel_ASC){                if(Road_Record(i) != 0){                    CarID = GetCarID()                    AheadCar_EachChannel = CarID                    V1 = min(Car_Speed, Road_Speed)                    if(AheadCar_EachChannel == 0){                        S1 = GetDistanceToAcrossRoad()                        V2 = min(Car_Speed, NextRoad_Speed)                        S2 = V2 - S1                        if(S2 &lt;= 0){                            S2 = 0                            Car_State = 0                            BehindCarState_EachChannel = 0                            if(S1 &lt; V1 &amp;&amp; RoadID == EndRoad){                                Car_State = 1                            }                        }                        else{                            Car_State = 1                            BehindCarState_EachChannel = 1                        }                    }                    else{                        S1 = GetDistanceToAheadCar()                        GetCarChannel                        if(BehindCarState_EachChannel == 1 &amp;&amp; S1 &lt; V1){                            Car_State = 1                        }                        else{                            Car_State = 0                            BehindCarState_EachChannel = 0                            /*                            这里可能需要找出由于前面阻挡的车为终止状态，而导致无法过                            路口的车。即 S2 &gt; 0 但是被前车所阻挡。后面这些车需要封锁这                            条路，然后更新一下地图，再算其最短路径。                            */                        }                    }                    if(Car_State == 1){                        1StateCarQueue_EachRoad = ArcossCarQueue_EachRoad.append(CarID)                    }                    else{                        0StateCarQueue_EachRoad = ArcossCarQueue_EachRoad.append(CarID)                     }                }            }        }     }    // 对道路内终止状态即Car_State == 0 的车进行调度。     for(EachRoad){        for(EachChannel){            for(LenOfRoad:1:1){                if(Car_State == 0){                    CarRun                }            }        }    }    // 对路口车辆调度。ASC表示升序。   TODO:需要加入一个到达终点车的判定。     for(EachCrossRoad_ASC){        CarCanAcross = 1        while(CarCanAcross){            for(EachRoad_ASC){                for(AcrossCarQueue_EachRoad){                    if(AcrossCarQueue_EachRoad == [] or CarCanAcross_EachRoad == 0){                        // CarCanAcross_EachRoad = 0                        break                    }                                        CarID = GetCarID()                    if(V1 - S1 &gt; 0 &amp;&amp; RoadID == EndRoad){                        CarOffline                    }                    if(S2 &lt;= 0){                        // CarRun                        // ChannelBlcok                          // AcrossCarQueue_EachRoad.pop()                        // 提取出被锁住的同channel的车，剩余队列继续进行路口调度                    }                    Car_Direction = GetCarDirection() // 1, 2, 3 分别表示 D, L, R                    NextRoadID = GetNextRoadID()                     if(NextRoadHaveSpace == 0){                        CarCanAcross_EachRoad = 0                        BlockedCar_EachRoad = AcrossCarQueue_EachRoad                        break                    }                    if(Car_Direction == 1){                        CarAcross   // CarCanAcross需要从小号车道开始进入                        AcrossCarQueue_EachRoad.pop()                        // 更新Both Roads                    }                    elseif(Car_Direction == 2){                        ConflictRoad = GetTurnLeftConflictRoad()                        if(AcrossCarQueue_ConflictRoad_FirstCar_Dierection == 1){                            break                        }                        CarAcross                        AcrossCarQueue_EachRoad.pop()                        /*                        (100,21,30,9,55)表示的是第100号路口，按顺时针方向道路为21                        , 30, 9, 55。此时本道路车辆左转前需要检查本道路的逆时针方向第                        一条道路有无直行车辆。有直行车则冲突，暂时结束对本道路的调度，                        转而调度下一道路。                        */                    }                    else{                        ConflictRoad1 = GetTurnRightConflictRoad1()                         ConflictRoad2 = GetTurnRightConflictRoad2()                        if(ArocssCarQueue_ConflictRoad1_FirstCar_Direction != 1 &amp;&amp; AcrossCarQueue_ConflictRoad2_FirstCar_Direction !=2){                            CarAcross                            AcrossCarQueue_EachRoad.pop()                        }                        /*                         本道路车右转前需要先检查本道路的顺时针方向第一条道路有无直行                        车辆，然后检查本道路顺时针方向第二条道路有无左转车辆。                        */                    }                }                if(sum(CarCanAcross_EachRoad) == 0){                    CarCanAcross = 0                    break                }            }        }    }    // 对出车库的车进行调度，如果没有空位，则延迟一个单位时间出发。    for(EachCrossRoad){        if(GarageHaveCar){            GetCarID()            GetStarTime()            if(StartTime &lt;= T &amp;&amp; MaxCar &lt; N &amp;&amp; RoadLoad &lt; n% ){                //若车辆达到计划出发时间，且所有道路总车辆小于N，且该道路负载程度小于n%。                //道路负载程度 = 道路车辆数/(道路长度*道路车道数)                if(RoadHaveSpace){                    CarOut //检索出第一条道路的起始列有无空当。有空当的话优先进入小号Channel。                }            }        }    }}</code></pre><h1 id="3-优化算法"><a href="#3-优化算法" class="headerlink" title="3 优化算法"></a>3 优化算法</h1><p>笔者之前犯了一个严重的错误——没有分析地图数据，而这直接导致了优化算法的思路出现了问题。我们一直纠结于如何排除死锁、优化路径组合和道路负载均衡。然而真正重要的一点，也是能在简单分析地图数据后就知道的——车辆的速度问题。</p><p>就地图数据来看，车辆的计划出发时间都在100个时刻之内，而实际的出发时间需要人为设定，这正是判题器的作用之一（输出车辆的实际出发时间）。我们再来看看系统调度总时间的量级，其实是在千位的。换句话说，即使考虑最极端的情况，我们让所有车辆都在第100个时刻之后出发，也影响不了太多总时间。再看车辆的速度其实只有2，4，6，8，10，12，15，16这七种，那么我们为何不分批次把所有同速度的车来进行调度呢？这样无疑去除了慢车对快车速度限制，同时间接地减少了死锁和道路负载过大的问题。这是我们30日上午幡然醒悟，然而时间有限，确实来不及实现了。</p><p>那么解题流程应该是这样：</p><ol><li>利用Floyd算法计算出每个路口节点的最短路径，并以此初始化所有车辆路径。</li><li>按照速度对所有车辆分批$P_1,\dots,P_7$。</li><li>对$P_1$车辆利用遗传算法优化调度系统的参数$MaxCar_1$和$RoadLoad_2$，并得到一个最优调度时间$T_1$。以此时间作为$P_2$车辆的计划出发时间。</li><li>依次对后续每批车辆进行第3步操作。</li><li>最终可以得到最优的系统总调度时间为$T = T_1 +\dots +T_7$，并输出每辆车的实际出发时间和最短路径。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;别问，问就惜败。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1g2ckyxwhfjj31hc0zkjwm.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Competition" scheme="http://yvanzh.top/categories/Competition/"/>
    
    
      <category term="Competition" scheme="http://yvanzh.top/tags/Competition/"/>
    
  </entry>
  
  <entry>
    <title>Generalized Zero-Shot Learning via Synthesized Examples</title>
    <link href="http://yvanzh.top/2019/03/29/Paper-Notes-3/"/>
    <id>http://yvanzh.top/2019/03/29/Paper-Notes-3/</id>
    <published>2019-03-29T09:01:33.000Z</published>
    <updated>2020-08-29T02:28:17.119Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1g1n1ma1eofj31hc0zkk9m.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><a href="https://arxiv.org/abs/1712.03878" target="_blank" rel="noopener">文章来源</a>：2018-CVPR</p><p>Tips：</p><ul><li>Normal Zero-Shot：测试集只有目标域数据（训练集与测试集不相交）。</li><li>Generalize Zero-Shot：测试集既有源域数据又有目标域数据（训练集与测试集相交）。</li></ul><p>文章主旨：</p><ul><li>在CVAE模型基础上加入一个针对生成样本的回归器，其将生成的样本映射到其对应的类属性上去。当模型优化完毕后，把不可见类的样本并纳入最终分类器的训练中，以平衡在GZSL中对可见类和不可见类的预测偏差。</li></ul><h1 id="2-详情"><a href="#2-详情" class="headerlink" title="2 详情"></a>2 详情</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx4.sinaimg.cn/wap690/007v1rTBgy1g1n1nqlja8j30vp0dgabk.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li>模型分析：<ul><li>假设隐变量与类属性是分离的(隐变量$z_n$表示$x_n$的非结构化部分，类属性向量$a_{y_n}$表示类特定的判别信息)，也就是说$x_n$受到这两者的共同影响；并且在类属性向量的指导下，有助于生成本质上具有更好区分度的样本。</li><li>包含一个多元回归器，其利用解码器的输出$\hat{x}_n$作为输入，映射到类属性向量$a_{y_n}$。它有两点重要作用：<ul><li>可以对生成器产生反馈，使其生成更好可区分性的样本。</li><li>可以通过计算类属性向量上的概率分布$p(a | x)$来使用无标签的样本进行训练(即进行半监督学习)。</li></ul></li><li>确保生成器的输出$\hat{x}$与实际输入$x$相似，比如可以用$p(z | \hat{x})$来近似$p(z | x)$。</li></ul></li><li>最终分类器：<ul><li>当生成模型训练好之后，就可以生成带标签的各类样本。首先，需要按照先验分布$p(z)$(一般对于二值型数据采用伯努利分布，对于连续型数据采用高斯分布)来随机生成非结构化部分即隐变量$z$，并指定需生成的类别$c$(通过类属性向量$a_c$)。然后就可以通过生成器来生成样本$x$。</li><li>当每个类别都生成了固定数量的样本之后，就可以利用它们来训练分类器(比如，SVM或softmax分类器)。由于这个阶段使用了来自可见类和不可见类的带标签样本，所以结果具有鲁棒性，可以减少对可见类的偏见。需要注意的是：训练分类器时，可以只使用来自可见类的原始标签样本，也可以用模型生成的可见类的附加样本进行扩展。</li></ul></li><li>多元回归器：<ul><li>对于可见类的样本$\{x_n,a_{y_n}\}_{n=1}^{N_s}$，定义监督损失：<script type="math/tex; mode=display">\mathcal{L}_{Sup}(\theta_R) = -\mathbb{E}_{ \{x_n,{a_{y_n}}\} } [p_R(a_{y_n} | x_n)] \tag{1}</script></li><li>对于由生成器生成的样本$\hat{x}$，定义非监督损失：<script type="math/tex; mode=display">\mathcal{L}_{Unsup}(\theta_R) = -\mathbb{E}_{p_{ {\theta}_G} (\hat{\mathbf{x}}| \mathbf{z,a})p(\mathbf{z})p(\mathbf{a})}[p_R(\mathbf{a} | \mathbf{\hat{x}})]\tag{2}</script></li><li>由(1)(2)式，得到损失函数：<script type="math/tex; mode=display">\underset{\theta_R} \ \mathcal{L}_R = \mathcal{L}_{Sup} + \lambda_R  \centerdot \mathcal{L}_{Unsup}\tag{3}</script></li><li>Tips：使用回归器通过反向传播来改进生成器。优化$\theta_R$时需固定生成器的分布（即固定$\theta_G$）。</li></ul></li><li>编码器和条件生成器：<ul><li>对于编码器$p_E(z|x)$和条件生成器$p_G(x|z,a)$，VAE损失函数为：<script type="math/tex; mode=display">\mathcal{L}_{VAE}(\theta_E,\theta_G) = -\mathbb{E}_{p_E(\mathbf{z}|\mathbf{x}),p(\mathbf{a}|\mathbf{x}))}[\log p_G(\mathbf{x}|\mathbf{z,a})] + \mathrm{KL}(p_E(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))\tag{4}</script>  其中第一项表示在希尔伯特再生空间中生成器的重构误差；第二项表示后验分布(编码器)需要与先验分布一致。</li></ul></li><li>设编码器$p_E(z|x)$、生成器$p_G(x|z,a)$和回归器$p_R(a|x)$都服从高斯分布。</li><li>回归器驱动的学习：<ul><li>确保$\mathbf{\hat{x}}$映射到$\mathbf{a}$，即保证回归的正确性（生成器要报警了！回归结果不好，回归器不背锅居然怪我？其实这是个$\theta_R$和$\theta_G$交替优化的过程，你一拳我一拳让模型变得更加漂亮~）：<script type="math/tex; mode=display">\mathcal{L}_c(\theta_G) = -\mathbb{E}_{p_G(\mathbf{\hat{x}}|\mathbf{z,a})p(\mathbf{z})p(\mathbf{a})}[\log p_R(\mathbf{a}|\mathbf{\hat{x}})] \tag{5}</script></li><li>确保$z$和$a$结合可以通过生成器得到对应样本（作为正则化项）：<script type="math/tex; mode=display">\mathcal{L}_{Reg}(\theta_G) = -\mathbb{E}_{p(\mathbf{z})p(\mathbf{a})}[\log p_G(\mathbf{\hat{x}}|\mathbf{z,a})] \tag{6}</script></li><li>$(5)(6)$式共同作用让$\hat{x}\backsim p_G(\mathbf{\hat{x}}|\mathbf{z,a})$。但还有一个最重要的点是确保$z$和$a$的独立性。为此，使用编码器来确保样本抽样分布和从生成的样本中得到的抽样分布遵循相同的分布：<script type="math/tex; mode=display">\mathcal{L}_E(\theta_G) = -\mathbb{E}_{\hat{x}\backsim p_G(\mathbf{\hat{x}}|\mathbf{z,a})}\mathrm{KL}[(p_E(\mathbf{z}|\mathbf{\hat{x}})||q\mathbf{(z)})] \tag{7}</script></li><li>由$(4)(5)(6)(7)$式，得到损失函数：<script type="math/tex; mode=display">\underset{\theta_G,\theta_E}{\mathrm{min}} \ \mathcal{L}_{VAE} + \lambda_c \centerdot \mathcal{L}_c + \lambda_{reg} \centerdot \mathcal{L}_{Reg} + \lambda_E \centerdot \mathcal{L}_E \tag{8}</script></li></ul></li><li>综上，通过$(3)(8)$式交替优化$\theta_R$和$\theta_G$，得到最终的生成模型后，利用其生成的样本来训练最终的分类器。</li></ol><h1 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h1><ol><li><p>笔者之前就认为VAE框架本身的局限之处在于隐变量的先验分布，为了简单方便计算一般采用伯努利分布或标准正太分布。然而这无疑限制了模型的表达能力。在本文章中通过分离隐变量$z$和类属性向量$a$，并引入一个将生成的样本$\hat{x}$映射到类属性向量$a$的多元回归器。既利用了VAE的生成能力，又增加了一个学习诱导，这样不失为一种好办法。</p></li><li><p>那么我们能不能从本质上改进VAE呢？VAE框架的根源其实说到底就是KL散度。KL散度可以度量两个概率分布的差异，但是我们甚至不能称其为距离，因为它不满足对称性。然而它的优势在于可以写成期望的形式，也就是可直接进行采样计算。$\mathrm{KL}(p(x)||q(x))$有一个比较明显的问题，就是当$q(x)$在某个区域等于0，而$p(x)$在该区域不等于0，那么KL散度就出现无穷大。这是KL散度的固有问题，我们只能想办法规避它，比如隐变量的先验分布用高斯分布而不是均匀分布。Idea is cheap，留待实践。 </p></li><li><p>VAE理论框架：</p><script type="math/tex; mode=display"> \begin{array}{c|c}  \hline  x_k, z_k & \text{表示随机变量}x,z\text{的第}k\text{个样本}\\  \hline  x_{(k)}, z_{(k)} & \text{表示多元变量}x,z\text{的第}k\text{个分量}\\  \hline  \mathbb{E}_{x\sim p(x)}[f(x)] & \text{表示对}f(x)\text{算期望，其中}x\text{的分布为}p(x)\\  \hline  \mathrm{KL}(p(x)\Vert q(x))& \text{两个分布的}KL\text{散度}\\  \hline  \Vert x\Vert^2& \text{向量}x\text{的}l^2\text{范数}\\  \hline  \mathcal{L}& \text{本文的损失函数的符号}\\  \hline  D,d & D\text{是输入}x\text{的维度，}d\text{是隐变量}z\text{的维度}\\  \hline  \end{array}</script><p> 数据样本$\{x_1,\dots,x_n\}$，其整体用$x$来描述，我们希望借助隐变量$z$描述$x$的分布$\tilde{p}(x)$：</p><script type="math/tex; mode=display">q(x)=\int q(x|z)q(z)dz,\quad q(x,z) = q(x|z)q(z)\tag{9}</script><p> 其中$q(z)$是先验分布（标准正态分布），目的是希望$q(x)$能逼近$\tilde{p}(x)$。这样（理论上）我们既描述了$p~(x)$，又得到了生成模型$q(x|z)$，一举两得。</p><p> 接下来就是利用KL散度进行近似。很多教程推导都是聚焦于后验分布$p(z|x)$。但事实上，直接来对$p(x,z)$进行近似是最为干脆的。具体来说，定义$p(x,z) = \tilde{p}(x)p(z|x)$，设想用一个联合概率分布$q(x,z)$来逼近$p(x,z)$，那么用KL散度来看它们的距离：</p><script type="math/tex; mode=display">KL\Big(p(x,z)\Big\Vert q(x,z)\Big) = \iint p(x,z)\ln \frac{p(x,z)}{q(x,z)} dzdx  \tag{10}</script><p> KL散度是我们的终极目标，因为我们希望两个分布越接近越好，所以KL散度越小越好。于是有：</p><script type="math/tex; mode=display">\begin{aligned}KL\Big(p(x,z)\Big\Vert q(x,z)\Big) =& \int \tilde{p}(x) \left[\int p(z|x)\ln \frac{\tilde{p}(x)p(z|x)}{q(x,z)} dz\right]dx\\  =& \mathbb{E}_{x\sim \tilde{p}(x)} \left[\int p(z|x)\ln \frac{\tilde{p}(x)p(z|x)}{q(x,z)} dz\right]  \end{aligned} \tag{11}</script><p> 由$\ln \frac{\tilde{p}(x)p(z|x)}{q(x,z)}=\ln \tilde{p}(x) + \ln \frac{p(z|x)}{q(x,z)}$，我们进一步对$(11)$进行简化：</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{x\sim \tilde{p}(x)} \left[\int p(z|x)\ln \tilde{p}(x)dz\right] =& \mathbb{E}_{x\sim \tilde{p}(x)} \left[\ln \tilde{p}(x)\int p(z|x)dz\right]\\  =&\mathbb{E}_{x\sim \tilde{p}(x)} \big[\ln \tilde{p}(x)\big]  \end{aligned}\tag{12}</script><p> 其中的$\tilde{p}(x)$是根据样本$x_1,x_2,\dots,x_n$确定的关于x的先验分布，尽管我们不一定能准确写出它的形式，但它是确定的、存在的，因此这一项只是一个常数，所以可以写出：</p><script type="math/tex; mode=display">\mathcal{L}=KL\Big(p(x,z)\Big\Vert q(x,z)\Big) - \text{常数}= \mathbb{E}_{x\sim \tilde{p}(x)} \left[\int p(z|x)\ln \frac{p(z|x)}{q(x,z)} dz\right]\tag{13}</script><p> 最后为了得到生成模型，所以我们把$q(x,z)$写成$q(x|z)q(z)$，于是就有:</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L} =& \mathbb{E}_{x\sim \tilde{p}(x)} \left[\int p(z|x)\ln \frac{p(z|x)}{q(x|z)q(z)} dz\right]\\  =&\mathbb{E}_{x\sim \tilde{p}(x)} \left[-\int p(z|x)\ln q(x|z)dz+\int p(z|x)\ln \frac{p(z|x)}{q(z)}dz\right]\end{aligned}\tag{14}</script><p> 简化后就是：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L} = &\mathbb{E}_{x\sim \tilde{p}(x)} \left[\mathbb{E}_{z\sim p(z|x)}\big[-\ln q(x|z)\big]+\mathbb{E}_{z\sim p(z|x)}\Big[\ln \frac{p(z|x)}{q(z)}\Big]\right]\\  = &\mathbb{E}_{x\sim \tilde{p}(x)} \Bigg[\mathbb{E}_{z\sim p(z|x)}\big[-\ln q(x|z)\big]+KL\Big(p(z|x)\Big\Vert q(z)\Big)\Bigg]  \end{aligned}\tag{15}</script><p> 中括号内的即为VAE的损失函数。</p></li></ol><blockquote><p>参考：<br><a href="https://spaces.ac.cn/archives/5343" target="_blank" rel="noopener">变分自编码器（二）：从贝叶斯观点出发</a> </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1g1n1ma1eofj31hc0zkk9m.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Transfer Learning" scheme="http://yvanzh.top/tags/Transfer-Learning/"/>
    
      <category term="Deep Learning" scheme="http://yvanzh.top/tags/Deep-Learning/"/>
    
      <category term="Zero-Shot" scheme="http://yvanzh.top/tags/Zero-Shot/"/>
    
  </entry>
  
  <entry>
    <title>高级计算机体系结构</title>
    <link href="http://yvanzh.top/2018/12/03/Advanced-Computer-Architecture/"/>
    <id>http://yvanzh.top/2018/12/03/Advanced-Computer-Architecture/</id>
    <published>2018-12-03T13:11:11.000Z</published>
    <updated>2020-08-29T02:28:17.107Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>计算机的设计思想、设计原理、设计技术。</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxtwrdfxyoj31hc0zgws0.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-知识要点"><a href="#1-知识要点" class="headerlink" title="1 知识要点"></a>1 知识要点</h1><h2 id="1-1-冯式结构的五个组成部分"><a href="#1-1-冯式结构的五个组成部分" class="headerlink" title="1.1 冯式结构的五个组成部分"></a>1.1 冯式结构的五个组成部分</h2><p>运算器、控制器、存储器、输入设备、输出设备。</p><h2 id="1-2-CPU-发展趋势"><a href="#1-2-CPU-发展趋势" class="headerlink" title="1.2 CPU 发展趋势"></a>1.2 CPU 发展趋势</h2><p>性能计算：</p><p>$CPUTime = \frac{Seconds}{Program} = \frac{Instruction}{Program} \cdot \frac{Cycles}{Instruction} \cdot \frac{Seconds}{Cycles}$</p><p>性能提升：</p><ol><li>半导体技术：<ul><li>特征尺寸。</li><li>时钟频率。</li></ul></li><li>计算机体系结构：<ul><li>高级语言编译器、标准化的操作系统。</li><li>指令更为简单的RISC(精简指令集)结构。</li></ul></li><li>多核技术出现的原因<ul><li>通过堆积晶体管密度提高性能遇到瓶颈</li><li>硬件多线程的并发已经存在一段时间了。虽然可以减少内存延迟，但是只有30%的提升</li><li>如今将多个处理内核封装在一个芯片中来提升性能</li></ul></li><li>两个1G多核处理器 VS 一个2G单核处理器<ul><li>前者在同时处理两个小于1G的任务时比后者要快</li><li>后者在处理一个大于1G小于2G的任务时比前者要快</li><li>后者成本要低</li></ul></li><li>编程模型：SIMD VS MIMD。<ul><li>SIMD发掘数据级并行<ul><li>矩阵运算</li><li>图像和声音处理</li></ul></li><li>SIMD比MIMD能耗效率高</li><li>SIMD数据操作只需要取一条指令</li><li>SIMD对个人移动设备(PMD)具有吸引力；MIMD一般用于商用计算机和大型服务器。</li><li>SIMD编程者对并行思维要求较低；MIMD反之</li></ul></li><li>虚拟化技术的意义<ul><li>CPU的虚拟化技术可以单CPU模拟多CPU并行，允许一个平台同时运行多个操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率</li></ul></li></ol><h2 id="1-3-Flynn’s-分类"><a href="#1-3-Flynn’s-分类" class="headerlink" title="1.3 Flynn’s 分类"></a>1.3 Flynn’s 分类</h2><ol><li>单指令流、单数据流(SISD)—{指令级并行(ILP)}<ul><li>解释：传统的顺序执行的单处理器计算机，其指令部件每次只对一条指令进行译码，并且只对一个操作部分分配数据。</li><li>例子：Intel Pentium4</li></ul></li><li>单指令流、多数据流(SIMD)—{数据级并行(DLP)}<ul><li>解释：以同步方式，在同一时间执行同一条指令。</li><li>例子：向量体系结构、多媒体指令扩展、GPU</li></ul></li><li>多指令流、多数据流(MIMD)<ul><li>解释：使用多个控制器来异步地控制多个处理器，从而实现空间上的并行性。</li><li>例子：Intel Xeon e5345(Clovertown)</li><li>紧耦合：MIMD；松耦合：MIPD</li></ul></li><li>多指令流、单数据流(MISD)<ul><li>没有实现商业化(why?)：显然多条指令处理同一数据可能频繁出现冲突，处理效率非常低。</li></ul></li></ol><h2 id="1-4-网络和存储技术"><a href="#1-4-网络和存储技术" class="headerlink" title="1.4 网络和存储技术"></a>1.4 网络和存储技术</h2><div class="table-container"><table><thead><tr><th>网络技术</th><th>速度</th><th>存储接口</th><th>速度</th></tr></thead><tbody><tr><td>WAN T1</td><td>56K/s</td><td>SCSI</td><td>5MB/s</td></tr><tr><td>以太网</td><td>10Mb/s</td><td>Fast SCSI</td><td>10MB/s</td></tr><tr><td>快速以太网</td><td>100Mb/s</td><td>WideSCSI</td><td>20MB/s</td></tr><tr><td>ATM/OC-3</td><td>155Mb/s</td><td>Ultra SCSI</td><td>40MB/s</td></tr><tr><td>千兆以太网</td><td>1Gb/s</td><td>光纤路径</td><td>2Gb/s</td></tr><tr><td>Infiniband</td><td>40Gb/s</td><td>光纤路径</td><td>4 or 8Gb/s</td></tr></tbody></table></div><p>网络比存储发展快！</p><h2 id="1-5-内存系统"><a href="#1-5-内存系统" class="headerlink" title="1.5 内存系统"></a>1.5 内存系统</h2><p>存储技术的目标：最大，最快，最便宜。</p><p>技术趋势：</p><ul><li>集成电路<ul><li>晶体管密度：25% year</li><li>晶片尺寸：10-20% year</li><li>晶体管数目：40-55% year</li></ul></li><li>DRAM容量：单个25-40% year (slowing)</li><li>闪存容量：50-60% year<ul><li>15-20X cheaper/bit than DRAM</li></ul></li><li>磁盘技术：40% year<ul><li>15-25X cheaper/bit than Flash</li><li>300-500X cheaper/bit than DRAM</li></ul></li></ul><p>局部性原理：</p><ul><li>时间局部性：程序结构中的循环</li><li>空间局部性：数组或记录</li></ul><p>层次：SRAM，DRAM，DISK，FLASH MEMORY</p><ul><li>顶层{CPU内}：寄存器、Cache</li><li>二层{主板内}：主存、Cache</li><li>三层{主板外}：磁盘、光盘</li><li>底层{离线}：磁带</li></ul><h2 id="1-6-Cache的使用问题"><a href="#1-6-Cache的使用问题" class="headerlink" title="1.6 Cache的使用问题"></a>1.6 Cache的使用问题</h2><p>Cache：</p><ul><li>高速缓存，也指基于局部性原理来管理的存储器。</li></ul><p>使用Cache的问题：</p><ol><li>如果要访问的数据不在Cache中怎么办？<ul><li>要访问的数据不在Cache中会导致缺失，缺失则将需要的数据装入Cache中。</li></ul></li><li>从主存中装入数据时装到Cache中的什么位置？<ul><li>直接映射<ul><li>每个主存地址对应到Cache的确定的位置</li></ul></li><li>全相联映射<ul><li>一个块可以放在Cache中的任何位置</li><li>需要检索Cache中的所有项：并行比较器</li></ul></li><li>组相联映射<ul><li>每个块有n个位置可放的Cache称为n路组相联Cache;</li><li>存储器中的一个块对应到Cache中唯一的组，但是可以放在组内的任意位置上</li></ul></li></ul></li><li>从主存中装入数据时一次装入多少数据？<ul><li>从cache到处理器：以字为单位；</li><li>从主存到cache：以块为单位；块设为多大？<ul><li>4K &amp; 64块；16K &amp; 64块；64K &amp; 256块；256K &amp; 256块。</li></ul></li></ul></li><li>如何判断Cache中对应的位置是否为有效的数据？<ul><li>有效位，有效位设置时表示一个块是有效的</li></ul></li><li>如果Cache装满了怎么办？<ul><li>先入先出法(FIFO)</li><li>随机替换法(RAND)</li><li>最近最少使用法(LRU)<ul><li>LRU算法实现：使用链表和hashmap。当需要插入新的数据项的时候，如果新数据项在链表中存在（一般称为命中），则把该节点移到链表头部，如果不存在，则新建一个节点，放到链表头部，若缓存满了，则把链表最后一个节点删除即可。在访问数据的时候，如果数据项在链表中存在，则把该节点移到链表头部，否则返回-1。这样一来在链表尾部的节点就是最近最久未访问的数据项。</li></ul></li></ul></li><li>写Cache时会有什么问题？<ul><li>写命中和写不命中</li></ul><ol><li>写回法<ul><li>写命中：只修改Cache内容，不立即写入主存，当被替换时才写入主存。</li><li>写不命中：将内存中包含欲写块的内容装入Cache中并修改。</li></ul></li><li>写直达法<ul><li>写命中：同时修改Cache和主存。</li><li>写不命中：直接写入主存。</li></ul></li><li>写一次法<ul><li>与写回法基本相同，只是第一次写命中时同时修改Cache和主存。</li></ul></li></ol></li></ol><h2 id="1-7-Cache的优化方法"><a href="#1-7-Cache的优化方法" class="headerlink" title="1.7 Cache的优化方法"></a>1.7 Cache的优化方法</h2><p>访问缺失：</p><ul><li>强制缺失(第一次访问)</li><li>容量缺失(因容量不够，块被移除，又被访问)</li><li>冲突失效(重复访问的多个地址映射在Cache中的同一位置)</li></ul><p>$AMAT = \text{命中时间} + \text{缺失率} * \text{缺失代价}$</p><ol><li><strong>更大的块</strong><ul><li>强制缺失减少</li><li>容量和冲突缺失增加，缺失代价增加</li></ul></li><li><strong>更大的Cache容量</strong><ul><li>缺失率降低</li><li>命中时间，功耗增加</li></ul></li><li><strong>更高的相关度</strong><ul><li>冲突缺失减少</li><li>命中时间增加，功耗增加</li></ul></li><li><strong>更多级Cache</strong><ul><li>内存访问时间减少</li></ul></li><li><strong>读缺失优先级更高</strong><ul><li>缺失代价降低</li></ul></li><li><strong>缓存索引避免地址转移</strong><ul><li>减少命中时间</li></ul></li></ol><p>优化度量：命中时间，缺失率，缺失代价，缓存带宽和功耗。</p><h2 id="1-8-存储访问模型"><a href="#1-8-存储访问模型" class="headerlink" title="1.8 存储访问模型"></a>1.8 存储访问模型</h2><ul><li><strong>UMA</strong>(uniform memory access)模型<ul><li>物理存储器被所有节点共享</li><li>所有节点访问任意存储单元的访问时间相同</li><li>发生访存竞争时，仲裁策略平等对待每个节点</li><li>每个节点的CPU可带有局部私有的高速缓存</li><li>外围I/O设备也可以共享，且每个节点有平等的访问权力</li></ul></li><li><strong>NUMA</strong>(Non-Uniform Memory Access)模型<ul><li>物理存储器被所有节点共享，任意节点可以直接访问任意内存模块</li><li>节点访问内存模块的速度不同，访问本地存储模块的速度一般是访问其他节点内存模块的3倍以上</li><li>发生访存竞争时，仲裁策略对节点可能是不平等的</li><li>各节点的CPU可带有局部私有高速缓存cache</li><li>外围I/O设备也可以共享，但对各节点是不平等的</li></ul></li></ul><h2 id="1-9-单CPU上常见的提升性能的方法和并行计算"><a href="#1-9-单CPU上常见的提升性能的方法和并行计算" class="headerlink" title="1.9 单CPU上常见的提升性能的方法和并行计算"></a>1.9 单CPU上常见的提升性能的方法和并行计算</h2><p>提升性能方法：</p><ul><li>提高单个处理器的工作频率</li><li>Locality：L1/L2/L3 Cache</li><li>多级流水线(提高CPU频率的利器)</li><li>超标量执行(多条流水线并同时发送多条指令)</li><li>乱序执行(指令重排)</li><li>单指令流多数据SIMD</li><li>超长指令字处理器(依赖于编译器分析)</li></ul><p>单CPU主要的并行：指令级并行(ILP)</p><h2 id="1-10-多线程的好处"><a href="#1-10-多线程的好处" class="headerlink" title="1.10 多线程的好处"></a>1.10 多线程的好处</h2><ul><li>创建一个线程比创建一个进程的代价小</li><li>线程的切换比进程间切换的代价小</li><li>充分利用多处理器</li><li>数据共享</li><li>快速响应特性</li></ul><h2 id="1-11-Tomasulo’s-算法"><a href="#1-11-Tomasulo’s-算法" class="headerlink" title="1.11 Tomasulo’s 算法"></a>1.11 Tomasulo’s 算法</h2><p>三步骤:</p><ul><li><strong>发射</strong><ul><li>从指令队列中获取指令，如果RS(Registers)可用，则发射指令到RS；</li><li>如果操作数可用，则发送数据至RS；</li><li>发射级完成了重命名。</li></ul></li><li><strong>执行</strong><ul><li>若有一个或几个操作数未就绪，等待该操作数，并同时监控CDB(CommonDataBus)；</li><li>当操作数可用，则存储至保留站；当所有操作数可用，则执行命令；</li><li>执行级检查了是否存在RAM竞争。</li></ul></li><li><strong>写结果</strong><ul><li>将结果写入CDB，并从CDB写入目的寄存器及等待此结果的保留站；</li><li>连续写入同一寄存器时，只有最后一次才能写入；</li><li>消除了WAW(Write After Write)竞争。</li></ul></li></ul><h2 id="1-12-仓库级计算机-WSC"><a href="#1-12-仓库级计算机-WSC" class="headerlink" title="1.12 仓库级计算机(WSC)"></a>1.12 仓库级计算机(WSC)</h2><ul><li><strong>提供互联网服务</strong><ul><li>搜索、社交网络、在线地图、视频分享、在线购物、电子邮件、云计算、etc.</li></ul></li><li><strong>不同于HPC“集群”</strong><ul><li>HPC集群有更高性能的处理器和互联网络</li><li>HPC集群强调线程级并行，WSC强调请求级并行</li></ul></li><li><strong>不同于数据中心</strong><ul><li>数据中心集中运行不同的机器和软件</li><li>数据中心强调虚拟机，具有硬件异构型，为不同客户提供服务</li></ul></li></ul><h2 id="1-13-批处理框架：MapReduce"><a href="#1-13-批处理框架：MapReduce" class="headerlink" title="1.13 批处理框架：MapReduce"></a>1.13 批处理框架：MapReduce</h2><ul><li><strong>Map</strong>：<ul><li>将程序员提供的函数应用于每条记录</li><li>在数千台计算机上运行</li><li>生成由键值对组成的中间结果</li></ul></li><li><strong>Reduce</strong>：<ul><li>收集上述分布式任务的输出，使用另一个函数应用于中间结果</li></ul></li></ul><h2 id="1-14-MESI协议图"><a href="#1-14-MESI协议图" class="headerlink" title="1.14 MESI协议图"></a>1.14 MESI协议图</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="//wx1.sinaimg.cn/large/007v1rTBgy1fxtwl9q77pj30h5083dgr.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="//ws2.sinaimg.cn/large/007v1rTBgy1fxtwlonrywj30go08oglt.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="1-15-基准程序的类别"><a href="#1-15-基准程序的类别" class="headerlink" title="1.15 基准程序的类别"></a>1.15 基准程序的类别</h2><p>基准程序(Benchmarks)：</p><ul><li>衡量一个系统，可通过运行一个或一组真实应用<ul><li>应用程序要有代表性，覆盖现实世界中常见的情况</li><li>应用程序负载(workload)也要有代表性，与实际情况比较吻合</li></ul></li><li>好的基准程序可以加速计算机的发展<ul><li>改进基准程序的性能应该对大多数程序有益</li></ul></li><li>好基准程序可以加速计算机的发展进程<ul><li>是有益于运行真实程序, 还是销售机器/发表论文?</li></ul></li><li>创造真正有益于真实程序的基准程序，而不是有益于基准程序的基准程序</li></ul><p>类别：</p><ul><li>玩具(Toy)基准程序<ul><li>10-100行</li><li>例如：sieve，puzzle，quicksort</li></ul></li><li>合成(Synthetic)基准程序<ul><li>试图匹配真正工作负载的平均频度</li><li>例如：Whetstone，Dhrystone</li></ul></li><li>内核(Kernels)程序<ul><li>时间密集(Time critical excerpts)</li><li>例如：Livemore loops，FFT，tree search</li></ul></li><li>真实程序(Actual workloads)<ul><li>例如：gcc，spice</li></ul></li></ul><h2 id="1-16-性能评测方法"><a href="#1-16-性能评测方法" class="headerlink" title="1.16 性能评测方法"></a>1.16 性能评测方法</h2><ol><li><strong>经验</strong>：使用系统的现有实例执行性能测试。<ul><li>前提条件：<ul><li>真实系统的存在</li><li>不同程度的可测性设计</li></ul></li><li>针对对象：<ul><li>系统用户(应用需求)</li><li>系统设计者(设计验证)</li></ul></li></ul></li><li><strong>分析</strong>：对系统进行数学抽象，推导出描述系统性能的公式。</li><li><strong>仿真</strong>：开发一个实现系统模型的计算机程序。通过运行计算机程序进行性能测试。</li><li><strong>统计</strong>：开发系统的统计抽象，并通过分析或模拟得出统计性能。</li></ol><h2 id="1-17-三种流水线冲突-冒险-Hazard"><a href="#1-17-三种流水线冲突-冒险-Hazard" class="headerlink" title="1.17 三种流水线冲突/冒险(Hazard)"></a>1.17 三种流水线冲突/冒险(Hazard)</h2><p>Hazard:指流水线遇到无法正确执行后续指令或执行了不该执行的指令</p><ul><li>Structural Hazards(hardware resource conflicts)<ul><li>现象：同一部件同时被不同指令所使用</li><li>解决方案：<ul><li>一个部件每条指令只能使用一次，且只能在特定周期使用</li><li>设置多个部件，以避免冲突，如指令寄存器IM和数据寄存器DM分开，或寄存器读口和寄存器写口分开。</li></ul></li></ul></li><li>Data Hazards(data dependencies)<ul><li>现象：后面指令用到前面指令结果时，前面指令结果还没产生</li><li>解决方案：<ul><li>采用转发(Forwarding/Bypassing)技术</li><li>load use 冒险需要一次阻塞(stall)</li><li>编译程序优化指令顺序</li></ul></li></ul></li><li>Control(Branch) Hazards(changes in program flow)<ul><li>现象：转移或异常改变执行流程，顺序执行指令在目标地址产生前已被取出</li><li>解决方案：<ul><li>采用静态或动态分支预测</li><li>编译程序优化指令顺序(实现分支延迟)</li></ul></li></ul></li></ul><h2 id="1-18-数据冒险-Data-Hazards"><a href="#1-18-数据冒险-Data-Hazards" class="headerlink" title="1.18 数据冒险(Data Hazards)"></a>1.18 数据冒险(Data Hazards)</h2><p>解决方法：</p><ol><li>硬件阻塞(stall)</li><li>软件插入“NOP”指令</li><li>合理实现寄存器堆的读/写操作<ul><li>前半时钟周期写，后半时钟周期读。若在同一个时钟内前面指令写入的数据正好是后面指令所读数据，则不会发生数据冒险</li></ul></li><li>转发技术(Forwarding或Bypassing旁路)<ul><li>load use 问题(需要一次阻塞stall)</li></ul></li><li>编译优化：调整指令顺序</li></ol><h2 id="1-19-Load指令的5个阶段-MIPS结构为例"><a href="#1-19-Load指令的5个阶段-MIPS结构为例" class="headerlink" title="1.19 Load指令的5个阶段(MIPS结构为例)"></a>1.19 Load指令的5个阶段(MIPS结构为例)</h2><div class="table-container"><table><thead><tr><th style="text-align:center">第一阶段</th><th style="text-align:center">第二阶段</th><th style="text-align:center">第三阶段</th><th style="text-align:center">第四阶段</th><th style="text-align:center">第五阶段</th></tr></thead><tbody><tr><td style="text-align:center">Ifetch</td><td style="text-align:center">Reg/Dec</td><td style="text-align:center">Exec</td><td style="text-align:center">Mem</td><td style="text-align:center">Wr</td></tr></tbody></table></div><ol><li>Ifetch(取指)：从指令存储器取指令并计算PC+4<ul><li>指令存储器、Addr</li></ul></li><li>Reg/Dec(取数和译码)：寄存器取数，同时对指令进行译码<ul><li>寄存器堆读口、指令译码器</li></ul></li><li>Exec(执行)：计算内存单元地址<ul><li>扩展器、ALU</li></ul></li><li>Mem(读存储器)：从数据存储器中<ul><li>数据存储器</li></ul></li><li>Wr(写寄存器)：将数据写到寄存器中<ul><li>寄存器堆写口</li></ul></li></ol><p>Tips:这里寄存器读口和写口可看成两个不同的部件</p><h2 id="1-20-动态分支预测"><a href="#1-20-动态分支预测" class="headerlink" title="1.20 动态分支预测"></a>1.20 动态分支预测</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/large/007v1rTBgy1fxtwk2qbklj30es05rjrj.jpg" alt="一位预测状态图" title="">                </div>                <div class="image-caption">一位预测状态图</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/large/007v1rTBgy1fxtwkqgu7oj30e009pt96.jpg" alt="二位预测状态图" title="">                </div>                <div class="image-caption">二位预测状态图</div>            </figure><p>比较：</p><ul><li>一位预测位：当连续两次的分支情况发生改变时，预测错误</li><li>二位预测位：在连续两次分支发生不同时，只会有一次预测错误</li></ul><p>一般采用二位预测位！</p><h1 id="2-后记"><a href="#2-后记" class="headerlink" title="2 后记"></a>2 后记</h1><blockquote><p>2018年12月3日</p></blockquote><p>战场，考场<br>聒噪，沉静<br>忐忑，镇定<br>顾盼，专心<br>窃喜，自信<br>得意忘形，波澜不惊<br>举世皆浊我独清<br>众人皆醉我独醒</p><div style="text-align:right">    ——YvanZh</div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;计算机的设计思想、设计原理、设计技术。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxtwrdfxyoj31hc0zgws0.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Courses" scheme="http://yvanzh.top/categories/Courses/"/>
    
    
      <category term="Advanced Computer Architecture" scheme="http://yvanzh.top/tags/Advanced-Computer-Architecture/"/>
    
  </entry>
  
  <entry>
    <title>聊聊图床</title>
    <link href="http://yvanzh.top/2018/12/02/Gallery/"/>
    <id>http://yvanzh.top/2018/12/02/Gallery/</id>
    <published>2018-12-02T03:11:11.000Z</published>
    <updated>2020-08-29T02:28:17.108Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>入坑微博图床，七牛云拜拜您嘞！</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxsc982ae2j31hc0u07jn.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>笔者刚开始写博客时使用的时<code>Hexo</code>自带的七牛云插件。只需在本地文件夹中放入图片，然后部署时同步到七牛云的测试域名中，即可在文章中插图，着实方便省心。然而却没有注意的是七牛云的测试域名只有三十天寿命，时间一到就会被收回，所以昨天博客的图片就全军覆没了。还好笔者还小心地保留着本地图片，不然真的是陪了夫人又折兵。</p><p>感叹当时没有做好功课，看到七牛云被多数推崇就傻傻地开始使用。后悔之余还是需要处理烂摊子的，于是笔者开始折腾起来：</p><ol><li>尝试将测试域名改为个人的博客网站域名。但是在七牛云绑定自己的域名需要有备案，而备案则意味着需要购买运营商的服务器，这对于笔者这种在<code>GitHub</code>和<code>Coding</code>上摸爬滚打的穷学生显然是一条绝路。</li><li>遍寻互联网终于找到了笔者想要的菜——微博图床：支持http和https，全网加速，不限流量。这里推荐一款国人制作的<a href="https://github.com/Semibold/Weibo-Picture-Store" target="_blank" rel="noopener">Chrome插件</a>，直接解决了笔者的燃眉之急。</li></ol><p>好在笔者的图片不多，亡羊补牢为时未晚。</p><p>吸取教训：学习知识与掌握工具应寻根探底，知其所以然，切忌人云亦云，切勿急功近利。保持怀疑，保持好奇。</p><blockquote><p>我不能创造的东西，我就不了解。</p><div style="text-align:right">    ——理查德·菲利普·费曼</div></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;入坑微博图床，七牛云拜拜您嘞！&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxsc982ae2j31hc0u07jn.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Gallery" scheme="http://yvanzh.top/categories/Gallery/"/>
    
    
      <category term="Gallery" scheme="http://yvanzh.top/tags/Gallery/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter Lab 手册</title>
    <link href="http://yvanzh.top/2018/11/24/JupyterLab-Handbook/"/>
    <id>http://yvanzh.top/2018/11/24/JupyterLab-Handbook/</id>
    <published>2018-11-24T13:11:11.000Z</published>
    <updated>2020-08-29T02:28:17.118Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>Lab 快点成熟起来吧，扩展和主题让人绝望！</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx4.sinaimg.cn/wap690/007v1rTBgy1fxsczcrh8lj31hc0zkall.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-添加Kernel"><a href="#1-添加Kernel" class="headerlink" title="1 添加Kernel"></a>1 添加Kernel</h1><p>使用<code>Anaconda</code>的用户在创建好新环境之后会遇到一个问题——<code>Anaconda</code>自带的<code>Jupyter Notebook</code>或<code>Jupyter Lab</code>并没有把新环境添加到<code>Kernel</code>中去。当然，可以在每个新建的环境中都安装<code>Jupyter</code>，可行但是不优雅。那么解决方法如下：</p><ul><li>在<code>Anacodna Prompt</code>中激活创建的环境<code>py36</code>:</li></ul><pre><code class="lang-shell">conda activate py36</code></pre><ul><li>安装插件<code>ipykernel</code>：</li></ul><pre><code class="lang-shell">pip install ipykernel</code></pre><ul><li>添加内核：</li></ul><pre><code class="lang-python">python -m ipykernel install --name py36 --display-name &quot;py36&quot;</code></pre><p><code>--name</code>参数后的<code>py36</code>是内核名称，用于系统保存。而<code>--display-name</code>参数后的<code>&quot;Py36&quot;</code>是在<code>Jupyter Notebook</code>网页中选择或切换内核时所显示的。建议二者设置为一样的，方便以后删除<code>Kernel</code>。</p><p>如果非要设置一个<code>--display-name</code>，然而一段时间后又忘了当时<code>--name</code>是什么了。那么查看所有<code>Kernel</code>,可在<code>CMD</code>中输入:</p><pre><code class="lang-shell">jupyter kernelspec list</code></pre><p>删除内核时，可在<code>CMD</code>中输入：</p><pre><code class="lang-shell">jupyter kernelspec remove py36</code></pre><h1 id="2-修改Home路径"><a href="#2-修改Home路径" class="headerlink" title="2 修改Home路径"></a>2 修改Home路径</h1><p>在<code>C:\Users\usersname\.jupyter</code>文件夹中打开<code>jupyter_notebook_config.py</code>文件，找到字段：</p><pre><code class="lang-python"># The directory to use for notebooks and kernels.  #c.NotebookApp.notebook_dir = &#39;&#39;</code></pre><p>删除<code>#</code>，并在单引号中添加所需修改的路径，譬如：</p><pre><code class="lang-python"># The directory to use for notebooks and kernels.  c.NotebookApp.notebook_dir = &#39;E:\jupyter workspace&#39;</code></pre><h1 id="3-快捷键"><a href="#3-快捷键" class="headerlink" title="3 快捷键"></a>3 快捷键</h1><h2 id="3-1-命令模式-按Esc键"><a href="#3-1-命令模式-按Esc键" class="headerlink" title="3.1 命令模式 (按Esc键)"></a>3.1 命令模式 (按Esc键)</h2><p><strong>Enter</strong> : 转入编辑模式<br><strong>Shift-Enter</strong> : 运行本单元，选中下个单元<br><strong>Ctrl-Enter</strong> : 运行本单元<br><strong>Alt-Enter</strong> : 运行本单元，在其下插入新单元<br><strong>Y</strong> : 单元转入代码状态<br><strong>M</strong> :单元转入markdown状态<br><strong>R</strong> : 单元转入raw状态<br><strong>1</strong> : 设定 1 级标题<br><strong>2</strong> : 设定 2 级标题<br><strong>3</strong> : 设定 3 级标题<br><strong>4</strong> : 设定 4 级标题<br><strong>5</strong> : 设定 5 级标题<br><strong>6</strong> : 设定 6 级标题<br><strong>Up</strong> : 选中上方单元<br><strong>K</strong> : 选中上方单元<br><strong>Down</strong> : 选中下方单元<br><strong>J</strong> : 选中下方单元<br><strong>Shift-K</strong> : 扩大选中上方单元<br><strong>Shift-J</strong> : 扩大选中下方单元<br><strong>A</strong> : 在上方插入新单元<br><strong>B</strong> : 在下方插入新单元<br><strong>X</strong> : 剪切选中的单元<br><strong>C</strong> : 复制选中的单元<br><strong>Shift-V</strong> : 粘贴到上方单元<br><strong>V</strong> : 粘贴到下方单元<br><strong>Z</strong> : 恢复删除的最后一个单元<br><strong>D,D</strong> : 删除选中的单元<br><strong>Shift-M</strong> : 合并选中的单元<br><strong>Ctrl-S</strong> : 文件存盘<br><strong>S</strong> : 文件存盘<br><strong>L</strong> : 转换行号<br><strong>O</strong> : 转换输出<br><strong>Shift-O</strong> : 转换输出滚动<br><strong>Esc</strong> : 关闭页面<br><strong>Q</strong> : 关闭页面<br><strong>H</strong> : 显示快捷键帮助<br><strong>I,I</strong> : 中断Notebook内核<br><strong>0,0</strong> : 重启Notebook内核<br><strong>Shift</strong> : 忽略<br><strong>Shift-Space</strong> : 向上滚动<br><strong>Space</strong> : 向下滚动</p><h2 id="3-2-编辑模式-按Enter键"><a href="#3-2-编辑模式-按Enter键" class="headerlink" title="3.2 编辑模式 (按Enter键)"></a>3.2 编辑模式 (按Enter键)</h2><p><strong>Tab</strong> : 代码补全或缩进<br><strong>Shift-Tab</strong> : 提示<br><strong>Ctrl-]</strong> : 缩进<br><strong>Ctrl-[</strong> : 解除缩进<br><strong>Ctrl-A</strong> : 全选<br><strong>Ctrl-Z</strong> : 复原<br><strong>Ctrl-Shift-Z</strong> : 再做<br><strong>Ctrl-Y</strong> : 再做<br><strong>Ctrl-Home</strong> : 跳到单元开头<br><strong>Ctrl-Up</strong> : 跳到单元开头<br><strong>Ctrl-End</strong> : 跳到单元末尾<br><strong>Ctrl-Down</strong> : 跳到单元末尾<br><strong>Ctrl-Left</strong> : 跳到左边一个字首<br><strong>Ctrl-Right</strong> : 跳到右边一个字首<br><strong>Ctrl-Backspace</strong> : 删除前面一个字<br><strong>Ctrl-Delete</strong> : 删除后面一个字<br><strong>Esc</strong> : 进入命令模式<br><strong>Ctrl-M</strong> : 进入命令模式<br><strong>Shift-Enter</strong> : 运行本单元，选中下一单元<br><strong>Ctrl-Enter</strong> : 运行本单元<br><strong>Alt-Enter</strong> : 运行本单元，在下面插入一单元<br><strong>Ctrl-Shift—</strong> : 分割单元<br><strong>Ctrl-Shift-Subtract</strong> : 分割单元<br><strong>Ctrl-S</strong> : 文件存盘<br><strong>Shift</strong> : 忽略<br><strong>Up</strong> : 光标上移或转入上一单元<br><strong>Down</strong> :光标下移或转入下一单元</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Lab 快点成熟起来吧，扩展和主题让人绝望！&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx4.sinaimg.cn/wap690/007v1rTBgy1fxsczcrh8lj31hc0zkall.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Jupyter Lab" scheme="http://yvanzh.top/categories/Jupyter-Lab/"/>
    
    
      <category term="Jupyter Lab" scheme="http://yvanzh.top/tags/Jupyter-Lab/"/>
    
  </entry>
  
  <entry>
    <title>Heterogeneous Domain Adaptation Through Progressive Alignment</title>
    <link href="http://yvanzh.top/2018/11/16/Paper-Notes-2/"/>
    <id>http://yvanzh.top/2018/11/16/Paper-Notes-2/</id>
    <published>2018-11-16T03:01:33.000Z</published>
    <updated>2020-08-29T02:28:17.119Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxscc6nvz0j31hc0wq13q.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><a href="https://ieeexplore.ieee.org/document/8475006" target="_blank" rel="noopener">文章来源</a>：2018-TNNLS</p><p>Tips：</p><ul><li>异构领域适应：假设源域和目标域样本分别采样于不同空间，其维度也可能不同。</li><li>同构领域适应：假设源域和目标域都采样于一个相同空间。</li></ul><p>本文考虑异构领域适应的五方面：</p><ul><li>特征维度差异</li><li>减少特征失配</li><li>减小分布差异</li><li>数据局部一致</li></ul><p>主旨：</p><ul><li>将不同维度的样本映射到同一空间中，通过学得一个共同的字典得到可迁移的特征空间，在这个空间中逐步减小两个领域特征的分布差异。</li></ul><h1 id="2-详情"><a href="#2-详情" class="headerlink" title="2 详情"></a>2 详情</h1><p>总体目标：</p><script type="math/tex; mode=display">\underset{\bf{B,S}}{\text{min}} \quad \underbrace{\underbrace{\mathcal{C_{1}}(\bf{X_{s},X_{t},B,S})}_{\text{feature alignment}} + \underbrace{\alpha \mathcal{C_{2}}(\bf{S_{s},S_{t}})}_{\text{distribution alignment}}}_{\text{progressive alignment}} + \underbrace{\beta \Omega(S)}_{\text{constraint}}  \tag{1}</script><ol><li><p>维度差异和特征失配问题：</p><ul><li><p>将领域特征分别经$\bf P_{s}$、$\bf P_{t}$投影到同一维度，并通过共享字典$\bf B$来学的一个新的特征空间。</p><script type="math/tex; mode=display">\underset{ {\bf{b}}_{s},{\bf{s}}_{s}}{\text{min}} \quad\sum_{i=1}^{n_{s}}{ \Vert{ {\bf{x}}_{s,i} - \sum_{j=1}^{k}{\bf{b}}_{s,j}{\bf{s}}_{s,j}^{j}}\Vert_{2}^{2} +\beta \sum_{i=1}^{n_{s}}{\Vert{ {\bf{s}}_{s,i}}}\Vert_{1} } \\\text{s.t.} \Vert{ {\bf{b}}_{s,j}}\Vert^{2} \le c,\quad \forall j  \tag{2}</script><script type="math/tex; mode=display">\underset{\bf P,B,S_{s},S_{t}}{\text{min}} \quad\Vert{\bf{P_{s}{X}_{s}}- BS_{s}}\Vert_{F}^{2} +\Vert{\bf{P_{t}{X}_{t}}- BS_{t}}\Vert_{F}^{2} + \beta \sum_{i=1}^{n}{\Vert{ {\bf{s}}_{i}}}\Vert_{1}  \\\text{s.t.} \quad \Vert{ {\bf{b}}_{j}}\Vert^{2} \le c,\quad \forall j  \tag{3}</script></li><li>然而对于异构领域适应来说，投影降维的信息损失是十分致命的。通过$\bf PXHX^{\top}P^{\top} = \bf{I}$使投影后的交叉领域数据保留协方差，即特征维度之间保持线性无关。并添加约束$\Vert{\bf{P}}\Vert_{F}^{2}$防止过拟合。  <script type="math/tex; mode=display">\underset{\bf P,B,S}{\text{min}} \quad \Vert{\bf{P{X}}- BS}\Vert_{F}^{2} + \beta \sum_{i=1}^{n}{\Vert{ {\bf{s}}_{i}}}\Vert_{1}  + \gamma \Vert{ {\bf{P}}}\Vert_{F}^{2} \\\text{s.t.} \quad {\bf PXHX^{\top}P^{\top}} = {\bf{I}} , \,\,  \Vert{ {\bf{b}}_{j}}\Vert^{2} \le c,\quad \forall j  \tag{4}</script></li></ul></li><li>分布差异问题： <ul><li>采用MMD来度量领域特征分布差异，可直接用学到的新特征$\bf S$代替经过映射$\phi(\cdot)$后的特征。<script type="math/tex; mode=display">\underset{ {\bf S}_{s},{\bf S}_{t}}{\text{min}} \quad \left|  \left| \frac{1}{n_{s}}\sum_{i=1}^{n_{s}}{ {\bf S}_{s,j}-\frac{1}{n_t}\sum_{i=1}^{n_{t}} { {\bf S}_{t,j}}}\right| \right| = \underset{\bf S}{\text{min}} \quad \text{tr}({\bf{SMS}^{\top}})  \tag{5}</script></li></ul></li><li>局部一致问题：<ul><li>新特征空间中一个样本的标签应该与其$k$近邻趋向一致。用$\bf W$表示邻接矩阵，注意其保留的是原始特征空间的数据关系。<script type="math/tex; mode=display">\frac{1}{2}\sum_{j,l=1}^{n}\Vert{ {\bf s}_{j}-{\bf s}_{l}}\Vert_{2}^{2} {\bf W}_{jl} \\  \tag{6}</script><script type="math/tex; mode=display">{\bf W}_{ij}=\begin{cases}\text{cosine}({\bf x}_{i},{\bf x}_{j}), \,\,\text{if}\,\, {\bf x}_{i}\in \mathcal{N}_{k}({\bf x}_{j}) \\0,\qquad \qquad \quad \,\,\,\, \text{otherwise}\end{cases}</script></li><li>展开后可用拉普拉斯矩阵表示：<script type="math/tex; mode=display">\sum_{j=1}^{n}{ {\bf s}_{j}{\bf s}_{j}^{\top}{\bf D}_{jj}} - \sum_{j,l=1}^{n} { {\bf s}_{j}{\bf s}_{l}^{\top}{\bf W}_{jl}} = \text{tr}({\bf SLS}^\top)  \tag{7}</script></li></ul></li><li>最终目标函数：<script type="math/tex; mode=display">\underset{\bf P,B,S}{\text{min}} \Vert{ {\bf PX-BS}}\Vert_{F}^{2} + \text{tr}({\bf S}(\alpha_{1} {\bf M} + \alpha_{2} {\bf L)}{\bf S}^{\top}) + \beta \sum _{i=1}^{n}{\Vert{ {\bf s}_{i}\Vert}_{1}} + \gamma \Vert { {\bf P}}\Vert_{F}^2 \\\text{s.t.} \quad {\bf PXHX^{\top}P^{\top}} = {\bf{I}} , \,\,  \Vert{ {\bf{b}}_{j}}\Vert^{2} \le c,\quad \forall j  \tag{8}</script></li></ol><h1 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h1><ol><li>字典学习的特征太浅显？</li><li>拉普拉斯矩阵那些事：<ol><li>三种形式<ul><li>Simple Laplacian：$L=D-W$</li><li>Symmetric normalized Laplacian：${\displaystyle L^{\text{sym}}:=D^{-{\frac {1}{2}}}LD^{-{\frac {1}{2}}}=I-D^{-{\frac {1}{2}}}WD^{-{\frac {1}{2}}}}$</li><li>Random walk normalized Laplacian：${\displaystyle L^{\text{rw}}:=D^{-1}L=I-D^{-1}A}$</li></ul></li><li>邻接矩阵<ul><li>第一步找近邻：一般采用K邻近法，找出同一类的K个样本。又分两种是否保留相似度的方法：一种是只需要邻近就保留，另一种是两者互为邻近才保留。一般采用前者。</li><li>第二步求相似度：<ul><li>二值：邻近即为1，非邻近即为0。过于单一。</li><li>余弦相似度：$cosine(\mathbf x,\mathbf x’)$。一般用这个。</li><li>径向基(高斯)核函数： $K({\mathbf  {x}},{\mathbf  {x’}})=\exp \left(-{\frac  {||{\mathbf  {x}}-{\mathbf  {x’}}||_{2}^{2}}{2\sigma ^{2}}}\right)$。计算复杂且引入了新参数，一般不采用。</li></ul></li></ul></li></ol></li><li>瑞利熵？</li><li>度量学习？</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxscc6nvz0j31hc0wq13q.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Transfer Learning" scheme="http://yvanzh.top/tags/Transfer-Learning/"/>
    
      <category term="Heterogeneous Domain Adaptation" scheme="http://yvanzh.top/tags/Heterogeneous-Domain-Adaptation/"/>
    
  </entry>
  
  <entry>
    <title>Deep Transfer Low-Rank Coding for Cross-Domain Learning</title>
    <link href="http://yvanzh.top/2018/11/14/Paper-Notes-1/"/>
    <id>http://yvanzh.top/2018/11/14/Paper-Notes-1/</id>
    <published>2018-11-14T10:01:33.000Z</published>
    <updated>2020-08-29T02:28:17.119Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx3.sinaimg.cn/wap690/007v1rTBgy1fxscc5pkdjj31hc0zl7kx.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p><a href="https://ieeexplore.ieee.org/document/8513988" target="_blank" rel="noopener">文章来源</a>：2018-TNNLS</p><p>寻找领域不变特征一般分为三类：</p><ul><li>子空间学习</li><li>非线性投影</li><li>[X] 字典学习<ul><li>稀疏约束</li><li>[X] 低秩约束</li><li>局部约束</li></ul></li></ul><p>为了实现两个领域的迁移需要减小其分布差异：</p><ul><li>[X] 边缘分布$P(X_{s})$与$P(X_{t})$</li><li>[X] 条件分布$P(Y_{s}|X_{s})$与$P(Y_{t}|X_{t})$</li></ul><h1 id="2-详情"><a href="#2-详情" class="headerlink" title="2 详情"></a>2 详情</h1><ol><li>从稀疏编码谈起：<script type="math/tex; mode=display"> \underset{D,Z}{\text{min}}  \quad \lVert {X-DZ}\rVert_{F}^{2} + \mathcal{N}(Z) \\ \text{s.t.} \,\, \lVert{ {d}_{i}^j}\rVert_{2}^2 \le 1 \quad \forall i,j \tag{1}</script></li><li>减小边缘分布差异：<ol><li>由于光照，角度等问题。直接通过字典学习得到新特征太浅显。$\Rightarrow$使用CNN提取抽象特征（参数共享）。</li><li>对于存在较大分布差异情况，学习的一个字典之后新特征空间中$P(Z_{s})$与$P(Z_{t})$的差异依然大。$\Rightarrow$通过task-specific全连接层学习多层字典来覆盖两个领域共同特征，逐步减小分布差异。<ul><li>task-specific全连接层的神经元个数需要根据秩固定。 </li></ul></li><li>$\Rightarrow$通过对$Z$的低秩约束来获取有识别力的新特征。（还能防止过拟合）<ul><li>只需约束最后一层的$Z_{k}$就能使中间层都能低秩学习：<script type="math/tex; mode=display">\left.\begin{aligned}Z_{k-1} = D_{k}Z_{k} \Rightarrow \text{rank}(Z_{k-1}) = \text{rank}(D_{k}Z_{k}) \\\text{rank}(D_{k}Z_{k}) \le \text{min}(\text{rank}(Z_{k}),\text{rank}(D_{k}))\end{aligned} \right\} \Rightarrow \text{rank}(Z_{k-1}) \le \text{rank}(Z_{k}) \\\Downarrow  \\ \text{rank}(Z_{1}) \le \text{rank}(Z_{2}) \le \cdots \text{rank}(Z_{k})  \tag{2}</script></li></ul></li><li>$\Rightarrow$改进结果：<script type="math/tex; mode=display">\underset{D_{1}\dots,D_{2},Z_{k}}{\text{min}}  \quad \lVert{X-D_{1}D_{2}\dots D_{k}Z_{k}}\rVert_{F}^{2} + \text{rank}(Z_{k}) \\ s.t. \,\, \lVert{ {d}_{i}^j}\rVert_{2}^2 \le 1 \quad \forall i,j  \tag{3}</script></li></ol></li><li>减小条件分布差异：<ol><li>半监督知识适应($Z_{k}=[Z_{k}^s,Z_{k}^t]$)：<ol><li>从传统的MMD谈起：<script type="math/tex; mode=display">\begin{aligned}\mathcal{M}(Z_{k}) &= \left| \left| {\frac{1}{m_{s}} \sum_{i=1}^{m_{s}} {z_{k,i}} - \frac{1}{m_{t}} \sum_{j=m_{s}+1}^{m}{z_{k,j}} } \right| \right| _{2}^2   \\&= \sum_{i=1}^m \sum_{j=1}^m {z_{k,i}^\top z_{k,j}W_{ij}=\text{tr}(Z_{k}WZ_{k}^\top)}  \end{aligned}\tag{4}</script></li><li>改进：<ol><li>传统的MMD只能减小边缘分布差异$\Rightarrow$采用类间MMD，可减小条件分布差异。</li><li>但目标域几乎无标签可用$\Rightarrow$对目标域样本添加软标签（样本属于每个类的概率）。<script type="math/tex; mode=display">\mathcal{C}(Z_{k}) = \sum_{c=1}^C \left| \left|{\frac{1}{m_{s}^c} \sum_{i=1}^{m_{s}^c} {z_{k,i}^{s}} - \frac{1}{m_{t}^c} \sum_{j=1}^{m_{t}}{p_{c,j}z_{k,j}^{t}} } \right| \right|_{2}^2 = \sum_{c=1}^C \text{tr}(Z_{k}W^{(c)}Z_{k}^\top)  \tag{5}</script></li></ol></li></ol></li></ol></li><li>“end-to-end”：<ul><li>添加softmax层，计算交叉熵损失：<script type="math/tex; mode=display">\mathcal{J}(Z_{K},\Theta,Y) = -\frac{1}{m}\sum _{i=1}^m \sum_{c=1}^C y_{c,i}\text{log}\frac{e^{\theta_{c}^\top}z_{k,i}}{\sum_{u=1}^C e^{\theta_{u}^\top}z_{k,i}}  \tag{6}</script></li></ul></li><li>非线性化：<ul><li>非线性的数据表示可有效减少统计和感知冗余，同时可提高神经网络训练速度，一举两得。使用ReLU这个非线性的激活函数：<script type="math/tex; mode=display">Z_{i} \approx f(D_{i+1}Z_{i+1})  \tag{7}</script></li></ul></li><li>低秩约束： <ul><li>希望同类样本的被一个基底张成。那么$Z_{k}$的真实秩即类的总数$C$。将$Z_{k}\approx AB$作为低秩约束，其中$A\in \mathbb{R}^{d_{k}\times C}$，$B\in \mathbb{R}^{C \times n}$。</li></ul></li><li>最终目标函数:  <script type="math/tex; mode=display">\begin{aligned}\mathcal{L} = &\mathcal{L}(Z_{k},\Theta,Y) + \lambda\Vert{X-D_{1}f(D_{2}f(\cdots f(D_{k}Z_{k}) ))}\Vert_{F}^2 \\&+\alpha\sum_{c=0}^C\text{tr}(Z_{k}W^{(c)}Z_{k}^{\top})+\beta\Vert{Z_{k}-AB}\Vert_{F}^2      \end{aligned}\tag{8}</script></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="//ws3.sinaimg.cn/large/007v1rTBgy1fxscc6d60kj31gn0ian9g.jpg" alt="image" title="">                </div>                <div class="image-caption">image</div>            </figure><h1 id="3-思考"><a href="#3-思考" class="headerlink" title="3 思考"></a>3 思考</h1><ol><li>深度学习与迁移学习结合起来，充分发挥各类神经网络的优势。CNN提取抽象特征，NN非线性拟合，GAN拟合分布，RNN序列模型，等等。</li><li>是否还需考虑保持特征空间的局部结构或谱结构，使用图拉普拉斯？</li><li>是否可以引入流形学习的方法，假设两个领域的数据是采样于一个高维流形，每类数据分布紧凑。那么学习一个特征映射并通过它得到一个新的特征空间。</li><li>低秩约束是否太强。能否充分利用好软标签即样本标签的概率来挖掘类间的关系。</li><li>能否引入文本信息，做更广泛的迁移？</li><li>如果使用BN层会不会效果更好？</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx3.sinaimg.cn/wap690/007v1rTBgy1fxscc5pkdjj31hc0zl7kx.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Paper Notes" scheme="http://yvanzh.top/categories/Paper-Notes/"/>
    
    
      <category term="Cross-Domain" scheme="http://yvanzh.top/tags/Cross-Domain/"/>
    
      <category term="Transfer Learning" scheme="http://yvanzh.top/tags/Transfer-Learning/"/>
    
      <category term="Deep Learning" scheme="http://yvanzh.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习数学基础</title>
    <link href="http://yvanzh.top/2018/11/08/ML-Math/"/>
    <id>http://yvanzh.top/2018/11/08/ML-Math/</id>
    <published>2018-11-08T14:33:33.000Z</published>
    <updated>2020-08-29T02:28:17.118Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>问渠那得清如许？为有源头活水来。</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx4.sinaimg.cn/wap690/007v1rTBgy1fxscc4qygxj31hc0tz7cv.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-信息论"><a href="#1-信息论" class="headerlink" title="1 信息论"></a>1 信息论</h1><h2 id="1-1-信息熵-Entropy"><a href="#1-1-信息熵-Entropy" class="headerlink" title="1.1 信息熵(Entropy)"></a>1.1 信息熵(Entropy)</h2><p>信息熵的本质是香农信息量$log\frac{1}{p}$的期望。</p><p>离散：</p><script type="math/tex; mode=display">H(p) = E[\log{\frac{1}{p}}] = \sum{p_{i} \cdot log\frac{1}{p_{i}}}</script><p>连续：</p><script type="math/tex; mode=display">H(p) = E[\log{\frac{1}{p}}] = \int{p(x) \cdot log\frac{1}{p(x)}}dx</script><p>香农信息量的意义：一个事件出现的概率越低，对其编码的长度越大。（huffman编码的思想。）</p><p>熵的意义：最短平均编码长度。（因为我们用真实分布$p_i$来编码，编码方案是最优的。）</p><h2 id="1-2-交叉熵-Cross-Entropy"><a href="#1-2-交叉熵-Cross-Entropy" class="headerlink" title="1.2 交叉熵(Cross Entropy)"></a>1.2 交叉熵(Cross Entropy)</h2><p>交叉熵的本质是对估计分布的编码长度。todo</p><p>离散:</p><script type="math/tex; mode=display">H(p,q) = E_{p}[-\log{q}] = \sum_{i} {p_i \cdot \log{\frac{1}{q_i}}}</script><p>连续：</p><script type="math/tex; mode=display">H(p,q) = E_{p}[-\log{q}] = \int{p(x) \cdot \log{\frac{1}{q(x)}} dx}</script><p>交叉熵的意义：估计平均编码长度。（用一个估计分布$q_i$来编码一个真实分布$p_i$，编码不一定是最优的。）</p><p>可证明$H(p,q)\le H(p)$，当且仅当$p=q$时等号成立。</p><h2 id="1-3-相对熵-Relative-Entropy"><a href="#1-3-相对熵-Relative-Entropy" class="headerlink" title="1.3 相对熵(Relative Entropy)"></a>1.3 相对熵(Relative Entropy)</h2><p>又称<strong>KL散度</strong>或<strong>信息增益</strong>，用来衡量来个分布之间的差异，具有不对称性。</p><script type="math/tex; mode=display">D_{KL} = H(p,q) - H(p)</script><p>相对熵的意义：估计平均编码长度相较于最短平均编码长度的冗余。</p><h1 id="2-概率论"><a href="#2-概率论" class="headerlink" title="2 概率论"></a>2 概率论</h1><h2 id="2-1-最大似然估计-MLE"><a href="#2-1-最大似然估计-MLE" class="headerlink" title="2.1 最大似然估计(MLE)"></a>2.1 最大似然估计(MLE)</h2><script type="math/tex; mode=display">max \space p(x_0 \mid \theta)</script><h2 id="2-2-贝叶斯估计-BE"><a href="#2-2-贝叶斯估计-BE" class="headerlink" title="2.2 贝叶斯估计(BE)"></a>2.2 贝叶斯估计(BE)</h2><script type="math/tex; mode=display">p()</script><h2 id="2-3-最大后验估计-MAP"><a href="#2-3-最大后验估计-MAP" class="headerlink" title="2.3 最大后验估计(MAP)"></a>2.3 最大后验估计(MAP)</h2><script type="math/tex; mode=display">max \space p(\theta \mid x_0) = \frac{p(x_0 \mid \theta) \cdot p(\theta)} {p(x_0)}</script><p>因$p(x_0)$，已知。故等价于：</p><script type="math/tex; mode=display">max \space p(x_0 \mid \theta) \cdot p(\theta)</script><h1 id="3-矩阵分析"><a href="#3-矩阵分析" class="headerlink" title="3 矩阵分析"></a>3 矩阵分析</h1><h2 id="3-1-特征值分解"><a href="#3-1-特征值分解" class="headerlink" title="3.1 特征值分解"></a>3.1 特征值分解</h2><h2 id="3-2-奇异值分解"><a href="#3-2-奇异值分解" class="headerlink" title="3.2 奇异值分解"></a>3.2 奇异值分解</h2><h2 id="3-4-中心矩阵"><a href="#3-4-中心矩阵" class="headerlink" title="3.4 中心矩阵"></a>3.4 中心矩阵</h2><p>给定m维数据的n个样本，用$m \times n$矩阵${X=[\mathbf{x}_{1},\mathbf{x}_{2},\ldots ,\mathbf{x}_{n}]}$。其表示样本均值为：</p>$$\overline { {\mathbf {x} } }={\frac {1}{n}}\sum _{ {j=1} }^{n}{\mathbf {x}}_{j}$$<p>其中${\mathbf {x} _{j}}$表示$X$矩阵的第$j$列。</p><ol><li><p>$n \times n$的中心矩阵可表示为：</p><script type="math/tex; mode=display">C_n = I_n - \frac{1}{n} \phi</script><p> $I_{n}$为单位矩阵， $\phi$表示全为1的矩阵。</p><ul><li>中心矩阵的性质：<ul><li>对称半正定；</li><li>幂等；</li><li>奇异；</li><li>有n-1个值为1的特征值，有一个值为0的特征值；</li><li>当n为1时，是零空间；</li><li>是投影矩阵，也就是说，把n维空间投影到n-1维子空间。</li></ul></li><li>中心矩阵的特性：<ul><li>给定$n$维的列向量$v$，若$u$中每个元素都是列向量$v$的所有元素的均值，有：<script type="math/tex; mode=display">C_{n}v = v - u</script></li></ul></li></ul></li><li><p>散度矩阵$S$可表示为：</p><script type="math/tex; mode=display">{S=\sum _{j=1}^{n}(\mathbf {x} _{j}-{\overline {\mathbf {x} }})(\mathbf {x} _{j}-{\overline {\mathbf {x} }})^{T}=\sum _{j=1}^{n}(\mathbf {x} _{j}-{\overline {\mathbf {x} }})\otimes (\mathbf {x} _{j}-{\overline {\mathbf {x} }})=\left(\sum _{j=1}^{n}\mathbf {x} _{j}\mathbf {x} _{j}^{T}\right)-n{\overline {\mathbf {x} }}{\overline {\mathbf {x} }}^{T}}</script><p> 或</p><script type="math/tex; mode=display">S= XC_{n}(XC_{n})^{T} = XC_{n}C_{n}X^{T}= XC_{n}X^{T}</script></li><li>协方差矩阵$\Sigma$可表示为：<script type="math/tex; mode=display">\Sigma = \frac{1}{n-1}S = \frac{1}{n-1}XC_{n}X^T</script></li></ol><h2 id="3-5-三个矩阵"><a href="#3-5-三个矩阵" class="headerlink" title="3.5 三个矩阵"></a>3.5 三个矩阵</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx3.sinaimg.cn/large/007v1rTBgy1g2ltpevt35j30nw0r6wg6.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="3-6-拉普拉斯矩阵-Graph-Laplacians"><a href="#3-6-拉普拉斯矩阵-Graph-Laplacians" class="headerlink" title="3.6 拉普拉斯矩阵(Graph Laplacians)"></a>3.6 拉普拉斯矩阵(Graph Laplacians)</h2><p>一般的$L = D - \frac{S^T+S}{2}$，其中$D$为对角矩阵，元素为$\sum(s_{ij}+s_{ji})/2$。</p><p>性质：对称，半正定。</p><ol><li><p>对于特征向量$\bf x$有：</p><script type="math/tex; mode=display"> {\bf x^\top Lx} = \frac{1}{2}\sum_{j,l=1}^{n}({ {\bf x}_{j}-{\bf x}_{l}} )^{2} {\bf S}_{jl} \\</script></li><li><p>对于特征矩阵$\bf X$有：</p><script type="math/tex; mode=display"> \text{tr}({\bf XLX}^\top) = \frac{1}{2}\sum_{j,l=1}^{n}\Vert{ {\bf x}_{j}-{\bf x}_{l}} \Vert_{2}^{2} {\bf S}_{jl} \\</script></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;问渠那得清如许？为有源头活水来。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx4.sinaimg.cn/wap690/007v1rTBgy1fxscc4qygxj31hc0tz7cv.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Math" scheme="http://yvanzh.top/categories/Math/"/>
    
    
      <category term="Math" scheme="http://yvanzh.top/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 之公式渲染</title>
    <link href="http://yvanzh.top/2018/11/08/Hexo-MathJax/"/>
    <id>http://yvanzh.top/2018/11/08/Hexo-MathJax/</id>
    <published>2018-11-08T14:33:33.000Z</published>
    <updated>2020-08-29T02:28:17.117Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>不写数学公式跟咸鱼有什么区别。</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxscc5cfoyj31hc0zi1kx.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-公式渲染"><a href="#1-公式渲染" class="headerlink" title="1 公式渲染"></a>1 公式渲染</h1><p><code>{{}}</code>或<code>{% %}</code>会导致解析Bug。(尤其在使用数学公式时需注意)</p><p>官方给出的解决方案为：</p><pre><code>{% raw %}{{Balabala}} or {% Balabala %}{% endraw %}</code></pre><p>然而这样十分不利于博客的迁移。</p><p>解决解决方案：</p><ol><li>更换默认的渲染引擎：<pre><code class="lang-bash">npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save</code></pre></li><li>修改<code>\node_modules\kramed\lib\rules\inline.js</code>路径中的代码如下：<pre><code class="lang-js">var inline = {//escape: /^\\([\\`*{}\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,autolink: /^&lt;([^ &gt;]+(@|:\/)[^ &gt;]+)&gt;/,url: noop,html: /^&lt;!--[\s\S]*?--&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)*?(?:&quot;[^&quot;]*&quot;|&#39;[^&#39;]*&#39;|[^&#39;&quot;&gt;])*?&gt;([\s\S]*?)?&lt;\/\1&gt;|^&lt;(\w+(?!:\/|[^\w\s@]*@)\b)(?:&quot;[^&quot;]*&quot;|&#39;[^&#39;]*&#39;|[^&#39;&quot;&gt;])*?&gt;/,link: /^!?\[(inside)\]\(href\)/,reflink: /^!?\[(inside)\]\s*\[([^\]]*)\]/,nolink: /^!?\[((?:\[[^\]]*\]|[^\[\]])*)\]/,reffn: /^!?\[\^(inside)\]/,strong: /^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/,//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,code: /^(`+)\s*([\s\S]*?[^`])\s*\1(?!`)/,br: /^ {2,}\n(?!\s*$)/,del: noop,text: /^[\s\S]+?(?=[\\&lt;!\[_*`$]| {2,}\n|$)/,math: /^\$\$\s*([\s\S]*?[^\$])\s*\$\$(?!\$)/,};</code></pre></li><li>在使用双花括号<code>{ {</code>和<code>{ \{</code>时中间添加一个空格即可。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;不写数学公式跟咸鱼有什么区别。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxscc5cfoyj31hc0zi1kx.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Hexo" scheme="http://yvanzh.top/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://yvanzh.top/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Git 手册</title>
    <link href="http://yvanzh.top/2018/11/04/Git-Handbook/"/>
    <id>http://yvanzh.top/2018/11/04/Git-Handbook/</id>
    <published>2018-11-04T14:33:33.000Z</published>
    <updated>2020-08-29T02:28:17.108Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p><strong><em>穿越过去和未来！</em></strong></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxscc4kjd5j31hc0tch5w.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><a id="more"></a><h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><blockquote><p>Git一款强大且应用广泛的分布式版本控制软件。</p></blockquote><p>本手册参考的是<a href="https://morvanzhou.github.io/tutorials/others/git/" target="_blank" rel="noopener">莫烦教学</a>和<a href="https://backlog.com/git-tutorial/cn/" target="_blank" rel="noopener">Git-Tutrial</a>。笔者边学习边做笔记加深印象，也方便以后查阅。</p><h1 id="2-基本操作"><a href="#2-基本操作" class="headerlink" title="2 基本操作"></a>2 基本操作</h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/007v1rTBgy1fxscvcez2yj30m8096jrg.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li>查看提交(<code>commit</code>)记录：<ul><li>详细：<code>git log</code>。</li><li>简洁：<code>git log --oneline</code>。</li></ul></li><li>查看文件状态：<ul><li>详细：<code>git status</code>。</li><li>简洁：<code>git status -s</code>。</li></ul></li><li>添加修改：<ul><li>对所有文件：<code>add .</code>。</li><li>对单个文件：<code>add file_name</code>。</li></ul></li><li>提交修改：<code>git commit -m &quot;comments&quot;</code>。</li><li>添加并提交修改：<code>git commit -am &quot;comments&quot;</code>。(仅对已被添加到管理库的文件有效)</li><li>查看文件的不同：<ul><li><code>$ git diff</code>：查看这次还没<code>add</code>(<code>unstaged</code>)的修改部分。</li><li><code>$ git diff --cached</code>：如果已经<code>add</code>了这次修改,文件变成了可提交状态<code>(staged)</code>,我们可以在<code>diff</code>中添加参数<code>--cached</code>来查看修改。</li><li><code>$ git diff head</code>：查看<code>add</code>过(<code>staged</code>)和没<code>add</code>(<code>unstaged</code>)的修改。</li></ul></li><li>将本地记录发布：<code>git push</code>。</li></ol><h1 id="3-回到从前"><a href="#3-回到从前" class="headerlink" title="3 回到从前"></a>3 回到从前</h1><h2 id="3-1-针对整个版本库"><a href="#3-1-针对整个版本库" class="headerlink" title="3.1 针对整个版本库"></a>3.1 针对整个版本库</h2><ol><li>把这次的修改加入添加到上次的<code>commit</code>中，而不产生新的<code>commit</code>(但版本号会改变)：<ul><li>不修改<code>comment</code>：<code>git commit --amend -no-edit</code>。</li><li>修改<code>comment</code>：<code>git commit --amend -m &quot;commets&quot;</code>。</li></ul></li><li>把经过<code>add</code>后(<code>staged</code>)文件回退到<code>add</code>之前(<code>modified</code>)：<code>git reset file_name</code>。</li><li>版本跳跃：<ul><li>当前版本的第前n个版本：<ul><li>每个上角标代表回退一个版本：<code>git reset --hard HEAD^</code>。</li><li>回退n个版本：<code>git reset --hard HEAD~n</code>。(当n取0时可回退到当前版本最初的<code>unmodified</code>状态)</li></ul></li><li>指定版本号的版本：<code>git reset --hard HEAD 3f33f33</code>。<ul><li>通过<code>git log</code>获取当前版本及之前的版本号。</li><li>通过<code>git reflog</code>获取<code>HEAD</code>的指向记录，包含了所有版本的版本号。</li></ul></li><li>指定<code>HEAD</code>记录的版本：<code>git reset --hard HEAD@{1}</code>。(需先在<code>git reflog</code>中找到所求版本的<code>HEAD</code>记录)</li></ul></li></ol><h2 id="3-2-针对单个文件"><a href="#3-2-针对单个文件" class="headerlink" title="3.2 针对单个文件"></a>3.2 针对单个文件</h2><p>回退单个文件到指定版本号：<code>git checkout 3f33f33 -- file</code>。</p><h1 id="4-分支管理"><a href="#4-分支管理" class="headerlink" title="4 分支管理"></a>4 分支管理</h1><ol><li>新建分支：<ul><li>新建不切换到该分支：<code>git branch branch_name</code>。</li><li>新建且切换到该分支：<code>git checkout -b branch_name</code>。</li></ul></li><li>删除指定分支：<code>git branch -d branch_name</code></li><li>查看所有分支：<code>git branch</code>。(<code>*</code>号表示当前所处分支)</li><li>切换分支：<code>git checkout branch_name</code>。(<code>HEAD</code>指向该分支)</li><li>合并分支：<ul><li><code>merge</code>：<ul><li>fast-forward：<code>git merge branch_name</code>。</li><li>no-fast-forward：<code>git merge --no-ff &quot;comments&quot; branch_name</code>。</li></ul></li><li><code>rebase</code>:<code>git rebase branch_name</code>。</li></ul></li><li><code>merge</code>冲突：通过人工修改来解决冲突，之后可使用<code>git commit -am &quot;comments&quot;</code>添加并提交。</li><li><code>rebase</code>冲突：通过人工修改来解决冲突，之后可使用<code>git add file_name</code>和<code>git rebase --continue</code>添加并提交。</li><li>临时修改：<ul><li>暂存修改：<code>git stash</code>。</li><li>其他任务：一般新建分支来完成其他任务，然后合并。</li><li>恢复暂存：<ul><li>查看缓存：<code>git stash list</code>。</li><li>恢复缓存：<code>git stash pop</code>。</li></ul></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;穿越过去和未来！&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;https://wx1.sinaimg.cn/wap690/007v1rTBgy1fxscc4kjd5j31hc0tch5w.jpg&quot; alt=&quot;&quot; title=&quot;&quot;&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="Git" scheme="http://yvanzh.top/categories/Git/"/>
    
    
      <category term="Git" scheme="http://yvanzh.top/tags/Git/"/>
    
  </entry>
  
</feed>
