{"meta":{"title":"YvanZh's Blog","subtitle":"Goketsu Monogatari","description":"Nothing For Nothing","author":"YvanZh","url":"http://yvanzh.top","root":"/"},"pages":[{"title":"关于我","date":"2018-08-15T03:11:11.000Z","updated":"2019-06-28T13:50:57.501Z","comments":true,"path":"about/index.html","permalink":"http://yvanzh.top/about/index.html","excerpt":"","text":"萌萌哒的时间轴@timeline{ 2018@item{ 8月15日🖖博客初建🖖 } @item{ 7月15日👏混入广东工业大学计算机学院读研👏 } @item{ 7月1日🎊大学毕业啦🎊 } 2014@item{ 9月1日🎉入学西北大学数学学院🎉 } } 座右铭@card{ 我深知自己的内心，也了解别人。我生来便和我所见到的任何人都不同；甚至我敢自信全世界也找不到一个生来像我这样的人。虽然我不比别人好，至少和他们不一样。大自然塑造了我，然后把模子打碎了。 《忏悔录》 ——卢梭 } 三大爱好@column-3{ @card{ 音乐} @card{ 阅读} @card{ 健身} } 一个信条@card{ 无限的可能，不断的进步，简单的心。}"},{"title":"哲思","date":"2018-08-16T10:27:49.000Z","updated":"2020-08-29T03:04:03.245Z","comments":true,"path":"cogito/index.html","permalink":"http://yvanzh.top/cogito/index.html","excerpt":"","text":"@card{ }"},{"title":"Categories","date":"2020-01-01T06:45:02.609Z","updated":"2019-06-28T13:50:57.502Z","comments":false,"path":"categories/index.html","permalink":"http://yvanzh.top/categories/index.html","excerpt":"","text":""},{"title":"远方","date":"2018-08-16T10:27:49.000Z","updated":"2020-08-29T03:04:23.929Z","comments":true,"path":"faraway/index.html","permalink":"http://yvanzh.top/faraway/index.html","excerpt":"","text":"@card{ }"},{"title":"音乐","date":"2018-08-16T10:27:49.000Z","updated":"2020-08-29T03:04:21.543Z","comments":true,"path":"music/index.html","permalink":"http://yvanzh.top/music/index.html","excerpt":"","text":"@card{ } 我是只会歌唱的傻瓜~云村传送门：热血丶白菜"},{"title":"Tags","date":"2020-08-29T03:04:38.266Z","updated":"2019-06-28T13:50:57.510Z","comments":false,"path":"tags/index.html","permalink":"http://yvanzh.top/tags/index.html","excerpt":"","text":""},{"title":"诗意","date":"2018-08-16T10:27:49.000Z","updated":"2020-08-29T03:04:41.571Z","comments":true,"path":"poetry/index.html","permalink":"http://yvanzh.top/poetry/index.html","excerpt":"","text":"@card{ } @card{ 我常常觉得这是多么奇怪啊：每个人爱着自己都超过爱所有其他人，但他重视别人关于他的意见，却更甚于重视自己关于自己的意见。 《沉思录》 ——马克·奥勒留 } @card{ 因为害怕自己并非明珠而不敢刻苦琢磨，又因为有几分相信自己是明珠，而不能与瓦砾碌碌为伍，遂逐渐远离世间，疏避人群，结果在内心不断地用愤懑和羞怒饲育着自己懦弱的自尊心。世上每个人都是驯兽师，而那匹猛兽，就是每人各自的性情。对我而言，猛兽就是这自大的羞耻心了。老虎正是它。我折损自己，施苦妻儿，伤害朋友。末了，我就变成了这副与内心一致的模样。如今想起来，我真是空费了自己那一点仅有的才能，徒然在口头上卖弄着什么“人生一事不为则太长，欲为一事则太短”的警句，可事实是，唯恐暴露才华不足的卑怯的畏惧，和厌恶钻研刻苦的惰怠，就是我的全部了。但远比我缺乏才华，可由于专念磨砺而成就堂堂诗家的，也颇不乏其人。成为老虎后的今天，我才总算看到了这一点。每当念及此处，即便现在也感到胸口被烧灼一般的悔恨。 《山月记》 ——中岛敦 } @card{ 人生的困扰大抵来自四个方面：不可避免的死亡，内心深处的孤独感，我们追求的自由，以及生活并无显而易见的意义可言。 ——欧文·亚隆 } @card{ 一生中我多次撒谎却始终诚实地遵守着一个儿时的诺言因此，那与孩子的心不能相容的世界再也没有饶恕过我 《结局或开始》 ——北岛 } @card{ 我永恒的灵魂，注视着你的心，纵然黑夜孤寂，白昼如焚。 《地狱一季·永恒》 ——阿尔蒂尔·兰波 } @card{ 未表达的情绪永远不会消亡，他们只是被活埋，并将在未来以更加丑陋的的方式涌现出来。 ——弗洛伊德 } @card{ 一个有趣的悖论是，当我接受自己的原本的样子时，我就能改变了。 ——卡尔·罗杰斯 } @card{ 一个人能观察落叶，羞花，从细微处欣赏一切，生活就不能把他怎么样。 ——毛姆 } @card{ 人类总是从自己选择的人生，看向自己没有选择的另一种人生，感到羡慕，感到后悔。 ——川村元气 } @card{ 人生碌碌，竞短论长，却不道荣枯有数，得失难量。 ——沈复 } @card{ 通常，过去的事情是因为熟透了，才从你身上掉下去。 ——西朵妮·科莱特 } @card{ 人不是活一辈子，也不是活几年几月几天，而是活那么几个瞬间。 ——帕斯捷尔纳克 } @card{ 真心喜欢过的人没法做朋友。因为再多看几眼，都还是想拥有。 ——凉劫 } @card{ 你可以剪掉所有的花，但不能阻止春天的来临。 ——聂鲁达 } @card{ 对骗子的惩罚，倒不是没有人相信他，而是他无法相信任何人。 ——萧伯纳 } @card{ 我喜欢我四岁的时候，怀疑一切的眼光。 《对照记》 ——张爱玲 } @card{ 只要你尝试过飞，日后走路时也会仰望天空，因为那是你曾经到过，并渴望回去的地方。 ——达·芬奇 } @card{ 明确的爱，直接的厌恶，真诚的喜欢。站在太阳下的坦荡，大声无愧地称赞自己。 《沿着塞纳河到冷翡翠》 ——黄永玉 } @card{ 人总是在接近幸福的时候倍感幸福，在幸福进行时却患得患失。 ——张爱玲 } @card{ 我们所过的每个平凡的日常，也许就是连续发生的奇迹。 《日常》 ——石原立也 } @card{ 有时我常常觉得，人活着就像在泥土里上行走，太过风轻云淡，回过头就会遗憾什么都没留下，连个脚印都没有，但是心里装的东西太重，一不小心就会陷进去，难以自拔。 《山月不知心底事》 ——辛夷坞 } @card{ 可能不能一下子过上我想要的生活，但一点点小小的改变，足以给我继续生活下去的勇气。 《无法成为野兽的我们》 ——野木亚纪子 } @card{ 不应当急于求成，应当去熟悉自己的研究对象，锲而不舍，时间会成全一切。凡事开始最难，然而更难的是何以善终。 ——莎士比亚 } @card{ 时间会带你去最正确的人身边，请先好好爱自己，然后那个还不知道在哪里的人，会来接你。 《岁月的童话》 ——高畑勋 } @card{ 我不再装模做样地拥有很多朋友，而是回到了孤单之中，以真正的我开始了独自的生活。有时我也会因为寂寞而难以忍受空虚的折磨，但我宁愿以这样的方式来维护自己的自尊，也不愿意耻辱为代价去换取那种表面的朋友。 《在细雨中呐喊》 ——余华 } @card{ 每一个不曾起舞的日子，都是对生命的辜负。 ——尼采 } @card{ 照自己的意愿一息尚存，也好过听别人的安排，虚张声势地过着浅薄的生活。 《我要快乐，不必正常》 ——珍妮特·温特森 } @card{ 去成为你想成为的人吧，永远不要让别人告诉你应该变成什么样。 《长靴皇后》 ——哈维·费斯特恩 } @card{ 生活总是这样，不能叫人处处都满意。但我们还要热情地活下去。人活一生，值得爱的东西很多，不要因为一个不满意，就灰心。 《人生》 ——路遥 } @card{ 醉过才知道酒浓，爱过才知道情重。你不能成为我的诗，正如我不能做你的梦。 《梦与诗》 ——胡适 } @card{ 我说不出来为什么爱你，但我知道，你就是我不爱别人的理由。 《虞美人盛开的山坡》 ——宫崎吾朗 } @card{ 从现在起，我要开始谨慎地选择我的生活，我不再轻易让自己迷失在各种诱惑里。我心中已经听到来自于远方的呼唤，再不需要回过头去关心身后的种种是非与议论。我已无暇顾及过去，我要向前走。 《生命不能承受之轻》 ——米兰·昆德拉 } @card{ 孤独这两字拆开来看，有孩童，有瓜果，有小犬，有蝴蝶，足以撑起一个盛夏傍晚的巷子口，人情味儿十足。稚儿擎瓜柳棚下，细犬逐蝶窄巷中，人间繁华多笑语，唯我空余两鬓风。孩童水果猫狗飞蝶当然热闹，可都和你无关，这就叫孤独。 ——林语堂 } @card{ 你没有如期归来，而这正是是离别的意义。 《白日梦》 ——北岛 } @card{ 人生在世，还不是有时笑笑人家，有时给人家笑笑。 ——林语堂 } @card{ 三样东西有助于缓解生命的疲劳：希望，睡眠和微笑。 ——康德 } @card{ 青春是面对现实去想象的能力，而不是按照别人的想象来欺骗自己。 《人生枷锁》 ——毛姆 } @card{ 人永远不知道，谁哪次不经意地跟你说了再见之后，就真的不会再见了。 《千与千寻》 ——宫崎骏 } @card{ 为了快乐，我们都对自己撒谎。 《记忆碎片》 ——克里斯托弗·诺兰 } @card{ 当我们这代人在游戏里砥砺成长，超越地域结交最真实的友谊，跨过虚拟收获最纯真的爱情。当游戏为代表的流行文化终于带上光环，成为被传颂的伟大故事，让我们激动欢笑流泪，这意味着，一代人的怕与爱被认可了。 《头号玩家》 ——恩斯特·克莱恩 } @card{ 我用尽了全力，过着平凡的一生。 《月亮与六便士》 ——毛姆 } @card{ 要是没有离别和重逢，要是不敢承担欢愉和悲痛，灵魂还有什么意义，还叫什么人生。 ——舒婷 } @card{ 不论被说些什么，我就是闪亮的水珠，冰冷的水滴，晶莹通透的雨点，结满枝头的年轻的山茱萸树。 《不畏风雨》 ——宫泽贤至 } @card{ 总有些人觉得你怎么做都不对，也许你唯一的不对就是理会他们。 《脱线森林》 ——狐狸阿北 } @card{ 我只有一辈子，对吧？但我有时候想成为别的人，有时候我觉得自己是在为没可能活过的另一种人生去奋斗，我只希望它能有点价值。 《达拉斯买家俱乐部》 ——让-马克·瓦雷 } @card{ 有我所不乐意的，在天堂里，我不愿意去；有我所不乐意的，在地狱里，我不愿意去；有我所不乐意的，在你们的世界里，我不愿意去。我只愿蓬勃生活在此时此刻，无所谓去哪，无所谓见谁。那些我将要去的地方，都是我素未谋面的故乡。那些我将要见的人，都会成为我的朋友。以前是以前，现在是现在。我不能选择怎么生，怎么死；但我能决定怎么爱，怎么活。这是我要的自由，我的黄金时代。 《黄金时代》 ——李樯 } @card{ 生命本来就是悲伤而严肃的，我们来到这个美好的世界里，彼此相逢，彼此问候，并结伴同游一段短暂的时间，然后我们就失去了对方，并且莫名其妙就消失了，就像我们突然莫名其妙地来到世上一般。 《苏菲的世界》 ——乔斯坦·贾德 } @card{ 取次花丛懒回顾，半缘修道半缘君。 《离思五首·其四》 ——元稹 } @card{ 哪里会有人喜欢孤独，不过是不喜欢失望罢了。 《挪威森林》 ——村上春树 } @card{ 不管活到什么岁数，总有太多思索、烦恼和迷惘，一个人如果失去这些，安于现状，才是真正意义上的青春的完结。 ——渡边淳一 } @card{ 你不是爱情的终点，只是爱情的原动力。我将这爱情献给路旁的花朵，献给这玻璃酒杯里摇晃着晶亮的阳光，献给这教堂红色圆顶。因为你，我爱上了这个世界。 《堤契诺之歌》 ——赫尔曼·黑塞 } @card{ 草在结它的种子，风在摇它的叶子，我们站着，不说话，就十分美好。 《门前》 ——顾城 } @card{ 不管前方的路有多苦，只要走的方向正确，不管多么崎岖不平，都比站在原地更接近幸福。 《千与千寻》 ——宫崎骏 } @card{ 让我与你握别，再轻轻抽出我的手，知道思念从此生根，浮云白日、山川庄严温柔。 《渡日》 ——席慕容 } @card{ 我的不幸，恰恰在于我缺乏拒绝别人的能力。我害怕一旦拒绝别人，便会在彼此的心里留下永远不可愈合的裂痕。 《人间失格》 ——太宰治 } @card{ 假如你不够快乐，也不要把眉头深锁。人生本来就短暂，为什么还要栽培苦涩。 《假如你不够快乐》 ——汪国真 } @card{ 生活总是让我们遍体鳞伤，但是后来，那些受伤的地方一定会变成我们最强壮的地方。 《永别了，武器》 ——海明威 } @card{ 你问我爱你值不值得，你应该知道的，爱就是不问值不值得。 ——张爱玲 } @card{ 哪种比较孤独：是活在自己的世界里，谁也不爱，还是心里爱着一个人，却始终无法向爱靠近？ 《质数的孤独》 ——保罗·乔尔达诺 } @card{ Genius is nothing but enduring patience.天才，无非是长久的忍耐。 ——福拜楼 } @card{ 生活不可能像你想象得那么好，但也不会像你想象得那么糟。我觉得人的脆弱和坚强都超乎自己的想象。有时，我可能脆弱得一句话就泪流满面；有时，也发现自己咬着牙走了很长的路。 《一生》 ——莫泊桑 }"}],"posts":[{"title":"2020秋招冲鸭！！！","slug":"Coding","date":"2020-08-11T12:32:39.000Z","updated":"2020-09-03T13:06:14.149Z","comments":true,"path":"2020/08/11/Coding/","link":"","permalink":"http://yvanzh.top/2020/08/11/Coding/","excerpt":"","text":"一些有意思的题24点游戏题目描述问题描述：给出4个1-10的数字，通过加减乘除，得到数字为24就算胜利输入：4个1-10的数字。[数字允许重复，但每个数字仅允许使用一次，测试用例保证无异常数字]输出：true or false 输入描述: 输入4个int整数 输出描述: 返回能否得到24点，能输出true，不能输出false 示例1输入 7 2 1 10 输出 true 解答 def p24(a): #返回条件 if len(a)== 1: if sum(a)== 24: return 1 else: return 0 #递归 for i in range(0,len(a)): for j in range(i+1,len(a)): p,q = a[i],a[j] for k in range(4): b = a[::] del b[j] del b[i] c = b[::] if k == 0: b.append(p+q) elif k == 1: b.append(p-q) c.append(q-p) elif k == 2: b.append(p*q) else: if q == 0: continue b.append(p/q) if p == 0: continue c.append(q/p) if p24(b) == 1 or p24(c)== 1: return 1 return 0 while True: try: a = list(map(int,input().split(&#39; &#39;))) if p24(a)==1: print(&#39;true&#39;) else: print(&#39;false&#39;) except: break","categories":[{"name":"Coding","slug":"Coding","permalink":"http://yvanzh.top/categories/Coding/"}],"tags":[{"name":"Coding","slug":"Coding","permalink":"http://yvanzh.top/tags/Coding/"}]},{"title":"Unsupervised Domain Adaptation via Structured Prediction Based Selective Pseudo-Labeling","slug":"Paper-Notes-8","date":"2019-12-09T05:32:39.000Z","updated":"2020-01-01T07:07:31.797Z","comments":true,"path":"2019/12/09/Paper-Notes-8/","link":"","permalink":"http://yvanzh.top/2019/12/09/Paper-Notes-8/","excerpt":"","text":"1 简介文章来源：2020-AAAI 文章代码 文章主旨：本文利用LPP保留原始空间的局部信息来学习投影子空间，每一次投影后采用NCP与SP结合的SPL方法对目标域样本计算软标签，并以软标签为基准在下一次迭代时选择性地添加目标域样本到特征矩阵中。通过指定次数的迭代，实现无监督的领域适应。 2 详情论文的方法框架如图所示： 2.1 Dimensionality Reduction利用PCA对原始特征矩阵$X\\in \\mathbb{R}^{m\\times n}$进行降维。由于PCA的降维是线性的，所以对降维后的$\\tilde{X}$每一列向量实行$L_2$归一化，即$\\tilde{x} \\leftarrow \\tilde{x}/ \\Vert \\tilde{x} \\Vert_2$。 2.2 Domain Alignment应用Supervised LPP学习一个保持原始空间局部结构的投影子空间： \\min_{P} \\sum_{i,j} \\Vert P^T\\tilde{x}_i -P^T\\tilde{x}_j \\Vert_2^2 M_{ij} \\tag{1}其中$M_{ij}$是根据标签和伪标签生成的相似矩阵。当$y_i=y_j$时，$M_{ij}$值为1；否则其值为0。 在原LPP中有一段：“度矩阵$D$提供了一个对数据点的天然测度，即$D_{ii}$的值越大，那么对应的点$P^T\\tilde{x}_i$就越重要”。这段话需要这样来理解，我们知道$D_{ii} = \\sum_{j}M_{ij}$。其值越大说明它与其他点的关联越密切，也就越重要。为了凸显这个重要性，LPP施加了一个对$P$的列向量$p$约束$p^T\\tilde{X}D\\tilde{X}^Tp = 1$，其转化为矩阵形式即为： P^T \\tilde{X} D \\tilde{X}^T P = I \\tag{2}然后考虑对投影矩阵$P$中的极大值添加正则化项： \\Vert P \\Vert_F^2 \\tag{3}那么结合(1)(2)(3)式，我们得到： \\min_{P} Tr(P^T\\tilde{X}L\\tilde{X}^TP) + Tr(P^TP) \\quad s.t \\, P^T\\tilde{X}D\\tilde{X}^TP = I \\tag{4}其中$L=D-M$。进一步，(4)式可化简为： \\min_{P} \\frac{Tr(P^T(\\tilde{X}L\\tilde{X}^T+I)P) } {P^T\\tilde{X}D\\tilde{X}^TP} \\tag{5}通过上式(5)求解$P$需要运用广义特征值分解：$Ap=\\lambda B p$，其中A为n阶实对称矩阵，B为n阶实对称正定矩阵，$p_0,\\dotsb,p_{m-1}$为P的列向量。在这里与(5)式对应的广义特征值分解为： \\tilde{X}D\\tilde{X}^Tp=\\lambda (\\tilde{X}L\\tilde{X}^T+I)p \\tag{6}最后把按照对应特征值从大到小的顺序将特征向量$p$排列成$P$，这里所选择维度也就是所学习的子空间的维度。 2.3 Pseudo-Labeling为目标域的样本打上的伪标签一般采用软标签的形式，因为错误的标签有时会导致模型在优化时被错误引导。 下文简单比较了NCP、SP和本文提出的SPL方法。 2.3.1 Pseudo-Labeling via Nearest Class Prototype (NCP)经过投影后，源域和目标域样本在子空间中的表示为：$z^s =P^T\\tilde{x}^s$，$z^t =P^T\\tilde{x}^t$。 然后对数据进行中心化：$z\\leftarrow z -\\bar{z}$，其中$\\bar{z}$表示均值。 接着再次利用$L_2$正则化来提高不同类之间的分离性：$z\\leftarrow z/\\Vert z \\Vert_2$。 之后对源域的每个类$y \\in \\mathcal{Y}$，以求均值的方式求出其原型的位置： \\bar{z}_{y}^s = \\frac{\\sum_{i=1}^{n_s}z_i^s\\delta(y,y_i^s)}{\\sum_{i=1}^{n_s}\\delta(y,y_i^s)} \\tag{7}其中$\\delta(y,y_i)=1$当且仅当$y=y_i$,否则为0。然后对类原型$\\bar{z}_{y}^s$也实施一次$L_2$正则化。 最后通过计算高斯核函数作为条件概率，为目标域样本打上伪标签： P_1(y|x^t) = \\frac{\\exp(-\\Vert z^t- \\bar{z}_y^s\\Vert)}{\\sum_{y=1}^{\\vert \\mathcal{Y} \\vert} \\exp(-\\Vert z^t - \\bar{z}_y^s \\Vert)} \\tag{8}其中$\\vert \\mathcal{Y} \\vert$表示类的个数。显然这里的伪标签是以软标签的形式存在的。 2.4.2 Pseudo-Labeling via Structured Prediction (SP)NCP只考虑了源域样本的信息（以源域样本中心为类的原型），而没有利用目标域样本的内蕴结构。于是SP方法考虑对投影后的目标域样本$z^t$使用K-means来分离出$\\vert \\mathcal{Y} \\vert$个类，其中初始化的中心点由(7)式来计算得出。之后利用最终得到的中心点$\\bar{z}^t$与投影后的源域样本的均值点$\\bar{z}^s$依次进行最近距离匹配，从而确定每个中心点$\\bar{z}^t_y$的类别。最后通过计算条件概率给目标域样本打上软标签： P_1(y|x^t) = \\frac{\\exp(-\\Vert z^t- \\bar{z}_y^t\\Vert)}{\\sum_{y=1}^{\\vert \\mathcal{Y} \\vert} \\exp(-\\Vert z^t - \\bar{z}_y^t \\Vert)} \\tag{9}2.4.3 Iterative Learning with Selective Pseudo-Labeling (SPL)可以看到NCP和SP分别以源域信息和目标域信息为基准来计算条件概率。于是SPL主张简单结合二者： p(y|x^t)=\\max\\{p_1(y|x^t),p_2(y|x^t)\\} \\tag{10}最终，$x^t$的伪标签被预测为： \\hat{y}^t = \\arg \\max_{y\\in \\mathcal{Y}} p(y|x^t) \\tag{11}此外，在学习投影矩阵的时候本文运用了一个技巧，即逐步添加目标域样本到特征矩阵$X$中。给定最大迭代次数$T$，第$k$次迭代时$X$中包含$kn_t/T$个目标域样本。为了避免某高概率类选择的样本太多，对每个伪标签$c\\in \\mathcal{Y}$样本按照最大概率排序选择前$kn^c_t/T$个。 算法流程如下图所示： 该模型实验效果比较显著，甚至超过了TADA、MEDA、Symnet等深度模型。 此外，作者进行了Ablation Study，即对pseudo-labeling (PL)，sample selection (S) for pseudo-labeling，nearest class prototype (NCP) 和 structured prediction (SP)这四个部分进行不同的组合。最后得出四个部分都使用的时候性能是最好的。 3 启发 $L_2$归一化提高样本分离度。 本文的SPL依然把目标域和源域的信息割裂开来了。能否把对所有样本实施K-means，得到然后在每一个簇中源域样本哪一类多，则这一簇中的目标域样本就为哪一类。 或可使用其他聚类方法，分层聚类，原型聚类，密度聚类等. 能否利用低秩表示来计算相似度矩阵。","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Domain Adaptation","slug":"Domain-Adaptation","permalink":"http://yvanzh.top/tags/Domain-Adaptation/"}]},{"title":"My First Paper","slug":"My-First-Paper","date":"2019-08-08T10:01:33.000Z","updated":"2020-01-01T07:00:28.550Z","comments":true,"path":"2019/08/08/My-First-Paper/","link":"","permalink":"http://yvanzh.top/2019/08/08/My-First-Paper/","excerpt":"5.1~8.7日，笔者在耕耘学术生涯中的第一篇论文，所以暂时搁置了博客。现在，盆友们，我胡汉三又回来惹！","text":"5.1~8.7日，笔者在耕耘学术生涯中的第一篇论文，所以暂时搁置了博客。现在，盆友们，我胡汉三又回来惹！ 此役历时三月有余。期间，由于实验结果达不到预期效果，于是笔者修改了模型，最后在六月下旬才做完实验。而后又马不停蹄地在参考文献中摸爬滚打，定型论文主线和框架。目前，论文的初稿已经写好了。接下来有两周的暑假时间，笔者准备回家好好休整，麻辣小龙虾等我很久了hiahiahia~🐱‍🏍 如果后续把论文挂在arXiv，代码挂在GitHub，笔者将会在此篇博文中添加相关链接。","categories":[{"name":"Researching","slug":"Researching","permalink":"http://yvanzh.top/categories/Researching/"}],"tags":[{"name":"Researching","slug":"Researching","permalink":"http://yvanzh.top/tags/Researching/"}]},{"title":"二三年华","slug":"23-years-old","date":"2019-04-30T03:11:11.000Z","updated":"2020-08-29T02:28:17.106Z","comments":true,"path":"2019/04/30/23-years-old/","link":"","permalink":"http://yvanzh.top/2019/04/30/23-years-old/","excerpt":"","text":"淡墨青衫，躬植荆棘。心囿象牙，身陷囹圄。毋宁刺猝以醒迷痴，不信宿命，仍抱愿景。尝求思愆而量得失，既怀逸兴，何惧天青。隔世恍恍，明月悠悠，旦辰只道，五载情愁。离人，走马。归客，行舟。乘风路，沐雨途。何处相逢，知音如故。望尽长安花，饮罢珠江水，梦回洞庭湖。","categories":[{"name":"Life","slug":"Life","permalink":"http://yvanzh.top/categories/Life/"}],"tags":[{"name":"Life","slug":"Life","permalink":"http://yvanzh.top/tags/Life/"}]},{"title":"Deep Self-Evolution Clustering","slug":"Paper-Notes-7","date":"2019-04-25T12:32:39.000Z","updated":"2020-08-29T02:28:17.121Z","comments":true,"path":"2019/04/25/Paper-Notes-7/","link":"","permalink":"http://yvanzh.top/2019/04/25/Paper-Notes-7/","excerpt":"","text":"1 简介To be continue 如今的聚类任务的难点并不是在于方法，而是在于确定任务数据的预设聚类模式。 传统的聚类方法依然需要分步进行聚类。比如将特征提取得到的特征来做聚类，在做聚类之前，数据的特征就是已经固定好了，也就是说在做聚类的过程中，数据特征是一成不变，也就无法使得聚类产生更好的效果。 2 详情 DSEC","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"http://yvanzh.top/tags/Clustering/"}]},{"title":"Unsupervised and Semi-Supervised Learning via ℓ1-Norm Graph","slug":"Paper-Notes-6","date":"2019-04-21T13:15:54.000Z","updated":"2020-08-29T02:28:17.121Z","comments":true,"path":"2019/04/21/Paper-Notes-6/","link":"","permalink":"http://yvanzh.top/2019/04/21/Paper-Notes-6/","excerpt":"","text":"1 简介文章来源：2011-ICCV 文章主旨: 由于图拉普拉斯无法直接得到好的聚类结构，后续需要利用$K$-means来进行聚类。然而$K$-means聚类容易局部收敛，且得到非唯一的聚类结果。 对于半监督学习来说，图拉普拉斯一般使用的都是采用的二次型的图嵌入（TODO），然而这种方式对噪声和异常值敏感。 作者提出一种$\\ell_1$范数约束的谱聚类框架，且扩展了一个半监督学习模型，并证明了其收敛性。 2 基本框架2.1 谱聚类对于$Q = \\left[q_{1}, q_{2}, \\cdots, q_{c}\\right] \\in \\mathbb{R}^{n \\times c}$，$q_{k} \\in \\mathbb{R}^{n \\times 1}$为$Q$的第$k$列。 Graph Ratio Cut： \\begin{gathered} \\min _{Q^{T} Q=I} \\operatorname{Tr}\\left(Q^{T} L Q\\right) \\\\ q_k = (0, \\cdots, 0, \\overbrace{\\frac{1}{\\sqrt{n_{k}}}, \\cdots, \\frac{1}{\\sqrt{n_{k}}}}^{n_k}, 0, \\cdots, 0)^{T} \\end{gathered} \\tag{1}Graph Normalized Cut： \\begin{gathered} \\min _{Q^{T}DQ=I} \\operatorname{Tr}\\left(Q^{T} L Q\\right) \\\\ q_{k}=(0, \\cdots, 0, \\overbrace{\\frac{1}{\\sqrt{q_{k}^{T} D q_{k}}}, \\cdots, \\frac{1}{\\sqrt{q_{k}^{T} D q_{k}}}}^{n_k}, 0, \\cdots, 0)^{T} \\end{gathered} \\tag{2}详情可参见谱聚类（spectral clustering）原理总结一文。 2.2 基本模型将$(2)$式写为： \\min _{Q^{T} D Q=I} \\sum_{i, j=1}^{n} W_{i j}\\left\\|q^{i}-q^{j}\\right\\|_{2}^{2} \\tag{3}$Q$的理想解就是当$x_i,x_j$属于同一类时，使得$q^i = q^j$。也就是说$Q$的许多行都是相等的，这样就有了很强的聚类结构。那么我们想要很多对$(i,j)$有$\\left|q^{i}-q^{j}\\right|_{2}=0$，那么用$\\ell_1$范数来解决这个问题也是等价的： \\min _{Q^{T} D Q=I} \\sum_{i, j=1}^{n} W_{i j}\\left\\|q^{i}-q^{j}\\right\\|_{2} \\tag{4}设一个$n^2$维向量$p$，其$((i-1)*n+j)$个元素为$W_{i j}\\left|q^{i}-q^{j}\\right|_{2}$。那么我们将$(4)$式写为： \\min _{Q^{T} D Q=I}\\|p\\|_{1} \\tag{5}这样我们可以直观地知道$p$的元素会由于$\\ell_1$范数的约束而变得稀疏，也就为$Q$提供了一个理想的聚类结果。 2.3 优化算法$(4)$式的拉格朗格日函数为： \\mathcal{L}(Q)=\\sum_{i, j=1}^{n} W_{i j}\\left\\|q^{i}-q^{j}\\right\\|_{2}-\\operatorname{Tr}\\left(\\Lambda\\left(Q^{T} D Q-I\\right)\\right) \\tag{6}设拉普拉斯矩阵$\\widetilde{L} = \\widetilde{D} -\\widetilde{W}$，$\\widetilde{D}$为第$i$个元素为$\\sum_j\\widetilde{W}_{ij}$的对角阵，$\\widetilde{W}$为： \\widetilde{W}_{i j}=\\frac{W_{i j}}{2\\left\\|q^{i}-q^{j}\\right\\|_{2}} \\tag{7}那么$\\mathcal{L}(Q)$对$Q$的偏导为零时，有： \\frac{\\partial \\mathcal{L}(Q)}{\\partial Q}=\\widetilde{L} Q-D Q \\Lambda=\\mathbf{0} \\tag{8}也就是说解$Q$为$D^{-1}\\widetilde{L}$的特征值。注意到$D^{-1}\\widetilde{L}$又依赖于$Q$，于是可以通过迭代来得到$Q$的局部最优解。 2.4 收敛性分析 引理1对任意非零向量$q$，$q_t \\in \\mathcal{R}^c$，有： \\|q\\|_{2}-\\|q\\|_{2}^{2} / 2\\left\\|q_{t}\\right\\|_{2} \\leq\\left\\|q_{t}\\right\\|_{2}-\\left\\|q_{t}\\right\\|_{2}^{2} / 2\\left\\|q_{t}\\right\\|_{2} \\tag{9} 我们需要利用引理1来证明算法可以收敛。 定理1算法1会在每次迭代中单调地降低问题$(4)$的目标，并接近于问题的局部最优。 证明： 根据算法1的第二步，可知： Q_{t+1}=\\arg \\min _{Q^{T} D Q=I} \\sum_{i, j=1}^{n}(\\widetilde{W}_{t})_{i j}\\left\\|q^{i}-q^{j}\\right\\|_{2}^{2} \\tag{10}又$\\left(\\widetilde{W}_{t}\\right)_{i j}=\\frac{W_{i j}}{2\\left|q_{t}^{i}-q_{t}^{j}\\right|_{2}}$，那么： \\sum_{i, j=1}^{n} \\frac{W_{i j}\\left\\|q_{t+1}^{i}-q_{t+1}^{j}\\right\\|_{2}^{2}}{2\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}} \\leq \\sum_{i, j=1}^{n} \\frac{W_{i j}\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}^{2}}{2\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}} \\tag{11}根据引理1，可得： \\begin{aligned} & \\sum_{i, j=1}^{n} W_{i j}\\left(\\left\\|q_{t+1}^{i}-q_{t+1}^{j}\\right\\|_{2}-\\frac{\\left\\|q_{t+1}^{i}-q_{t+1}^{j}\\right\\|_{2}^{2}}{2\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}}\\right) \\\\ & \\leq \\sum_{i, j=1}^{n} W_{i j}\\left(\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}-\\frac{\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}^{2}}{2\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}}\\right) \\end{aligned} \\tag{12}结合$(11)(12)$式，可得： \\sum_{i, j=1}^{n} W_{i j}\\left\\|q_{t+1}^{i}-q_{t+1}^{j}\\right\\|_{2} \\leq \\sum_{i, j=1}^{n} W_{i j}\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2} \\tag{13}因此，算法1将在每次迭代$t$中单调地降低问题$(4)$的目标，直到算法收敛。当达到收敛时，$(13)$式等号成立，则$Q_t$和$\\widetilde{L}_t$将满足$(8)$式，也就是问题$(4)$的$\\text{KKT}$条件。定理1得证。 3 半监督框架3.1 问题描述设$Y=[\\left(y^{1}\\right)^{T},\\left(y^{2}\\right)^{T}, \\cdots,\\left(y^{n}\\right)^{T}] \\in \\mathbb{R}^{n \\times c}$为初始的标签矩阵。若$x_i$为无标签数据，则$y^i=0$。若$x_i$为$k$类，则$y^i$的第$k$个元素为1，否则为0。 传统的半监督学习需要解决的问题如下： \\min _{Q} \\operatorname{Tr}\\left(Q^{T} L Q\\right)+\\operatorname{Tr}(Q-Y)^{T} U(Q-Y) \\tag{14}其中$L$为拉普拉斯矩阵，$U$为控制$x_i$初始标签$y^i$的影响程度的对角阵（相当于超参），$Q \\in \\mathcal{R}^{n \\times c}$为需要求解的标签矩阵。 类似的$(14)$式可写为： \\min _{Q} \\sum_{i, j=1}^{n} W_{i j}\\left\\|q^{i}-q^{j}\\right\\|_{2}^{2}+\\operatorname{Tr}(Q-Y)^{T} U(Q-Y) \\tag{15}为了得到最优解$Q$，我们需要解决以下半监督分类问题（注意这个用的是$\\ell_1$范数）： \\min _{Q} \\sum_{i, j=1}^{n} W_{i j}\\left\\|q^{i}-q^{j}\\right\\|_{2}+\\operatorname{Tr}(Q-Y)^{T} U(Q-Y) \\tag{16}3.2 优化算法$(16)$式对$Q$求偏导为零时有： \\widetilde{L} Q+U(Q-Y)=\\mathbf{0} \\Rightarrow Q=(\\widetilde{L}+U)^{-1} U Y \\tag{17}3.3 收敛性分析 定理1算法2会在每次迭代中单调地降低问题$(16)$的目标，并接近于问题的局部最优。 证明： 设$f(Q)=\\operatorname{Tr}(Q-Y)^{T} U(Q-Y)$，据算法2的第二步可知： Q_{t+1}=\\arg \\min _{Q} \\sum_{i, j=1}^{n}\\left(\\tilde{W}_{t}\\right)_{i j}\\left\\|q^{i}-q^{j}\\right\\|_{2}^{2}+f(Q) \\tag{18}注意到$(\\tilde{W}_{t})_{i j}=\\frac{W_{i j}}{2\\left|q_{t}^{i}-q_{t}^{j}\\right|_{2}}$，则： \\begin{aligned} & \\sum_{i, j=1}^{n} \\frac{W_{i j}\\left\\|q_{t+1}^{i}-q_{t+1}^{j}\\right\\|_{2}^{2}}{2\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}}+f\\left(Q_{t+1}\\right) \\\\ & \\leq \\sum_{i, j=1}^{n} \\frac{W_{i j}\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}^{2}}{2\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}}+f\\left(Q_{t}\\right) \\end{aligned} \\tag{19}将$(19)$式和$(12)$式两边求和，可得： \\begin{aligned} & \\sum_{i, j=1}^{n} W_{i j}\\left\\|q_{t+1}^{i}-q_{t+1}^{j}\\right\\|_{2}+f\\left(Q_{t+1}\\right) \\\\ & \\leq \\sum_{i, j=1}^{n} W_{i j}\\left\\|q_{t}^{i}-q_{t}^{j}\\right\\|_{2}+f\\left(Q_{t}\\right) \\end{aligned} \\tag{20}因此，算法2在每次迭代$t$中单调地降低问题$(16)$的目标，收敛时$Q_t$和$L_t$满足$(17)$式。由于问题$(16)$是一个凸优化问题，满足式$(17)$表明$Q_t$是问题$(16)$的全局最优解。因此，算法2收敛到问题$(16)$的全局最优。定理2得证。 4 思考 本文模型简单来说就是在Normalized Cut谱聚类的基础上，将$\\ell_2$范数约束项的平方等价为了一个$\\ell_1$范数，目的是为了构造出一个与$Q$相关的$\\widetilde{W}$，从而联系了$Q$和$L$并使得二者交替得以更新。相较于固定$L$的原始Normalized Cut算法来说，$L$的更新无疑带来了算法的提升。说穿了感觉就像是一个优化的小trick造就了一篇顶会。 从直观层面谈一谈聚类算法：（TODO） $K$-means。其一般采用欧氏距离所以只能局限于球形簇。距离度量的不确定性是算是聚类算法的通病。我们需要通过多次尝试，或者精细地分析所研究数据的特点来确定选取何种距离。$K$-means最大的问题是初始化过程对结果的影响非常大，对此有$K$-means++和二分$K$-means改进了初始化过程。而对噪点和异常值十分敏感的问题，出现了抛弃均值而转投中值怀抱的尝试（$K$-mediods）。 DESCAN。 分层聚类。 谱聚类。主要的两个问题， 流形聚类。 子空间聚类。","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"http://yvanzh.top/tags/Clustering/"},{"name":"Spectral Clustering","slug":"Spectral-Clustering","permalink":"http://yvanzh.top/tags/Spectral-Clustering/"}]},{"title":"Flexible Manifold Embedding:A Framework for Semi-Supervised and Unsupervised Dimension Reduction","slug":"Paper-Notes-5","date":"2019-04-13T07:52:52.000Z","updated":"2020-08-29T02:28:17.120Z","comments":true,"path":"2019/04/13/Paper-Notes-5/","link":"","permalink":"http://yvanzh.top/2019/04/13/Paper-Notes-5/","excerpt":"","text":"1 简介文章来源：2010-TPAMI 文章主旨： 很多降维方法都是使用线性映射$F=X^TW$（比如PCA，LDA，LPP，SDA）。它们简单高效，但是在实际应用中预测标签$F$位于训练样本所张成的低维空间中未免太多严格。 提出一个新的框架，同时优化预测标签$F$，线性回归函数$h(X)$和回归残差$F_0$。其结合了标签适配度和流形平滑度（其实指的就是局部信息或局部一致性）有关的两项，以及一个灵活的惩罚项$\\Vert F_0 \\Vert^2$。 后知后觉： 本文的模型依然是线性的，保留了线性映射的简单高效的特点，同时利用残差放宽了线性约束，很好避免了过拟合。在深度学习还没有兴起时，这无疑是个很好的方法。感觉何凯明也是吸取了残差的思想才创造了Res-Net，本文的影响可见一斑。 2 相关工作2.1 LGC and GFHFLocal and global consistency (LGC)： g_L(F) = \\frac{1}{2}\\sum_{i,j=1}^m \\Big\\Vert \\frac{F_{i.}}{\\sqrt{D_{ii}}}- \\frac{F_{i.}}{\\sqrt{D_{jj}}} \\Big \\Vert^2 S_{ij} +\\lambda\\sum _{i=1}^n \\Vert F_{i.} -Y_{i.} \\Vert^2 \\tag{1}Gaussian fields and harmonic functions (GFHF): g_G(F) = \\frac{1}{2} \\sum_{i,j=1}^m \\Vert F_{i.} - F_{j.} \\Vert^2 S_{ij} +\\lambda_{\\infty} \\sum_{i =1} ^n \\Vert F_{i.} -Y{i.} \\Vert^2 \\tag{2}其中配分系数$\\lambda$用来平衡流形平滑度和标签适配。$\\lambda_{\\infty}$是个非常大的数。 注意到$(1)(2)$式共享一个方程： \\operatorname{Tr}(F^TMF) +\\operatorname{Tr}(F-Y)^T U (F-Y) \\tag{3}其中$M \\in \\mathcal{R}^{m\\times m}$代表拉普拉斯矩阵，$U \\in \\mathcal{R}^{m \\times m}$为对角矩阵。 在LGC中，$M$表示一个归一化的拉普拉斯矩阵$\\hat{L}$，$U$表示元素为$\\lambda$的对角矩阵。 在GFHF中，$M$表示普通的拉普拉斯矩阵，$U$表示前n个元素和后$m-n$个元素分别为$\\lambda_\\infty$和$0$的对角矩阵。 2.2 MRManifold Regularization (MR) 扩展了许多现有的算法。比如岭回归和SVM，使得它们可以通过加入一个基于几何的正则化项来进行半监督学习。 来简单看一下LapRLS/L，它就是将MR对岭回归进行扩展，同时计算岭回归的误差并保留流形平滑度。其表示为： g_M(W,b) = \\lambda_A\\Vert W \\Vert^2 + \\lambda_I\\operatorname{Tr}r(W^TXLX^TW) + \\frac{1}{n} \\sum_{i =1}^n \\Vert W^Tx_i +b -Y_{i.}^T \\Vert^2 \\tag{4}2.3 SDASemi-Supervised Discriminant Analysis (SDA) 的核心假设依然是流形平滑，也就是在低维空间中相近的点具有相似的特征表示。 定义$X_l = [x_1,x_2,\\dots,x_n]$为有标签数据的矩阵。第$i$类的样本数为$n_i$。图相似矩阵$\\tilde{S}^w,\\tilde{S}^b \\in \\mathcal{R}^{n\\times n}$，其中$\\tilde{S}^w_{ij}= \\delta_{y_i,y_j}/n_{y_i}$，$\\tilde{S}^b_{ij}= (\\frac{1}{n})-\\tilde{S}^w_{ij}$。它们分别对应的拉普拉斯矩阵为$\\tilde{L}_w$和$\\tilde{L}_b$。 类内散度: S_w = \\sum_{i =1}^n(x_i - \\bar{x}_{y_i})(x_i - \\bar{x}_{y_i})^T = X_l\\tilde{L}_wX_l^T \\tag{5}类间散度： S_b = \\sum_{l =1}^c n_c ( \\bar{x}_l- \\bar{x})(\\bar{x}_l - \\bar{x})^T = X_l \\tilde{L}_b X_l^T \\tag{6}SDA： g_S(W) = \\frac{\\vert W^TX_l \\tilde{L}_bX^T_l W \\vert } {\\vert W^T(X_l(\\tilde{L}_w +\\tilde{L}_b)X_l^T +\\alpha XLX^T + \\beta I) W \\vert} \\tag{7}3 模型框架3.1 联系 LGC/GFHF 和 LapRLSL/LLGC/GFHF基于标签传播和随机游走而被提出，LapRLSL/L的提出为了对岭回归的进行半监督扩展。LGC/GFHF只能为现有的数据点找到一个合适的映射关系，而LapRLSL/L可以通过线性函数$h(x)$为新数据点提供一个映射。 命题1当拉普拉斯矩阵$M\\in \\mathcal{R}^{m\\times m}$满足$M\\mathbf{1} = 0$与$\\mathbf{1}^TM = 0^T，$LapRLSL/L是扩展到样本外的LGC/GFHF。 证明： 假设LGC/GFHF的解$F$位于由$X$张成的线性子空间中，比如$F= h(X)= X^TW + \\mathbf{1}b^T$。其中$W \\in \\mathcal{R}^{f\\times c}$为投影矩阵，$b\\in \\mathcal{R}^{c \\times 1}$为偏置项。那么LGC/GFHF的目标函数$(3)$式可被写为： \\begin{gathered} Tr[(X^TW + \\mathbf{1}b^T)^T M (X^TW + \\mathbf{1}b^T)] \\\\ + Tr[(X^TW + \\mathbf{1}b^T - Y )^T U(X^TW + \\mathbf{1}b^T -Y)] \\end{gathered} \\tag{8}接着添加一个正则化项$(\\lambda_A)/(\\lambda_I) \\Vert W \\Vert^2$，并设$M = L$，对角阵$U$的前$n$个元素和后$m-n$个元素分别为$(1)/(n\\lambda_A)$和$0$。则$(8)$式变为： \\frac{\\lambda_A}{\\lambda_I}\\Vert W \\Vert^2 \\operatorname{Tr}r(W^TXLX^TW) + \\frac{1}{n\\lambda_I}\\sum_{i =1}^n \\Vert W^Tx_i + b - Y_i^T \\Vert^2 \\tag{9}于是$(9)$式就等于$(1)/(\\lambda_I)g_M(W,b)$。命题1得证。 3.2 FME从命题1我们知道LapRLSL/L中的预测标签$F$也是被限制在由所有训练样本$X$所张成的空间中。尽管我们学得的线性函数可以映射新的数据点，但是$W$中的参数的个数并不依赖于样本的个数。因此，这个线性函数可能会过拟合来自非线性流形的训练样本。于是作者提出FME(Flexible Manifold Embedding)框架来解决这个问题。 Fig.1 如Fig.1所示，设$F = h(X) +F_0 = X^TW + \\mathbf{1}b^T + F_0$，作者通过使用回归残差来放宽约束。其中$F_0 \\in \\mathcal{R}^{m \\times c}$就是用来建模$F$与$h(X)$之间失配的回归残差。FME就是为了同时寻找最优的预测标签$F$，回归残差$F_0$，和线性回归函数$h(X)$。即： \\begin{aligned} (F^*,F^*_0,W^*,b^*) = \\arg \\underset{F,F_0,W,b}{\\min} &\\operatorname{Tr}(F-Y)^TU(F-Y) +\\operatorname{Tr}(F^TMF) \\\\ &+ \\mu(\\Vert W \\Vert^2 + \\gamma \\Vert F_0 \\Vert^2) \\end{aligned} \\tag{10}其中$M \\in \\mathcal{R}^{m\\times m}$为拉普拉斯矩阵，$U \\in \\mathcal{R}^{m \\times m}$为对角阵。前人也做过一些类似工作，不过都是聚焦于二分类任务。在这里，作者扩展到多类别的降维任务，且类别的独立性可以从提取的特征中捕捉到。 与LGC、GFHF和LapRLS/L类似，$(10)$式的前两项分别表示标签适配度和流形平滑度。考虑到不同样本（比如$j \\neq i$）的预测标签$F_i$和给定标签$Y_j$之间的近似是无意义的，因此设定$U$为前$n$个元素和后$m-n$个元素分别为$1$和$0$的对角阵。此外，为了保持流形结构（比如，$F$应该尽可能地在整个图中保持平滑），在半监督学习中$M$应该被设为图的拉普拉斯矩阵。用高斯核函数来计算$M = D - S$，其中$D_{ii} = \\sum_{j}S_{ij}$，若$x_i$为$x_j$的$k$近邻，$S_{ij} = exp(- \\Vert x_i - x_j \\Vert^2/t)$；否则$S_{ij} = 0$。 $(10)$式中的后两项用来控制投影矩阵$W$和回归残差$F_0$。相较于LapRLS/L，我们不强制$F$位于训练样本$X$所张成的空间中。因此，我们的框架更灵活，同时也能更好地处理驻留在非线性流形的样本。 用$F-X^TW -\\mathbf{1}b^T$替换$F_0$，那么得到： \\begin{aligned} (F^*,W^*,b^*) = \\arg \\underset{F,W,b}{\\min} &\\operatorname{Tr}(F-Y)^T U (F-Y) + \\operatorname{Tr}(F^TMF) \\\\ &+\\mu(\\vert W \\Vert^2 +\\gamma \\Vert X^TW + \\mathbf{1}b^T - F \\Vert^2) \\end{aligned} \\tag{11} 定理1设$U,M \\in \\mathcal{R}^{m\\times m}$，$F,Y\\in \\mathcal{R}^{m\\times c}$，$W\\in \\mathcal{R}^{f \\times c}$，$b \\in\\mathcal{R}^{c \\times 1}$。若$U$和$M$为半正定矩阵，且$\\mu ,\\gamma \\ge 0$，则 g(F,W,b) = \\operatorname{Tr}(F-Y)^TU(F-Y) + \\operatorname{Tr}(F^TMF) + \\mu(\\Vert W \\Vert^2 + \\gamma \\Vert X^TW + \\mathbf{1}b^T - F \\Vert^2)对于$F,W,b$为联合凸的。 证明： 在$g(F,W,b)$中，我们移除常数项$\\operatorname{Tr}(Y^TUY)$，那么可以写为： g(F,W,b) = Tr \\begin{bmatrix} F \\\\ W \\\\ b^T \\end{bmatrix}^T P \\begin{bmatrix} F \\\\ W \\\\ b^T \\end{bmatrix} - Tr \\begin{bmatrix} F \\\\ W \\\\ b^T \\end{bmatrix}^T \\begin{bmatrix} 2UY \\\\ 0 \\\\ 0 \\end{bmatrix}其中： P = \\begin{bmatrix} \\mu\\gamma I+M+U & -\\mu \\gamma X^T & -\\mu \\gamma \\mathbf{1} \\\\ - \\mu\\gamma X & \\mu I +\\mu \\gamma XX^T & \\mu \\gamma X\\mathbf{1} \\\\ - \\mu \\gamma \\mathbf{1}^T & \\mu \\gamma \\mathbf{1}^TX^T & \\mu \\gamma m \\end{bmatrix}因此我们要证$g(F,W,b)$对$F,W,b$为联合凸的，只需要证$P$为半正定矩阵即可。 对任意向量$z = [z_1^T,z_2^T,z_3^T]\\in \\mathcal{R}^{(m+f+1)\\times 1}$，其中$z_1 \\in \\mathcal{R}^{m \\times 1},z_2 \\in \\mathcal{R}^{f\\times 1},z_3$是个标量，则有： \\begin{aligned} z^{T} P z=& z_{1}^{T}(\\mu \\gamma I+M+U) z_{1}-2 \\mu \\gamma z_{1}^{T} X^{T} z_{2}-2 \\mu \\gamma z_{1}^{T} 1 z_{3} \\\\ &+z_{2}^{T}\\left(\\mu I+\\mu \\gamma X X^{T}\\right) z_{2}+2 \\mu \\gamma z_{2}^{T} X \\mathbf{1} z_{3}+\\mu \\gamma m z_{3}^{T} z_{3} \\\\ = &z_{1}^{T}(M+U) z_{1}+\\mu z_{2}^{T} z_{2}+\\mu \\gamma\\left(z_{1}^{T} z_{1}-2 z_{1}^{T} X^{T} z_{2}\\right.\\\\ &-2 z_{1}^{T} 1 z_{3}+z_{2}^{T} X X^{T} z_{2}+2 z_{2}^{T} X \\mathbf{1} z_{3}+m z_{3}^{T} z_{3} ) \\\\ =& z_{1}^{T}(M+U) z_{1}+\\mu z_{2}^{T} z_{2}+\\mu \\gamma\\left(z_{1}-X^{T} z_{2}-\\mathbf{1} z_{3}\\right)^{T} \\\\ & \\times\\left(z_{1}-X^{T} z_{2}-\\mathbf{1} z_{3}\\right)\\end{aligned}也就是若$U,M$为半正定矩阵，且$\\mu,\\gamma \\ge 0$，那么$\\forall z,z^TPz \\ge 0$。因此$P$为正定矩阵，也就是说$g(F,W,b)$是凸函数。定理1得证。 为了得到最优解，首先我们设$(11)$式对$b,W$的偏导为$0$。可得： \\begin{gathered} b = \\frac{1}{m}(F^T\\mathbf{1} - W^TX\\mathbf{1}) \\\\ W = \\gamma(\\gamma XH_cX^T + I)^{-1}XH_cF = AF \\end{gathered} \\tag{12}其中$A = \\gamma(\\gamma XH_cX^T + I)^{-1}XH_c,H_c = I - (1/m)\\mathbf{1}\\mathbf{1}^T$用来对数据进行中心化。利用$(12)$式重新表示$(11)$式中回归函数 $X^TW+\\mathbf{1}b^T$为： \\begin{aligned} X^{T} W+\\mathbf{1} b^{T} &=X^{T} A F+\\frac{1}{m} \\mathbf{1 1}^{T} F-\\frac{1}{m} \\mathbf{1 1}^{T} X^{T} A F \\\\ &=H_{c} X^{T} A F+\\frac{1}{m} \\mathbf{1 1}^{T} F=B F \\end{aligned} \\tag{13}其中$B=H_{c} X^{T} A+(1 / m) \\mathbf{1} \\mathbf{1}^{T}$。代入$(11)$是，得到： \\begin{aligned} F^{*}=& \\arg \\min _{F} \\operatorname{Tr}(F-Y)^{T} U(F-Y) \\\\ &+\\operatorname{Tr}\\left(F^{T} M F\\right)+\\mu\\left(\\operatorname{Tr}\\left(F^{T} A^{T} A F\\right)\\right.\\\\ &+\\gamma \\operatorname{Tr}(B F-F)^{T}(B F-F) ) \\end{aligned} \\tag{14}然后设$(14)$式对$F$偏导为$0$，可得： F=\\left(U+M+\\mu \\gamma(B-I)^{T}(B-I)+\\mu A^{T} A\\right)^{-1} U Y \\tag{15}根据$H_cH_c = H_c = H_c^T$和$\\gamma\\mu A^TXH_cX^TA+\\mu A^TA = \\gamma \\mu A^TXH_c = \\gamma\\mu H_cX^TA$，$(15)$式中的$\\mu \\gamma(B-I)^{T}(B-I)+ \\mu A^TA$可写为$\\mu \\gamma\\left(A^{T} X-I\\right) H_{c}\\left(X^{T} A-I\\right)+ \\mu A^TA$。因此可得： \\begin{aligned} \\mu \\gamma(B-I)^{T}(B-I)+\\mu A^{T} A=& \\mu \\gamma H_{c}-\\mu \\gamma^{2} H_{c} X^{T} \\\\ & \\times\\left(\\gamma X H_{c} X^{T}+I\\right)^{-1} X H_{c} \\end{aligned} \\tag{16}根据定义$X_c = XH_c$，我们可以计算预测标签$F$： F=\\left(U+M+\\mu \\gamma H_{c}-\\mu \\gamma^{2} N\\right)^{-1} U Y \\tag{17}其中$N=X_{c}^{T}\\left(\\gamma X_{c} X_{c}^{T}+I\\right)^{-1} X_{c}=X_{c}^{T} X_{c}\\left(\\gamma X_{c}^{T} X_{c}+I\\right)^{-1}$。 综上，首先利用$(17)$式得到最优解$F$，然后根据$(12)$式得到最优解$W,b$。 3.3 FME/U通过设定$(11)$式中的矩阵$U$为$0$可以简单得到FME的无监督版本： \\begin{aligned}\\left(F^{*}, W^{*}, b^{*}\\right)=& \\arg \\min_{F, W, b, F^{T} V F=I} \\operatorname{Tr}\\left(F^{T} M F\\right) \\\\ &+\\mu\\left(\\|W\\|^{2}+\\gamma\\left\\|X^{T} W+1 b^{T}-F\\right\\|^{2}\\right) \\end{aligned} \\tag{18}其中$V$被设为$H_c$，$I$为单位矩阵。 在无监督学习中，变量$F$可以看作低维表征的隐变量。我们限制$F$经过中心化操作后位于一个球面上，以避免$F=0$。同时$(18)$式是一个一般形式的等式，其可以通过使用不同的矩阵$M$和$V$来进行监督学习（TODO）。FME/U很自然地提供了一个对新数据映射方法，即$h(X) = X^TW+ \\mathbf{1}b^T$，且由于增加了一个残差惩罚项（$\\Vert h(X) - F \\Vert^2$），所以较于前人研究的“硬核”映射$F = X^TW$更显灵活性。 同样类似的优化步骤，首先设$(18)$式对于$W,b$的偏导为$0$，根据$(12)$式计算$W,b$，然后将$W,b$代入$(18)$式中，得到： \\begin{aligned} F^{*}= \\arg \\min _{F, F^{T} H_{c} F=I} &\\operatorname{Tr}\\left(F^{T} M F\\right)+\\mu\\left(\\operatorname{Tr}\\left(F^{T} A^{T} A F\\right)\\right.\\\\ &+\\gamma \\operatorname{Tr}(B F-F)^{T}(B F-F) ) \\end{aligned} \\tag{19}最后根据$(16)$式，可以将$(19)$式写为： \\begin{aligned} F^{*} &=\\arg \\min _{F, F^{T} H_{c} F=I} \\operatorname{Tr} F^{T}\\left(M+\\mu \\gamma H_{c}-\\mu \\gamma^{2} N\\right) F \\\\ &=\\arg \\min _{F, F^{T} H_{c} F=I} \\operatorname{Tr} F^{T}\\left(M-\\mu \\gamma^{2} N\\right) F \\end{aligned} \\tag{20}其中$N=X_{c}^{T}\\left(\\gamma X_{c} X_{c}^{T}+I\\right)^{-1} X_{c}=X_{c}^{T} X_{c}\\left(\\gamma X_{c}^{T} X_{c}+I\\right)^{-1}$。 综上，我们先通过$(20)$式得到最优解$F$，然后通过$(12)$式得到最优解$W,b$。 4 模型比较模型比较分三个部分： 比较FME与其他的半监督学习算法LGC,GFHF,LapRLS/L。 比较FME/U与Graph Embedding Framwork。 比较FME/U与Spectral Regression。 4.1 比较FME 示例1LGC与GFHF为FME的两种特殊情形。 证明： 若我们设$\\mu = 0$，那么FME的目标函数$(11)$式就退化成了$(3)$式，也就是LGC和GFHF的一般形式。示例1得证。 示例2LapRLS/L也是FME的一种特殊情形。 证明： 若我们设$(11)$式中$\\mu (\\lambda_A)/(\\lambda_I)$且$\\gamma \\rightarrow \\infty$，那么可得$F = X^TW+\\mathbf{1}b^T$，代入$(11)$式有： \\begin{aligned} g(W, b)=& \\operatorname{Tr}\\left(X^{T} W+\\mathbf{1} b^{T}\\right)^{T} M\\left(X^{T} W+1 b^{T}\\right) \\\\ &+\\mu\\|W\\|^{2}+\\operatorname{Tr}\\left(X^{T} W+1 b^{T}-Y\\right)^{T} \\\\ & U\\left(X^{T} W+1 b^{T}-Y\\right) \\end{aligned} \\tag{21}进一步设$M = L$且$U$为前$n$个元素和后$m-n$个元素分别为$(1)/(n\\lambda_I)$和$0$的对角阵。那么$g(W,b)$就等于$(4)$式中的$(1)/(\\lambda_I)g_M(W,b)$。示例2得证。 4.2 比较FME/U与GE此前有一篇很厉害的文章提出了广义的图嵌入框架整合了一大堆降维算法（如PCA, LDA, ISOMAP, LLE, LE）。文章把各个算法给定的统计和几何性质编码为图形关系，并且每个算法都可以被看作是直接图嵌入，线性图嵌入，或其他扩展。直接图嵌入的目标函数为： F^{*}=\\arg \\min _{F, F^{T} V F=I} \\operatorname{Tr}\\left(F^{T} M F\\right) \\tag{22}其中$V$为另一个图拉普拉斯矩阵（比如中心矩阵$H_c$），那么有$V\\mathbf{1} = \\mathbf{0},\\mathbf{1}^TV = \\mathbf{0}^T$。 然而直接的图嵌入计算得到的对于训练样本低维表征$F$并不能为新的数据点提供映射。那篇文章中也给出了解决方法，线性化，核化，向量化等。假设一个硬线性映射为$F = X^TW +\\mathbf{1}b^T$，那么目标函数在线性图嵌入中表示为： \\begin{aligned} W^{*}=& \\arg \\min_{W,\\left(X^{T} W+\\mathbf{1} b^{T}\\right)^{T} V\\left(X^{T} W+\\mathbf{1} b^{T}\\right)=I} \\\\ & \\operatorname{Tr}\\left(X^{T} W+\\mathbf{1} b^{T}\\right)^{T} M\\left(X^{T} W+\\mathbf{1} b^{T}\\right) \\\\ =&\\arg \\min_{W, W^{T} X V X^{T} W=I} \\operatorname{Tr}\\left(W^{T} X M X^{T} W\\right) \\end{aligned} \\tag{23} 示例3直接图嵌入和它的线性化是$FME/U$的一种特殊情形。 证明： 若我们设$\\mu = 0$，那么$FME/U$的目标函数退化为$(22)$式中的直接图嵌入。 当$(18)$式中$\\mu \\rightarrow 0,\\mu \\gamma \\rightarrow \\infty$时，有$F = X^TW +\\mathbf{1}b^T$。替换$(18)$式中的的$F$后$FME/U$的目标函数退化为$(23)$式中的线性图嵌入。示例3得证。 4.3 比较FME/U与SRSR的提出是为了解决投影矩阵$W$来映射新数据点的问题。分为两个步骤，首先$(18)$式已经得到最优解$F$，然后计算最优解$W,b$： \\left[W^{*}, b^{*}\\right]=\\arg \\min _{W, b}\\left\\|X^{T} W+1 b^{T}-F\\right\\|^{2}+\\lambda\\|W\\|^{2} \\tag{24} 示例4SR是$FME/U$的一种特殊情形。 证明： 当$\\mu \\rightarrow 0, \\gamma = 1/\\lambda$，那么$(18)$式会退化为$(22)$式，也就是说我们先解$F$。然后目标函数从$(18)$式转化到$(24)$式来解$W$，此时注意到SR目标函数为$W^{*}=\\left(X H_{c} X^{T}+\\lambda I\\right)^{-1} X H_{c} F$，且与FME/U中的一致。示例4得证。 4.4 汇总直接上图： Fig.2 5 思考 文章的创新点在于加入了一个回归残差作为惩罚项，并整合了多个降维算法。看这篇文章最大的收获，其一就是学会了一种利用残差来放宽约束的方法，可以避免过拟合非线性流形的训练数据。其二就是了解了各大知名的降维算法。论文有些地方的还没有读通透，需要再仔细研读一下。 对于迁移学习的研究的思考。 我们一般假设源域和源域数据采样一个高维流形之中，那么一般的做法大多是对源域和目标域数据进行本文中所描述的“硬核”线性投影（$W^TX+b$），得到一个利于最终任务的子空间（当然其中需要有各种图拉普拉斯约束，MMD约束等）。也就是说我们得到的子空间其实是一个线性的空间（见Fig.1），对于投影后不在这个空间的新数据而言，约束太过严格了。那么我们能否加一个残差项来补足呢？也就是说我们最后学的映射是$F =W^TX+b+ F_0$觉得这一点很值得一试。 对于深度学习的思考。 按照作者的理论，其实普通的全连接神经网络的每一层的输出相当于对输入做了一次投影$W^TX+b$，那么为了让输出有更好的表现力，利于最终任务，我们能否在每一层中加入一个残差项呢？这个残差项跟何凯明的Res-Net不一样。我们的目的是使得输出有更好非线性表现，也就是相当于一个激活函数的作用，重要的是这个激活函数不需人为指定。至于是否会出现过拟合，梯度爆炸和梯度消失，结合BN层会产生哪些“化学反应”等问题，还需仔细琢磨一下。这也不失为一种创新的方法。","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yvanzh.top/tags/Machine-Learning/"},{"name":"Manifold Learning","slug":"Manifold-Learning","permalink":"http://yvanzh.top/tags/Manifold-Learning/"},{"name":"Dimension Reduction","slug":"Dimension-Reduction","permalink":"http://yvanzh.top/tags/Dimension-Reduction/"}]},{"title":"Hexo 之 Indigo 主题博客置顶","slug":"Hexo-Top","date":"2019-04-08T06:11:11.000Z","updated":"2020-08-29T02:28:17.117Z","comments":true,"path":"2019/04/08/Hexo-Top/","link":"","permalink":"http://yvanzh.top/2019/04/08/Hexo-Top/","excerpt":"如何让文章C位出道~","text":"如何让文章C位出道~ 详情首先修改/node_modules/hexo-generator-index/lib/generator.js文件，添加置顶功能： &#39;use strict&#39;; var pagination = require(&#39;hexo-pagination&#39;); module.exports = function(locals){ var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) { if(a.top &amp;&amp; b.top) { // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 } else if(a.top &amp;&amp; !b.top) { // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; } else if(!a.top &amp;&amp; b.top) { return 1; } else return b.date - a.date; // 都没定义按照文章日期降序排 }); var paginationDir = config.pagination_dir || &#39;page&#39;; return pagination(&#39;&#39;, posts, { perPage: config.index_generator.per_page, layout: [&#39;index&#39;, &#39;archive&#39;], format: paginationDir + &#39;/%d/&#39;, data: { __index: true } }); }; 然后修改/themes/indigo/layout/_partial/post.ejs文件，给索引页面增加置顶样式： &lt;%- partial(&#39;post/toc&#39;, { post: post}) %&gt; &lt;article id=&quot;&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;&quot; class=&quot;post-article article-type-&lt;%= post.layout %&gt; fade&quot; itemprop=&quot;blogPost&quot;&gt; &lt;div class=&quot;post-card&quot;&gt; &lt;h1 class=&quot;post-card-title&quot;&gt;&lt;%- post.title %&gt;&lt;/h1&gt; &lt;div class=&quot;post-meta&quot;&gt; &lt;%- partial(&#39;post/date&#39;, {date_format: config.date_format}) %&gt; &lt;%- partial(&#39;post/category&#39;) %&gt; &lt;% if (post.top&gt;0) { %&gt; &lt;span&gt;&lt;icon class=&quot;icon icon-thumb-tack icon-pr&quot;&gt;&lt;/icon&gt;&lt;font color=&quot;ff4081&quot;&gt;置顶&lt;/font&gt;&lt;/span&gt; &lt;% } %&gt; &lt;%- partial(&#39;plugins/page-visit&#39;) %&gt; &lt;/div&gt; &lt;div class=&quot;post-content&quot; id=&quot;post-content&quot; itemprop=&quot;postContent&quot;&gt; &lt;%- post.content %&gt; &lt;/div&gt; &lt;%- partial(&#39;post/copyright&#39;) %&gt; &lt;%- partial(&#39;post/reward-btn&#39;) %&gt; &lt;div class=&quot;post-footer&quot;&gt; &lt;%- partial(&#39;post/tag&#39;) %&gt; &lt;%- partial(&#39;post/share-fab&#39;) %&gt; &lt;/div&gt; &lt;/div&gt; &lt;%- partial(&#39;post/nav&#39;) %&gt; &lt;%- partial(&#39;post/comment&#39;) %&gt; &lt;/article&gt; &lt;%- partial(&#39;post/reward&#39;) %&gt; 之后修改/themes/indigo/layout/_partial/index-item.ejs文件，给正文页面增加置顶样式： &lt;article id=&quot;&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;&quot; class=&quot;article-card article-type-&lt;%= post.layout %&gt;&quot; itemprop=&quot;blogPost&quot;&gt; &lt;div class=&quot;post-meta&quot;&gt; &lt;%- partial(&#39;post/date&#39;, {date_format: config.date_format}) %&gt; &lt;%- partial(&#39;post/category&#39;) %&gt; &lt;% if (post.top&gt;0) { %&gt; &lt;span&gt;&lt;icon class=&quot;icon icon-thumb-tack icon-pr&quot;&gt;&lt;/icon&gt;&lt;font color=&quot;ff4081&quot;&gt;置顶&lt;/font&gt;&lt;/span&gt; &lt;% } %&gt; &lt;/div&gt; &lt;%- partial(&#39;post/title&#39;, { hasLink: true }) %&gt; &lt;div class=&quot;post-content&quot; id=&quot;post-content&quot; itemprop=&quot;postContent&quot;&gt; &lt;% if(theme.excerpt_render) { %&gt; &lt;%- post.excerpt || post.content %&gt; &lt;% } else { %&gt; &lt;%- post.excerpt ? strip_html(post.excerpt) : truncate(strip_html(post.content), { length: theme.excerpt_length }) %&gt; &lt;% } %&gt; &lt;a href=&quot;&lt;%- url_for(post.path) %&gt;&quot; class=&quot;post-more waves-effect waves-button&quot;&gt; &lt;%= __(&#39;post.continue_reading&#39;) %&gt; &lt;/a&gt; &lt;/div&gt; &lt;% if(post.tags &amp;&amp; post.tags.length){ %&gt; &lt;div class=&quot;post-footer&quot;&gt; &lt;%- partial(&#39;post/tag&#39;) %&gt; &lt;/div&gt; &lt;% } %&gt; &lt;/article&gt; 最后大功告成，只需要在.md文件中加入top初始化参数，比如： title: Hexo置顶文章 date: 1111-11-11 11:11:11 tags: Hexo categories: Hexo top: 1 当top的值取0的时候，表示默认排序，即是按照时间顺序来排序。 当top的值取1到无穷大（最好取整数）的时候，值越高越靠前。 参考：解决Hexo-theme-indigo的置顶问题","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/tags/Hexo/"}]},{"title":"Clustering and Projected Clustering with Adaptive Neighbors","slug":"Paper-Notes-4","date":"2019-04-07T09:01:33.000Z","updated":"2020-08-29T02:28:17.120Z","comments":true,"path":"2019/04/07/Paper-Notes-4/","link":"","permalink":"http://yvanzh.top/2019/04/07/Paper-Notes-4/","excerpt":"","text":"1 简介文章来源：2014-KDD 文章主旨： 由于相似性度量和数据聚类分为两个步骤进行，可能会导致学习到的数据相似度不是最佳的。本文提出一个新框架让两者同时进行。 对拉普拉斯矩阵中的相似度矩阵进行秩约束，从而诱导聚类结构。 把模型扩展到投影聚类来处理高维数据。 吐槽与膜拜： 这篇文章的符号设定简直反人类，且还有几处小错误。但是瑕不掩瑜，论文字字珠玑，最牛批的是直接让$K$-means和谱聚类沦为了跟班小弟。当时就WC了，小脑袋瓜都怎么长的，咋这好使。传统的机器学习确实是宝刀未老，14年的文章现在来看也毫不过时（也可能是我涉猎太少）。反观现在的深度学习研究感觉处在一个病态阶段，大都是在用体力劳动的结果说话，反复修改参数，堆砌部件，结果好就是老大，丝毫不管为什么好，为什么不好，模型的可解释性如何。不得不感叹，我们离真正的人工智能时代之间确实还是道阻路且长，在让机器学会如何学习之前，只能姑且被称为人力驱动的劳动密集型AI。在此之前，传统的机器学习还是应该受到重视的，因为深度学习的诱导和和驱动的源泉还是这些传统机器学习知识。一通认知浅薄的大话，不知不觉就扯远了，总之本文绝对是值得反复读的经典没错！ 2 模型框架 更小的距离应该被分配更大的邻近概率： \\underset{\\forall i,s_i^T \\mathbf{1} = 1,0\\le s_i \\le 1}{\\mathrm{min}} \\sum_{i,j=1}^{n} (\\Vert x_i - x_j \\Vert_2^2 s_{ij} + \\gamma s_{ij}^2) \\tag{1} 其中第二项相当于正则化项，表示在不考虑数据的距离信息情况下，每个数据点$x_i$与其他点的邻近概率趋向于$\\frac{1}{n}$（算术平均数≤平方平均数），以此作为一个先验的邻近分配。 进一步，将$d_{ij}^x = \\Vert x_i - x_j \\Vert_2^2$代入，则$(1)$式可转化为： \\underset{\\forall i,s_i^T \\mathbf{1} = 1,0\\le s_i \\le 1}{\\mathrm{min}} \\Big\\Vert s_i + \\frac{1}{2\\gamma}d_i^x \\Big\\Vert_2^2 \\tag{2} 诱导连通分量为$c$个（即将数据聚为$c$簇）。 将$(1)$式中得到的$S\\in \\mathbb{R}^{n\\times n}$看作图节点的相似矩阵，每个节点$i$被赋值为$f_i \\in \\mathbb{R}^{1\\times c}$，有： \\sum^n_{i,j=1} \\Vert f_i - f_j \\Vert_2^2s_{ij} = 2Tr(F^TL_SF) \\tag{3} 若相似矩阵$S$为非负的，那么拉普拉斯矩阵满足一个重要性质： 定理1在拉普拉斯矩阵$L_S$中，特征根0的重数等于图的相似矩阵$S$的连通分量个数。 也就是说，我们可以通过约束$rank(L_S) = n-c$来得到很好的聚类结构。而不需要进行$K$-means或者其他离散化程序。因此，结合(1)式，得到： \\begin{gathered} J_{opt} = \\underset{S}{\\mathrm{min}} \\sum_{i,j =1}^n(\\Vert x_i - x_j\\Vert_2^2s_{ij} + \\gamma s_{ij}^2) \\\\ s.t. \\ \\ \\ \\ \\forall i,s_i^T\\mathbf{1}=1,0\\le s_i \\le 1,rank(L_S) = n - c \\end{gathered} \\tag{4} 对于$(4)$式的优化算法。 设$\\sigma_i(L_S)$是$L_S$最小的第$i$个特征值，且因为$L_S$为半正定的，$\\sigma_i(L_S)\\ge 0$。则可引入一个足够大的$\\lambda$，将$(4)$式转化为： \\begin{gathered} \\underset{S}{\\mathrm{min}} \\sum_{i,j =1}^n(\\Vert x_i - x_j\\Vert_2^2s_{ij} + \\gamma s_{ij}^2) + 2\\lambda \\sum \\sigma_i(L_S) \\\\ s.t. \\ \\ \\ \\ \\forall i,s_i^T\\mathbf{1}=1,0\\le s_i \\le 1 \\end{gathered} \\tag{5} 根据Ky Fan定理，我们可以得到： \\sum_{i=1}^c \\sigma_i(L_S) = \\underset{F \\in \\mathbb{R}^{n \\times c},F^T F = I} {\\min} Tr(F^T L_S F) \\tag{6} 将$(6)$式代入$(5)$式，有： \\begin{gathered} \\underset{S,F}{\\mathrm{min}} \\sum_{i,j =1}^n(\\Vert x_i - x_j\\Vert_2^2s_{ij} + \\gamma s_{ij}^2) + 2\\lambda Tr(F^TL_SF) \\\\ s.t. \\ \\ \\ \\ \\forall i,s_i^T\\mathbf{1}=1,0\\le s_i \\le 1 ,F\\in\\mathbb{R}^{n\\times c},F^TF =I \\end{gathered}\\tag{7} 这样一来，问题就简单了很多。我们可以先固定$S$，那么$(7)$式变成了： \\underset{F \\in \\mathbb{R}^{n \\times c},F^T F = I} {\\min} Tr(F^T L_S F) \\tag{8} 我们对$(8)$式求的$F$其实就是$L_S$最小的$c$个特征值所对应的特征向量。然后固定$F$，那么$(7)$式就变成了： \\begin{gathered} \\underset{S}{\\mathrm{min}} \\sum_{i,j =1}^n(\\Vert x_i - x_j\\Vert_2^2s_{ij} + \\gamma s_{ij}^2) + 2\\lambda Tr(F^TL_SF) \\\\ s.t. \\ \\ \\ \\ \\forall i,s_i^T\\mathbf{1}=1,0\\le s_i \\le 1 \\end{gathered} \\tag{9} 根据$(3)$式，有： \\begin{gathered} \\underset{S}{\\mathrm{min}} \\sum_{i,j =1}^n(\\Vert x_i - x_j\\Vert_2^2s_{ij} + \\gamma s_{ij}^2 + 2\\lambda \\Vert f_i - f_j\\Vert_2^2 s_{ij}) \\\\ s.t. \\ \\ \\ \\ \\forall i,s_i^T\\mathbf{1}=1,0\\le s_i \\le 1 \\end{gathered}\\tag{10} 注意$d_{ij}= d^x_{ij} + \\lambda d^f_{ij}$，其中$d_{ij}^x = \\Vert x_i - x_j \\Vert_2^2$，$d_{ij}^f = \\Vert f_i - f_j \\Vert_2^2$。根据$(2)$式，有： \\underset{\\forall i,s_i^T \\mathbf{1} = 1,0\\le s_i \\le 1}{\\mathrm{min}} \\Big\\Vert s_i + \\frac{1}{2\\gamma}d_i \\Big\\Vert_2^2 \\tag{11} 其中$s_i$和$d_i$分别为$S$和$d_{ij}$的第$i$列向量。 综上，交替迭代$(8)$和$(11)$式更新$F$和$S$即可。 3 联系$K$-means 引理1$HD^xH =-2HXX^TH$ 证明： 中心矩阵定义为： H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^T \\tag{12}由$d^x_{ij} =\\Vert x_i -x_j \\Vert_2^2 = x_i^Tx_i +x_j^Tx_j -2x_i^Tx_j$，可得： D^x = Diag(XX^T) \\mathbf{1} \\mathbf{1}^T + \\mathbf{1}\\mathbf{1}^TDiag(XX^T) - 2XX^T \\tag{13}同时注意$H\\mathbf{1} = \\mathbf{1}^TH = 0$，则对$(13)$式左乘右乘$H$，得证。 定理2当$\\gamma \\rightarrow \\infty$时，$(4)式$等价于$K$-means问题。 证明： 将$(4)$式写成矩阵形式： \\underset{S\\mathbf{1},S\\ge0,rank(L_S)=n-c}{\\mathrm{min}} Tr(S^TD^x) + \\gamma \\Vert S \\Vert^2_F \\tag{14}由于$rank(L_S) = n- c$，那么解$S$有确切的$c$个连通分量，也就是$S$经过适当变换之后可以写成块对角的形式，例如： \\left(\\begin{matrix} S_1 & 0 & 0\\\\ 0 & S_2 & 0\\\\ 0 & 0 & S_3 \\end{matrix}\\right)第$i$个连通分量$S_i \\in \\mathbb R^{n_i\\times n_i}$，其中$n_i$为该连通分量中的数据个数。那么$(14)$式可转化为对每个连通分量$i$： \\underset{S_i\\mathbf{1},S_i\\ge0}{\\mathrm{min}} Tr(S_i^TD_i^x) + \\gamma \\Vert S_i \\Vert^2_F \\tag{15}当$\\gamma \\rightarrow \\infty$时，$(15)$式就转化为： \\underset{S_i\\mathbf{1},S_i\\ge0}{\\mathrm{min}} \\gamma \\Vert S_i \\Vert^2_F \\tag{16}那么$(16)$的最优解就是$S_i$的所有元素都等于$\\frac{1}{n_i}$。 因此，当$\\gamma \\rightarrow \\infty$时，$(14)$式的最优解$S$应该是这样的： s_{ij} = \\begin{cases} \\frac{1}{n_k} \\ \\ \\ \\ x_i,x_j \\ \\text{are in the same component }k\\\\ \\ 0 \\ \\ \\ \\ \\ \\text{otherwise} \\end{cases} \\tag{17}也就是说对于每一个最优的划分$\\mathcal{V}$来说，总有$\\Vert S \\Vert_F^2 = c$。到了这里发现$(14)$式的第二项在后续的优化过程中是个常数，$(14)$式可写为： \\underset{S\\in \\mathcal{V}}{\\mathrm{min}} \\ Tr(S^TD^x) \\tag{18}其中$S$为对称阵，且$\\mathbf{1}^T S = \\mathbf{1}^T$，再结合$(12)$式。可知： Tr(HD^xHS) = Tr(D^xS)- \\frac{1}{n}\\mathbf{1}^TD^x\\mathbf{1} \\tag{19}那么根据$(19)$式，$(18)$式可写为： \\underset{S\\in \\mathcal{V}}{\\mathrm{min}} \\ Tr(HD^xHS) \\tag{20}定义一个标签矩阵$Y \\in \\mathbb{R}^{n\\times c}$，其中： y_{ij} = \\begin{cases} \\frac{1}{\\sqrt {n_k}} \\ \\ \\ \\ x_i \\ \\text{belongs to the }k\\text{-th component}\\\\ \\ \\ 0 \\ \\ \\ \\ \\ \\ \\text{otherwise} \\end{cases} \\tag{21}结合$(20)(21)$式和引理1，可得： \\begin{aligned} &\\underset{S\\in \\mathcal{V}}{\\mathrm{min}} \\ Tr(HD^xHS) \\\\ &\\Leftrightarrow \\underset{S\\in \\mathcal{V}}{\\mathrm{max}} \\ Tr(HXX^THS) \\\\ &\\Leftrightarrow \\underset{S\\in \\mathcal{V}}{\\mathrm{max}} \\ Tr(X^THSHX) \\\\ &\\Leftrightarrow \\underset{S\\in \\mathcal{V}}{\\mathrm{min}} \\ Tr(X^TH(I-S)HX) \\\\ &\\Leftrightarrow \\underset{Y}{\\mathrm{min}} \\ Tr(X^TH(I-YY^T)HX) \\\\ &\\Leftrightarrow \\underset{Y}{\\mathrm{min}} \\ Tr(S_w) \\end{aligned}\\tag{22}终于得证了当$\\gamma \\rightarrow \\infty$时，这就是一个实打实的$K$-means问题。当标签$Y$已知时，$S_w$其实就是线性判别分析(LDA)中的类内散度矩阵。而$K$-means就是要找到一个最优的$Y$来使得$S_w$的迹最小。 值得注意的是，尽管$\\gamma \\rightarrow\\infty$时，算法只能用来解决$K$-means问题（对数据进行球形分区），但是当$\\gamma$不是很大的时候，还是能解决任意形状的分区问题的。牛批！ 4 联系谱聚类当给定图的相似矩阵$S$时，谱聚类要解决的问题是： \\underset{F \\in \\mathbb{R}^{n \\times c},F^T F = I} {\\min} Tr(F^T L_S F) \\tag{23}与$(8)$式一样，最优解$F$就是由拉普拉斯矩阵$L_S$中最小的$c$个特征值所对应的特征向量组成的。 通常，在给定$S$后求得的$F$并不能直接用来聚类，因为$S$没有给出明确的连通分量个数$c$。也就是需要对$F$进行$K$-means或其他离散化过程才能得到最终的聚类结果。 但是在本文的模型中，$F$和$S$是交替优化的。当最终收敛时，$(23)$式求出来$F$，$S$同时也被求得。并且得益于$rank(L_S) = n -c$这一约束，学得的$S$有明确的连通分量个数$c$。所以$F$直接就是聚类的结果，不需要像传统的谱聚类一样再对$F$聚类。 因此，最优解$F$可以被写为： F =YQ \\tag{24}其中$Y\\in \\mathbb{R}^{n \\times c}$就是$(21)$式中定义的标签矩阵；$Q \\in \\mathbb{R}^{c \\times c}$是任意的正交矩阵。意思是$rank(F) = c$，也就是说本文模型得到的$F$直接就是聚类的结果。 可以看到，传统的谱聚类只能在给定相似矩阵$S$时来求$F$，并且还需要对$F$再聚类才能得到结果。而本文算法不仅不需要给定$S$，还能生成一个自适应的$S$。厉害！ 5 确定$\\gamma$的值众所周知，调好超参就等于成功了一半。在本文的模型中，$\\gamma$恐怕是最难缠的了，其跨度是$[0,\\infty)$，想想都头疼。于是作者做了下面的工作大大减少了死于调参的脑细胞。 对于$(7)$式中的$\\gamma$，可以将$(7)$式等价于$(2)$式。$(2)$式的拉格朗日函数为： \\mathcal{L}(s_i,\\eta,\\beta_i) = \\frac{1}{2}\\Big\\Vert s_i + \\frac{d_i^x}{2\\gamma_i} \\Big\\Vert_2^2- \\eta (s_i^T \\mathbf{1} - 1) - \\beta_i^Ts_i \\tag{25}其中$\\eta,\\beta_i \\ge 0$为拉格朗日乘子 。根据$\\mathrm{KKT}$条件，最优解$s_i$可被表示为： s_{ij}= \\frac{-d_{ij}^x}{2\\gamma_i}+\\eta+ \\beta_{ij} \\tag{26}通常关注数据的局部性可以得到更好的效果，最好就是学的一个稀疏的$s_i$，也就是只有$x_i$的$k$个最临近的点才有机会与$x_i$相连。稀疏的相似矩阵$S$另一个好处当然是可以缓解后续的计算压力。 不失一般性地假设$d_{i1}^x,d_{i2}^x,\\dots,d_{in}^x$从小到大排列。如果最优解$s_i$仅有$k$个非零元素，那么根据$(26)$式，我们知道$s_{ik}&gt; 0$且$s_{i,k+1} = 0$。因此，可得到： \\begin{cases} -\\frac{d_{ik}^x}{2\\gamma_i} + \\eta + \\beta_{ij}> 0\\\\ -\\frac{d_{i,k+1}^x}{2\\gamma_i} +\\eta + \\beta_{ij}\\le 0 \\end{cases} \\tag{27}再根据$(26)$式和$s_i^T\\mathbf{1} = 1$，我们有： \\begin{aligned} &\\sum_{j=1}^k (-\\frac{d_{ij}^x}{2\\gamma_i}+\\eta+\\beta_{ij}) = 1\\\\ &\\Leftrightarrow \\eta+\\beta_{ij} = \\frac{1}{k} +\\frac{1}{2k\\gamma_i}\\sum_{j=1}^k d_{ij}^x \\end{aligned}\\tag{28}因此对于$(2)$式，为了得到有$k$个非零元素的最优解$s_i$，根据$(27)(28)$式我们可以将$\\gamma_i$设为： \\begin{gathered} \\frac{k}{2} d_{i k}^{x}-\\frac{1}{2} \\sum_{j=1}^{k} d_{i j}^{x}","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"http://yvanzh.top/tags/Clustering/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://yvanzh.top/tags/Machine-Learning/"}]},{"title":"Hexo 之博客备份","slug":"Hexo-Backup","date":"2019-04-03T02:01:23.000Z","updated":"2020-08-29T02:28:17.109Z","comments":true,"path":"2019/04/03/Hexo-Backup/","link":"","permalink":"http://yvanzh.top/2019/04/03/Hexo-Backup/","excerpt":"再也不用担心博客迷路了！","text":"再也不用担心博客迷路了！ 1 机制与原理Hexo先在本地生成静态网页文件并暂存于.deploy_git文件夹中，然后部署时只把该文件夹push到远程仓库的master分支中。因此，我们可以在仓库中创建第二个分支来备份源文件。这样我们就能在利用Git的分支管理系统来进行多终端工作了。 2 备份步骤 2019年5月30日更新：由于GitHub的公开仓库才能免费使用静态网页，为避免源码泄露使用Coding仓库。 打开Coding中博客所在的仓库，新建一个分支，命名为hexo。 在仓库的settings的Branches选项中，把默认分支设为hexo。（因只需要对hexo分支进行手动操作，这样同步不需要指定分支，更加方便。） 在本地任意目录下，git bash here：git clone git@git.dev.tencent.com:YvanZh/YvanZh.coding.me.git 在克隆到本地的YvanZh.coding.me文件夹中，删除.git文件夹以外的所有文件。 将博客的源文件全部复制到YvanZh.coding.me文件夹中，除了.deploy_git。复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的git的文件。如果没有的话，新建一个，内容如下： .DS_Store Thumbs.db db.json *.log node_modules/ public/ .deploy*/ 如果之前克隆主题文件，需要删除theme文件夹中的.git文件夹。 最后git bash here： git add . git commit -m &quot;add branch&quot; git push 这样就将博客源文件上传到了hexo分支，完成了备份。PS：注意.git文件夹为隐藏文件夹，请务必开启隐藏文件可见。 3 更换终端后搭建环境的过程与之前无异： 安装Git，并绑定全局用户名和邮箱，配置SSH密钥。 安装Node.js。 安装Hexo： npm install hexo-cli -g 之后无需进行博客初始化，直接在任意文件夹下，gti bash here： git clone git@git.dev.tencent.com:YvanZh/YvanZh.coding.me.git 然后cd YvanZh.coding.me，进入克隆到本地的文件夹： npm install npm install hexo-deployer-git --save hexo g -d 终于可以开始写新博客了： hexo new newpage 最好每次写完博客之后更新一下hexo分支: git add . git commit -m &quot;xxx&quot; git push 咦~好像出现了一点不明真相的Bug（貌似是因为在第4步时没有按照SSH协议的方法来克隆仓库）导致git push需要输入密码，那么解决办法如下：git remote set-url origin git@git.dev.tencent.com:YvanZh/YvanZh.coding.me.git 4 结语颇费，美滋滋，滚去看论文了~","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/tags/Hexo/"}]},{"title":"2019华为软件精英挑战赛","slug":"HuaWei-CodeCraft2019","date":"2019-04-01T06:01:33.000Z","updated":"2020-08-29T02:28:17.117Z","comments":true,"path":"2019/04/01/HuaWei-CodeCraft2019/","link":"","permalink":"http://yvanzh.top/2019/04/01/HuaWei-CodeCraft2019/","excerpt":"别问，问就惜败。","text":"别问，问就惜败。 1 总结研一上学期就获悉了华为比赛的信息，笔者作为本科期间多次参加数学建模竞赛的老菜鸟，其实还是有点想法的，再加上实验室的小伙伴（C++同学）的“勾搭”，直接就上了“贼船”。 2019年3月10日至29日为练习阶段，30日9:00~17:00为初赛阶段。三只小菜鸡不停啄米，走了不少弯路，最后一周通宵了两次。最后由于时间紧张，棋差一招，很遗憾，很不甘，来年再战！ 接下来说说一些关键的时间点： 10日，JAVA同学和C++同学分别开始构建底层数据结构。笔者开始读任务书，关注论坛。 17日，笔者写完调度系统的伪代码。 26日，JAVA同学实现了判题器，同时C++同学陷入调Bug自闭状态。 30日，最重要的优化没来得及实现，败北。 下面是比赛的思路： 2 调度系统话不多说，直接上伪代码： for(T){ // 对车进行标记，终止状态为0，等待状态为1，可入库状态为2。ASC表示升序。 for(EachRoad){ 1StateCarQueue_EachRoad = queue[] 0StateCarQueue_EachRoad = queue[] // 是否把终止状态的车也顺便放在一个队列里面，这样可以省去调度时遍历的时间。 AheadCar_EachChannel(1:NumberOfChannel) = 0 //对每个Channel设一个变量 for((i = LenOfRoad):1:1){ for(EachChannel_ASC){ if(Road_Record(i) != 0){ CarID = GetCarID() AheadCar_EachChannel = CarID V1 = min(Car_Speed, Road_Speed) if(AheadCar_EachChannel == 0){ S1 = GetDistanceToAcrossRoad() V2 = min(Car_Speed, NextRoad_Speed) S2 = V2 - S1 if(S2 &lt;= 0){ S2 = 0 Car_State = 0 BehindCarState_EachChannel = 0 if(S1 &lt; V1 &amp;&amp; RoadID == EndRoad){ Car_State = 1 } } else{ Car_State = 1 BehindCarState_EachChannel = 1 } } else{ S1 = GetDistanceToAheadCar() GetCarChannel if(BehindCarState_EachChannel == 1 &amp;&amp; S1 &lt; V1){ Car_State = 1 } else{ Car_State = 0 BehindCarState_EachChannel = 0 /* 这里可能需要找出由于前面阻挡的车为终止状态，而导致无法过 路口的车。即 S2 &gt; 0 但是被前车所阻挡。后面这些车需要封锁这 条路，然后更新一下地图，再算其最短路径。 */ } } if(Car_State == 1){ 1StateCarQueue_EachRoad = ArcossCarQueue_EachRoad.append(CarID) } else{ 0StateCarQueue_EachRoad = ArcossCarQueue_EachRoad.append(CarID) } } } } } // 对道路内终止状态即Car_State == 0 的车进行调度。 for(EachRoad){ for(EachChannel){ for(LenOfRoad:1:1){ if(Car_State == 0){ CarRun } } } } // 对路口车辆调度。ASC表示升序。 TODO:需要加入一个到达终点车的判定。 for(EachCrossRoad_ASC){ CarCanAcross = 1 while(CarCanAcross){ for(EachRoad_ASC){ for(AcrossCarQueue_EachRoad){ if(AcrossCarQueue_EachRoad == [] or CarCanAcross_EachRoad == 0){ // CarCanAcross_EachRoad = 0 break } CarID = GetCarID() if(V1 - S1 &gt; 0 &amp;&amp; RoadID == EndRoad){ CarOffline } if(S2 &lt;= 0){ // CarRun // ChannelBlcok // AcrossCarQueue_EachRoad.pop() // 提取出被锁住的同channel的车，剩余队列继续进行路口调度 } Car_Direction = GetCarDirection() // 1, 2, 3 分别表示 D, L, R NextRoadID = GetNextRoadID() if(NextRoadHaveSpace == 0){ CarCanAcross_EachRoad = 0 BlockedCar_EachRoad = AcrossCarQueue_EachRoad break } if(Car_Direction == 1){ CarAcross // CarCanAcross需要从小号车道开始进入 AcrossCarQueue_EachRoad.pop() // 更新Both Roads } elseif(Car_Direction == 2){ ConflictRoad = GetTurnLeftConflictRoad() if(AcrossCarQueue_ConflictRoad_FirstCar_Dierection == 1){ break } CarAcross AcrossCarQueue_EachRoad.pop() /* (100,21,30,9,55)表示的是第100号路口，按顺时针方向道路为21 , 30, 9, 55。此时本道路车辆左转前需要检查本道路的逆时针方向第 一条道路有无直行车辆。有直行车则冲突，暂时结束对本道路的调度， 转而调度下一道路。 */ } else{ ConflictRoad1 = GetTurnRightConflictRoad1() ConflictRoad2 = GetTurnRightConflictRoad2() if(ArocssCarQueue_ConflictRoad1_FirstCar_Direction != 1 &amp;&amp; AcrossCarQueue_ConflictRoad2_FirstCar_Direction !=2){ CarAcross AcrossCarQueue_EachRoad.pop() } /* 本道路车右转前需要先检查本道路的顺时针方向第一条道路有无直行 车辆，然后检查本道路顺时针方向第二条道路有无左转车辆。 */ } } if(sum(CarCanAcross_EachRoad) == 0){ CarCanAcross = 0 break } } } } // 对出车库的车进行调度，如果没有空位，则延迟一个单位时间出发。 for(EachCrossRoad){ if(GarageHaveCar){ GetCarID() GetStarTime() if(StartTime &lt;= T &amp;&amp; MaxCar &lt; N &amp;&amp; RoadLoad &lt; n% ){ //若车辆达到计划出发时间，且所有道路总车辆小于N，且该道路负载程度小于n%。 //道路负载程度 = 道路车辆数/(道路长度*道路车道数) if(RoadHaveSpace){ CarOut //检索出第一条道路的起始列有无空当。有空当的话优先进入小号Channel。 } } } } } 3 优化算法笔者之前犯了一个严重的错误——没有分析地图数据，而这直接导致了优化算法的思路出现了问题。我们一直纠结于如何排除死锁、优化路径组合和道路负载均衡。然而真正重要的一点，也是能在简单分析地图数据后就知道的——车辆的速度问题。 就地图数据来看，车辆的计划出发时间都在100个时刻之内，而实际的出发时间需要人为设定，这正是判题器的作用之一（输出车辆的实际出发时间）。我们再来看看系统调度总时间的量级，其实是在千位的。换句话说，即使考虑最极端的情况，我们让所有车辆都在第100个时刻之后出发，也影响不了太多总时间。再看车辆的速度其实只有2，4，6，8，10，12，15，16这七种，那么我们为何不分批次把所有同速度的车来进行调度呢？这样无疑去除了慢车对快车速度限制，同时间接地减少了死锁和道路负载过大的问题。这是我们30日上午幡然醒悟，然而时间有限，确实来不及实现了。 那么解题流程应该是这样： 利用Floyd算法计算出每个路口节点的最短路径，并以此初始化所有车辆路径。 按照速度对所有车辆分批$P_1,\\dots,P_7$。 对$P_1$车辆利用遗传算法优化调度系统的参数$MaxCar_1$和$RoadLoad_2$，并得到一个最优调度时间$T_1$。以此时间作为$P_2$车辆的计划出发时间。 依次对后续每批车辆进行第3步操作。 最终可以得到最优的系统总调度时间为$T = T_1 +\\dots +T_7$，并输出每辆车的实际出发时间和最短路径。","categories":[{"name":"Competition","slug":"Competition","permalink":"http://yvanzh.top/categories/Competition/"}],"tags":[{"name":"Competition","slug":"Competition","permalink":"http://yvanzh.top/tags/Competition/"}]},{"title":"Generalized Zero-Shot Learning via Synthesized Examples","slug":"Paper-Notes-3","date":"2019-03-29T09:01:33.000Z","updated":"2020-08-29T02:28:17.119Z","comments":true,"path":"2019/03/29/Paper-Notes-3/","link":"","permalink":"http://yvanzh.top/2019/03/29/Paper-Notes-3/","excerpt":"","text":"1 简介文章来源：2018-CVPR Tips： Normal Zero-Shot：测试集只有目标域数据（训练集与测试集不相交）。 Generalize Zero-Shot：测试集既有源域数据又有目标域数据（训练集与测试集相交）。 文章主旨： 在CVAE模型基础上加入一个针对生成样本的回归器，其将生成的样本映射到其对应的类属性上去。当模型优化完毕后，把不可见类的样本并纳入最终分类器的训练中，以平衡在GZSL中对可见类和不可见类的预测偏差。 2 详情 模型分析： 假设隐变量与类属性是分离的(隐变量$z_n$表示$x_n$的非结构化部分，类属性向量$a_{y_n}$表示类特定的判别信息)，也就是说$x_n$受到这两者的共同影响；并且在类属性向量的指导下，有助于生成本质上具有更好区分度的样本。 包含一个多元回归器，其利用解码器的输出$\\hat{x}_n$作为输入，映射到类属性向量$a_{y_n}$。它有两点重要作用： 可以对生成器产生反馈，使其生成更好可区分性的样本。 可以通过计算类属性向量上的概率分布$p(a | x)$来使用无标签的样本进行训练(即进行半监督学习)。 确保生成器的输出$\\hat{x}$与实际输入$x$相似，比如可以用$p(z | \\hat{x})$来近似$p(z | x)$。 最终分类器： 当生成模型训练好之后，就可以生成带标签的各类样本。首先，需要按照先验分布$p(z)$(一般对于二值型数据采用伯努利分布，对于连续型数据采用高斯分布)来随机生成非结构化部分即隐变量$z$，并指定需生成的类别$c$(通过类属性向量$a_c$)。然后就可以通过生成器来生成样本$x$。 当每个类别都生成了固定数量的样本之后，就可以利用它们来训练分类器(比如，SVM或softmax分类器)。由于这个阶段使用了来自可见类和不可见类的带标签样本，所以结果具有鲁棒性，可以减少对可见类的偏见。需要注意的是：训练分类器时，可以只使用来自可见类的原始标签样本，也可以用模型生成的可见类的附加样本进行扩展。 多元回归器： 对于可见类的样本$\\{x_n,a_{y_n}\\}_{n=1}^{N_s}$，定义监督损失：\\mathcal{L}_{Sup}(\\theta_R) = -\\mathbb{E}_{ \\{x_n,{a_{y_n}}\\} } [p_R(a_{y_n} | x_n)] \\tag{1} 对于由生成器生成的样本$\\hat{x}$，定义非监督损失：\\mathcal{L}_{Unsup}(\\theta_R) = -\\mathbb{E}_{p_{ {\\theta}_G} (\\hat{\\mathbf{x}}| \\mathbf{z,a})p(\\mathbf{z})p(\\mathbf{a})}[p_R(\\mathbf{a} | \\mathbf{\\hat{x}})]\\tag{2} 由(1)(2)式，得到损失函数：\\underset{\\theta_R} \\ \\mathcal{L}_R = \\mathcal{L}_{Sup} + \\lambda_R \\centerdot \\mathcal{L}_{Unsup}\\tag{3} Tips：使用回归器通过反向传播来改进生成器。优化$\\theta_R$时需固定生成器的分布（即固定$\\theta_G$）。 编码器和条件生成器： 对于编码器$p_E(z|x)$和条件生成器$p_G(x|z,a)$，VAE损失函数为：\\mathcal{L}_{VAE}(\\theta_E,\\theta_G) = -\\mathbb{E}_{p_E(\\mathbf{z}|\\mathbf{x}),p(\\mathbf{a}|\\mathbf{x}))}[\\log p_G(\\mathbf{x}|\\mathbf{z,a})] + \\mathrm{KL}(p_E(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z}))\\tag{4} 其中第一项表示在希尔伯特再生空间中生成器的重构误差；第二项表示后验分布(编码器)需要与先验分布一致。 设编码器$p_E(z|x)$、生成器$p_G(x|z,a)$和回归器$p_R(a|x)$都服从高斯分布。 回归器驱动的学习： 确保$\\mathbf{\\hat{x}}$映射到$\\mathbf{a}$，即保证回归的正确性（生成器要报警了！回归结果不好，回归器不背锅居然怪我？其实这是个$\\theta_R$和$\\theta_G$交替优化的过程，你一拳我一拳让模型变得更加漂亮~）：\\mathcal{L}_c(\\theta_G) = -\\mathbb{E}_{p_G(\\mathbf{\\hat{x}}|\\mathbf{z,a})p(\\mathbf{z})p(\\mathbf{a})}[\\log p_R(\\mathbf{a}|\\mathbf{\\hat{x}})] \\tag{5} 确保$z$和$a$结合可以通过生成器得到对应样本（作为正则化项）：\\mathcal{L}_{Reg}(\\theta_G) = -\\mathbb{E}_{p(\\mathbf{z})p(\\mathbf{a})}[\\log p_G(\\mathbf{\\hat{x}}|\\mathbf{z,a})] \\tag{6} $(5)(6)$式共同作用让$\\hat{x}\\backsim p_G(\\mathbf{\\hat{x}}|\\mathbf{z,a})$。但还有一个最重要的点是确保$z$和$a$的独立性。为此，使用编码器来确保样本抽样分布和从生成的样本中得到的抽样分布遵循相同的分布：\\mathcal{L}_E(\\theta_G) = -\\mathbb{E}_{\\hat{x}\\backsim p_G(\\mathbf{\\hat{x}}|\\mathbf{z,a})}\\mathrm{KL}[(p_E(\\mathbf{z}|\\mathbf{\\hat{x}})||q\\mathbf{(z)})] \\tag{7} 由$(4)(5)(6)(7)$式，得到损失函数：\\underset{\\theta_G,\\theta_E}{\\mathrm{min}} \\ \\mathcal{L}_{VAE} + \\lambda_c \\centerdot \\mathcal{L}_c + \\lambda_{reg} \\centerdot \\mathcal{L}_{Reg} + \\lambda_E \\centerdot \\mathcal{L}_E \\tag{8} 综上，通过$(3)(8)$式交替优化$\\theta_R$和$\\theta_G$，得到最终的生成模型后，利用其生成的样本来训练最终的分类器。 3 思考 笔者之前就认为VAE框架本身的局限之处在于隐变量的先验分布，为了简单方便计算一般采用伯努利分布或标准正太分布。然而这无疑限制了模型的表达能力。在本文章中通过分离隐变量$z$和类属性向量$a$，并引入一个将生成的样本$\\hat{x}$映射到类属性向量$a$的多元回归器。既利用了VAE的生成能力，又增加了一个学习诱导，这样不失为一种好办法。 那么我们能不能从本质上改进VAE呢？VAE框架的根源其实说到底就是KL散度。KL散度可以度量两个概率分布的差异，但是我们甚至不能称其为距离，因为它不满足对称性。然而它的优势在于可以写成期望的形式，也就是可直接进行采样计算。$\\mathrm{KL}(p(x)||q(x))$有一个比较明显的问题，就是当$q(x)$在某个区域等于0，而$p(x)$在该区域不等于0，那么KL散度就出现无穷大。这是KL散度的固有问题，我们只能想办法规避它，比如隐变量的先验分布用高斯分布而不是均匀分布。Idea is cheap，留待实践。 VAE理论框架： \\begin{array}{c|c} \\hline x_k, z_k & \\text{表示随机变量}x,z\\text{的第}k\\text{个样本}\\\\ \\hline x_{(k)}, z_{(k)} & \\text{表示多元变量}x,z\\text{的第}k\\text{个分量}\\\\ \\hline \\mathbb{E}_{x\\sim p(x)}[f(x)] & \\text{表示对}f(x)\\text{算期望，其中}x\\text{的分布为}p(x)\\\\ \\hline \\mathrm{KL}(p(x)\\Vert q(x))& \\text{两个分布的}KL\\text{散度}\\\\ \\hline \\Vert x\\Vert^2& \\text{向量}x\\text{的}l^2\\text{范数}\\\\ \\hline \\mathcal{L}& \\text{本文的损失函数的符号}\\\\ \\hline D,d & D\\text{是输入}x\\text{的维度，}d\\text{是隐变量}z\\text{的维度}\\\\ \\hline \\end{array} 数据样本$\\{x_1,\\dots,x_n\\}$，其整体用$x$来描述，我们希望借助隐变量$z$描述$x$的分布$\\tilde{p}(x)$： q(x)=\\int q(x|z)q(z)dz,\\quad q(x,z) = q(x|z)q(z)\\tag{9} 其中$q(z)$是先验分布（标准正态分布），目的是希望$q(x)$能逼近$\\tilde{p}(x)$。这样（理论上）我们既描述了$p~(x)$，又得到了生成模型$q(x|z)$，一举两得。 接下来就是利用KL散度进行近似。很多教程推导都是聚焦于后验分布$p(z|x)$。但事实上，直接来对$p(x,z)$进行近似是最为干脆的。具体来说，定义$p(x,z) = \\tilde{p}(x)p(z|x)$，设想用一个联合概率分布$q(x,z)$来逼近$p(x,z)$，那么用KL散度来看它们的距离： KL\\Big(p(x,z)\\Big\\Vert q(x,z)\\Big) = \\iint p(x,z)\\ln \\frac{p(x,z)}{q(x,z)} dzdx \\tag{10} KL散度是我们的终极目标，因为我们希望两个分布越接近越好，所以KL散度越小越好。于是有： \\begin{aligned}KL\\Big(p(x,z)\\Big\\Vert q(x,z)\\Big) =& \\int \\tilde{p}(x) \\left[\\int p(z|x)\\ln \\frac{\\tilde{p}(x)p(z|x)}{q(x,z)} dz\\right]dx\\\\ =& \\mathbb{E}_{x\\sim \\tilde{p}(x)} \\left[\\int p(z|x)\\ln \\frac{\\tilde{p}(x)p(z|x)}{q(x,z)} dz\\right] \\end{aligned} \\tag{11} 由$\\ln \\frac{\\tilde{p}(x)p(z|x)}{q(x,z)}=\\ln \\tilde{p}(x) + \\ln \\frac{p(z|x)}{q(x,z)}$，我们进一步对$(11)$进行简化： \\begin{aligned}\\mathbb{E}_{x\\sim \\tilde{p}(x)} \\left[\\int p(z|x)\\ln \\tilde{p}(x)dz\\right] =& \\mathbb{E}_{x\\sim \\tilde{p}(x)} \\left[\\ln \\tilde{p}(x)\\int p(z|x)dz\\right]\\\\ =&\\mathbb{E}_{x\\sim \\tilde{p}(x)} \\big[\\ln \\tilde{p}(x)\\big] \\end{aligned}\\tag{12} 其中的$\\tilde{p}(x)$是根据样本$x_1,x_2,\\dots,x_n$确定的关于x的先验分布，尽管我们不一定能准确写出它的形式，但它是确定的、存在的，因此这一项只是一个常数，所以可以写出： \\mathcal{L}=KL\\Big(p(x,z)\\Big\\Vert q(x,z)\\Big) - \\text{常数}= \\mathbb{E}_{x\\sim \\tilde{p}(x)} \\left[\\int p(z|x)\\ln \\frac{p(z|x)}{q(x,z)} dz\\right]\\tag{13} 最后为了得到生成模型，所以我们把$q(x,z)$写成$q(x|z)q(z)$，于是就有: \\begin{aligned}\\mathcal{L} =& \\mathbb{E}_{x\\sim \\tilde{p}(x)} \\left[\\int p(z|x)\\ln \\frac{p(z|x)}{q(x|z)q(z)} dz\\right]\\\\ =&\\mathbb{E}_{x\\sim \\tilde{p}(x)} \\left[-\\int p(z|x)\\ln q(x|z)dz+\\int p(z|x)\\ln \\frac{p(z|x)}{q(z)}dz\\right]\\end{aligned}\\tag{14} 简化后就是： \\begin{aligned}\\mathcal{L} = &\\mathbb{E}_{x\\sim \\tilde{p}(x)} \\left[\\mathbb{E}_{z\\sim p(z|x)}\\big[-\\ln q(x|z)\\big]+\\mathbb{E}_{z\\sim p(z|x)}\\Big[\\ln \\frac{p(z|x)}{q(z)}\\Big]\\right]\\\\ = &\\mathbb{E}_{x\\sim \\tilde{p}(x)} \\Bigg[\\mathbb{E}_{z\\sim p(z|x)}\\big[-\\ln q(x|z)\\big]+KL\\Big(p(z|x)\\Big\\Vert q(z)\\Big)\\Bigg] \\end{aligned}\\tag{15} 中括号内的即为VAE的损失函数。 参考：变分自编码器（二）：从贝叶斯观点出发","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"http://yvanzh.top/tags/Transfer-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yvanzh.top/tags/Deep-Learning/"},{"name":"Zero-Shot","slug":"Zero-Shot","permalink":"http://yvanzh.top/tags/Zero-Shot/"}]},{"title":"高级计算机体系结构","slug":"Advanced-Computer-Architecture","date":"2018-12-03T13:11:11.000Z","updated":"2020-08-29T02:28:17.107Z","comments":true,"path":"2018/12/03/Advanced-Computer-Architecture/","link":"","permalink":"http://yvanzh.top/2018/12/03/Advanced-Computer-Architecture/","excerpt":"计算机的设计思想、设计原理、设计技术。","text":"计算机的设计思想、设计原理、设计技术。 1 知识要点1.1 冯式结构的五个组成部分运算器、控制器、存储器、输入设备、输出设备。 1.2 CPU 发展趋势性能计算： $CPUTime = \\frac{Seconds}{Program} = \\frac{Instruction}{Program} \\cdot \\frac{Cycles}{Instruction} \\cdot \\frac{Seconds}{Cycles}$ 性能提升： 半导体技术： 特征尺寸。 时钟频率。 计算机体系结构： 高级语言编译器、标准化的操作系统。 指令更为简单的RISC(精简指令集)结构。 多核技术出现的原因 通过堆积晶体管密度提高性能遇到瓶颈 硬件多线程的并发已经存在一段时间了。虽然可以减少内存延迟，但是只有30%的提升 如今将多个处理内核封装在一个芯片中来提升性能 两个1G多核处理器 VS 一个2G单核处理器 前者在同时处理两个小于1G的任务时比后者要快 后者在处理一个大于1G小于2G的任务时比前者要快 后者成本要低 编程模型：SIMD VS MIMD。 SIMD发掘数据级并行 矩阵运算 图像和声音处理 SIMD比MIMD能耗效率高 SIMD数据操作只需要取一条指令 SIMD对个人移动设备(PMD)具有吸引力；MIMD一般用于商用计算机和大型服务器。 SIMD编程者对并行思维要求较低；MIMD反之 虚拟化技术的意义 CPU的虚拟化技术可以单CPU模拟多CPU并行，允许一个平台同时运行多个操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率 1.3 Flynn’s 分类 单指令流、单数据流(SISD)—{指令级并行(ILP)} 解释：传统的顺序执行的单处理器计算机，其指令部件每次只对一条指令进行译码，并且只对一个操作部分分配数据。 例子：Intel Pentium4 单指令流、多数据流(SIMD)—{数据级并行(DLP)} 解释：以同步方式，在同一时间执行同一条指令。 例子：向量体系结构、多媒体指令扩展、GPU 多指令流、多数据流(MIMD) 解释：使用多个控制器来异步地控制多个处理器，从而实现空间上的并行性。 例子：Intel Xeon e5345(Clovertown) 紧耦合：MIMD；松耦合：MIPD 多指令流、单数据流(MISD) 没有实现商业化(why?)：显然多条指令处理同一数据可能频繁出现冲突，处理效率非常低。 1.4 网络和存储技术 网络技术 速度 存储接口 速度 WAN T1 56K/s SCSI 5MB/s 以太网 10Mb/s Fast SCSI 10MB/s 快速以太网 100Mb/s WideSCSI 20MB/s ATM/OC-3 155Mb/s Ultra SCSI 40MB/s 千兆以太网 1Gb/s 光纤路径 2Gb/s Infiniband 40Gb/s 光纤路径 4 or 8Gb/s 网络比存储发展快！ 1.5 内存系统存储技术的目标：最大，最快，最便宜。 技术趋势： 集成电路 晶体管密度：25% year 晶片尺寸：10-20% year 晶体管数目：40-55% year DRAM容量：单个25-40% year (slowing) 闪存容量：50-60% year 15-20X cheaper/bit than DRAM 磁盘技术：40% year 15-25X cheaper/bit than Flash 300-500X cheaper/bit than DRAM 局部性原理： 时间局部性：程序结构中的循环 空间局部性：数组或记录 层次：SRAM，DRAM，DISK，FLASH MEMORY 顶层{CPU内}：寄存器、Cache 二层{主板内}：主存、Cache 三层{主板外}：磁盘、光盘 底层{离线}：磁带 1.6 Cache的使用问题Cache： 高速缓存，也指基于局部性原理来管理的存储器。 使用Cache的问题： 如果要访问的数据不在Cache中怎么办？ 要访问的数据不在Cache中会导致缺失，缺失则将需要的数据装入Cache中。 从主存中装入数据时装到Cache中的什么位置？ 直接映射 每个主存地址对应到Cache的确定的位置 全相联映射 一个块可以放在Cache中的任何位置 需要检索Cache中的所有项：并行比较器 组相联映射 每个块有n个位置可放的Cache称为n路组相联Cache; 存储器中的一个块对应到Cache中唯一的组，但是可以放在组内的任意位置上 从主存中装入数据时一次装入多少数据？ 从cache到处理器：以字为单位； 从主存到cache：以块为单位；块设为多大？ 4K &amp; 64块；16K &amp; 64块；64K &amp; 256块；256K &amp; 256块。 如何判断Cache中对应的位置是否为有效的数据？ 有效位，有效位设置时表示一个块是有效的 如果Cache装满了怎么办？ 先入先出法(FIFO) 随机替换法(RAND) 最近最少使用法(LRU) LRU算法实现：使用链表和hashmap。当需要插入新的数据项的时候，如果新数据项在链表中存在（一般称为命中），则把该节点移到链表头部，如果不存在，则新建一个节点，放到链表头部，若缓存满了，则把链表最后一个节点删除即可。在访问数据的时候，如果数据项在链表中存在，则把该节点移到链表头部，否则返回-1。这样一来在链表尾部的节点就是最近最久未访问的数据项。 写Cache时会有什么问题？ 写命中和写不命中 写回法 写命中：只修改Cache内容，不立即写入主存，当被替换时才写入主存。 写不命中：将内存中包含欲写块的内容装入Cache中并修改。 写直达法 写命中：同时修改Cache和主存。 写不命中：直接写入主存。 写一次法 与写回法基本相同，只是第一次写命中时同时修改Cache和主存。 1.7 Cache的优化方法访问缺失： 强制缺失(第一次访问) 容量缺失(因容量不够，块被移除，又被访问) 冲突失效(重复访问的多个地址映射在Cache中的同一位置) $AMAT = \\text{命中时间} + \\text{缺失率} * \\text{缺失代价}$ 更大的块 强制缺失减少 容量和冲突缺失增加，缺失代价增加 更大的Cache容量 缺失率降低 命中时间，功耗增加 更高的相关度 冲突缺失减少 命中时间增加，功耗增加 更多级Cache 内存访问时间减少 读缺失优先级更高 缺失代价降低 缓存索引避免地址转移 减少命中时间 优化度量：命中时间，缺失率，缺失代价，缓存带宽和功耗。 1.8 存储访问模型 UMA(uniform memory access)模型 物理存储器被所有节点共享 所有节点访问任意存储单元的访问时间相同 发生访存竞争时，仲裁策略平等对待每个节点 每个节点的CPU可带有局部私有的高速缓存 外围I/O设备也可以共享，且每个节点有平等的访问权力 NUMA(Non-Uniform Memory Access)模型 物理存储器被所有节点共享，任意节点可以直接访问任意内存模块 节点访问内存模块的速度不同，访问本地存储模块的速度一般是访问其他节点内存模块的3倍以上 发生访存竞争时，仲裁策略对节点可能是不平等的 各节点的CPU可带有局部私有高速缓存cache 外围I/O设备也可以共享，但对各节点是不平等的 1.9 单CPU上常见的提升性能的方法和并行计算提升性能方法： 提高单个处理器的工作频率 Locality：L1/L2/L3 Cache 多级流水线(提高CPU频率的利器) 超标量执行(多条流水线并同时发送多条指令) 乱序执行(指令重排) 单指令流多数据SIMD 超长指令字处理器(依赖于编译器分析) 单CPU主要的并行：指令级并行(ILP) 1.10 多线程的好处 创建一个线程比创建一个进程的代价小 线程的切换比进程间切换的代价小 充分利用多处理器 数据共享 快速响应特性 1.11 Tomasulo’s 算法三步骤: 发射 从指令队列中获取指令，如果RS(Registers)可用，则发射指令到RS； 如果操作数可用，则发送数据至RS； 发射级完成了重命名。 执行 若有一个或几个操作数未就绪，等待该操作数，并同时监控CDB(CommonDataBus)； 当操作数可用，则存储至保留站；当所有操作数可用，则执行命令； 执行级检查了是否存在RAM竞争。 写结果 将结果写入CDB，并从CDB写入目的寄存器及等待此结果的保留站； 连续写入同一寄存器时，只有最后一次才能写入； 消除了WAW(Write After Write)竞争。 1.12 仓库级计算机(WSC) 提供互联网服务 搜索、社交网络、在线地图、视频分享、在线购物、电子邮件、云计算、etc. 不同于HPC“集群” HPC集群有更高性能的处理器和互联网络 HPC集群强调线程级并行，WSC强调请求级并行 不同于数据中心 数据中心集中运行不同的机器和软件 数据中心强调虚拟机，具有硬件异构型，为不同客户提供服务 1.13 批处理框架：MapReduce Map： 将程序员提供的函数应用于每条记录 在数千台计算机上运行 生成由键值对组成的中间结果 Reduce： 收集上述分布式任务的输出，使用另一个函数应用于中间结果 1.14 MESI协议图 1.15 基准程序的类别基准程序(Benchmarks)： 衡量一个系统，可通过运行一个或一组真实应用 应用程序要有代表性，覆盖现实世界中常见的情况 应用程序负载(workload)也要有代表性，与实际情况比较吻合 好的基准程序可以加速计算机的发展 改进基准程序的性能应该对大多数程序有益 好基准程序可以加速计算机的发展进程 是有益于运行真实程序, 还是销售机器/发表论文? 创造真正有益于真实程序的基准程序，而不是有益于基准程序的基准程序 类别： 玩具(Toy)基准程序 10-100行 例如：sieve，puzzle，quicksort 合成(Synthetic)基准程序 试图匹配真正工作负载的平均频度 例如：Whetstone，Dhrystone 内核(Kernels)程序 时间密集(Time critical excerpts) 例如：Livemore loops，FFT，tree search 真实程序(Actual workloads) 例如：gcc，spice 1.16 性能评测方法 经验：使用系统的现有实例执行性能测试。 前提条件： 真实系统的存在 不同程度的可测性设计 针对对象： 系统用户(应用需求) 系统设计者(设计验证) 分析：对系统进行数学抽象，推导出描述系统性能的公式。 仿真：开发一个实现系统模型的计算机程序。通过运行计算机程序进行性能测试。 统计：开发系统的统计抽象，并通过分析或模拟得出统计性能。 1.17 三种流水线冲突/冒险(Hazard)Hazard:指流水线遇到无法正确执行后续指令或执行了不该执行的指令 Structural Hazards(hardware resource conflicts) 现象：同一部件同时被不同指令所使用 解决方案： 一个部件每条指令只能使用一次，且只能在特定周期使用 设置多个部件，以避免冲突，如指令寄存器IM和数据寄存器DM分开，或寄存器读口和寄存器写口分开。 Data Hazards(data dependencies) 现象：后面指令用到前面指令结果时，前面指令结果还没产生 解决方案： 采用转发(Forwarding/Bypassing)技术 load use 冒险需要一次阻塞(stall) 编译程序优化指令顺序 Control(Branch) Hazards(changes in program flow) 现象：转移或异常改变执行流程，顺序执行指令在目标地址产生前已被取出 解决方案： 采用静态或动态分支预测 编译程序优化指令顺序(实现分支延迟) 1.18 数据冒险(Data Hazards)解决方法： 硬件阻塞(stall) 软件插入“NOP”指令 合理实现寄存器堆的读/写操作 前半时钟周期写，后半时钟周期读。若在同一个时钟内前面指令写入的数据正好是后面指令所读数据，则不会发生数据冒险 转发技术(Forwarding或Bypassing旁路) load use 问题(需要一次阻塞stall) 编译优化：调整指令顺序 1.19 Load指令的5个阶段(MIPS结构为例) 第一阶段 第二阶段 第三阶段 第四阶段 第五阶段 Ifetch Reg/Dec Exec Mem Wr Ifetch(取指)：从指令存储器取指令并计算PC+4 指令存储器、Addr Reg/Dec(取数和译码)：寄存器取数，同时对指令进行译码 寄存器堆读口、指令译码器 Exec(执行)：计算内存单元地址 扩展器、ALU Mem(读存储器)：从数据存储器中 数据存储器 Wr(写寄存器)：将数据写到寄存器中 寄存器堆写口 Tips:这里寄存器读口和写口可看成两个不同的部件 1.20 动态分支预测 一位预测状态图 二位预测状态图 比较： 一位预测位：当连续两次的分支情况发生改变时，预测错误 二位预测位：在连续两次分支发生不同时，只会有一次预测错误 一般采用二位预测位！ 2 后记 2018年12月3日 战场，考场聒噪，沉静忐忑，镇定顾盼，专心窃喜，自信得意忘形，波澜不惊举世皆浊我独清众人皆醉我独醒 ——YvanZh","categories":[{"name":"Courses","slug":"Courses","permalink":"http://yvanzh.top/categories/Courses/"}],"tags":[{"name":"Advanced Computer Architecture","slug":"Advanced-Computer-Architecture","permalink":"http://yvanzh.top/tags/Advanced-Computer-Architecture/"}]},{"title":"聊聊图床","slug":"Gallery","date":"2018-12-02T03:11:11.000Z","updated":"2020-08-29T02:28:17.108Z","comments":true,"path":"2018/12/02/Gallery/","link":"","permalink":"http://yvanzh.top/2018/12/02/Gallery/","excerpt":"入坑微博图床，七牛云拜拜您嘞！","text":"入坑微博图床，七牛云拜拜您嘞！ 正文笔者刚开始写博客时使用的时Hexo自带的七牛云插件。只需在本地文件夹中放入图片，然后部署时同步到七牛云的测试域名中，即可在文章中插图，着实方便省心。然而却没有注意的是七牛云的测试域名只有三十天寿命，时间一到就会被收回，所以昨天博客的图片就全军覆没了。还好笔者还小心地保留着本地图片，不然真的是陪了夫人又折兵。 感叹当时没有做好功课，看到七牛云被多数推崇就傻傻地开始使用。后悔之余还是需要处理烂摊子的，于是笔者开始折腾起来： 尝试将测试域名改为个人的博客网站域名。但是在七牛云绑定自己的域名需要有备案，而备案则意味着需要购买运营商的服务器，这对于笔者这种在GitHub和Coding上摸爬滚打的穷学生显然是一条绝路。 遍寻互联网终于找到了笔者想要的菜——微博图床：支持http和https，全网加速，不限流量。这里推荐一款国人制作的Chrome插件，直接解决了笔者的燃眉之急。 好在笔者的图片不多，亡羊补牢为时未晚。 吸取教训：学习知识与掌握工具应寻根探底，知其所以然，切忌人云亦云，切勿急功近利。保持怀疑，保持好奇。 我不能创造的东西，我就不了解。 ——理查德·菲利普·费曼","categories":[{"name":"Gallery","slug":"Gallery","permalink":"http://yvanzh.top/categories/Gallery/"}],"tags":[{"name":"Gallery","slug":"Gallery","permalink":"http://yvanzh.top/tags/Gallery/"}]},{"title":"Jupyter Lab 手册","slug":"JupyterLab-Handbook","date":"2018-11-24T13:11:11.000Z","updated":"2020-08-29T02:28:17.118Z","comments":true,"path":"2018/11/24/JupyterLab-Handbook/","link":"","permalink":"http://yvanzh.top/2018/11/24/JupyterLab-Handbook/","excerpt":"Lab 快点成熟起来吧，扩展和主题让人绝望！","text":"Lab 快点成熟起来吧，扩展和主题让人绝望！ 1 添加Kernel使用Anaconda的用户在创建好新环境之后会遇到一个问题——Anaconda自带的Jupyter Notebook或Jupyter Lab并没有把新环境添加到Kernel中去。当然，可以在每个新建的环境中都安装Jupyter，可行但是不优雅。那么解决方法如下： 在Anacodna Prompt中激活创建的环境py36: conda activate py36 安装插件ipykernel： pip install ipykernel 添加内核： python -m ipykernel install --name py36 --display-name &quot;py36&quot; --name参数后的py36是内核名称，用于系统保存。而--display-name参数后的&quot;Py36&quot;是在Jupyter Notebook网页中选择或切换内核时所显示的。建议二者设置为一样的，方便以后删除Kernel。 如果非要设置一个--display-name，然而一段时间后又忘了当时--name是什么了。那么查看所有Kernel,可在CMD中输入: jupyter kernelspec list 删除内核时，可在CMD中输入： jupyter kernelspec remove py36 2 修改Home路径在C:\\Users\\usersname\\.jupyter文件夹中打开jupyter_notebook_config.py文件，找到字段： # The directory to use for notebooks and kernels. #c.NotebookApp.notebook_dir = &#39;&#39; 删除#，并在单引号中添加所需修改的路径，譬如： # The directory to use for notebooks and kernels. c.NotebookApp.notebook_dir = &#39;E:\\jupyter workspace&#39; 3 快捷键3.1 命令模式 (按Esc键)Enter : 转入编辑模式Shift-Enter : 运行本单元，选中下个单元Ctrl-Enter : 运行本单元Alt-Enter : 运行本单元，在其下插入新单元Y : 单元转入代码状态M :单元转入markdown状态R : 单元转入raw状态1 : 设定 1 级标题2 : 设定 2 级标题3 : 设定 3 级标题4 : 设定 4 级标题5 : 设定 5 级标题6 : 设定 6 级标题Up : 选中上方单元K : 选中上方单元Down : 选中下方单元J : 选中下方单元Shift-K : 扩大选中上方单元Shift-J : 扩大选中下方单元A : 在上方插入新单元B : 在下方插入新单元X : 剪切选中的单元C : 复制选中的单元Shift-V : 粘贴到上方单元V : 粘贴到下方单元Z : 恢复删除的最后一个单元D,D : 删除选中的单元Shift-M : 合并选中的单元Ctrl-S : 文件存盘S : 文件存盘L : 转换行号O : 转换输出Shift-O : 转换输出滚动Esc : 关闭页面Q : 关闭页面H : 显示快捷键帮助I,I : 中断Notebook内核0,0 : 重启Notebook内核Shift : 忽略Shift-Space : 向上滚动Space : 向下滚动 3.2 编辑模式 (按Enter键)Tab : 代码补全或缩进Shift-Tab : 提示Ctrl-] : 缩进Ctrl-[ : 解除缩进Ctrl-A : 全选Ctrl-Z : 复原Ctrl-Shift-Z : 再做Ctrl-Y : 再做Ctrl-Home : 跳到单元开头Ctrl-Up : 跳到单元开头Ctrl-End : 跳到单元末尾Ctrl-Down : 跳到单元末尾Ctrl-Left : 跳到左边一个字首Ctrl-Right : 跳到右边一个字首Ctrl-Backspace : 删除前面一个字Ctrl-Delete : 删除后面一个字Esc : 进入命令模式Ctrl-M : 进入命令模式Shift-Enter : 运行本单元，选中下一单元Ctrl-Enter : 运行本单元Alt-Enter : 运行本单元，在下面插入一单元Ctrl-Shift— : 分割单元Ctrl-Shift-Subtract : 分割单元Ctrl-S : 文件存盘Shift : 忽略Up : 光标上移或转入上一单元Down :光标下移或转入下一单元","categories":[{"name":"Jupyter Lab","slug":"Jupyter-Lab","permalink":"http://yvanzh.top/categories/Jupyter-Lab/"}],"tags":[{"name":"Jupyter Lab","slug":"Jupyter-Lab","permalink":"http://yvanzh.top/tags/Jupyter-Lab/"}]},{"title":"Heterogeneous Domain Adaptation Through Progressive Alignment","slug":"Paper-Notes-2","date":"2018-11-16T03:01:33.000Z","updated":"2020-08-29T02:28:17.119Z","comments":true,"path":"2018/11/16/Paper-Notes-2/","link":"","permalink":"http://yvanzh.top/2018/11/16/Paper-Notes-2/","excerpt":"","text":"1 简介文章来源：2018-TNNLS Tips： 异构领域适应：假设源域和目标域样本分别采样于不同空间，其维度也可能不同。 同构领域适应：假设源域和目标域都采样于一个相同空间。 本文考虑异构领域适应的五方面： 特征维度差异 减少特征失配 减小分布差异 数据局部一致 主旨： 将不同维度的样本映射到同一空间中，通过学得一个共同的字典得到可迁移的特征空间，在这个空间中逐步减小两个领域特征的分布差异。 2 详情总体目标： \\underset{\\bf{B,S}}{\\text{min}} \\quad \\underbrace{\\underbrace{\\mathcal{C_{1}}(\\bf{X_{s},X_{t},B,S})}_{\\text{feature alignment}} + \\underbrace{\\alpha \\mathcal{C_{2}}(\\bf{S_{s},S_{t}})}_{\\text{distribution alignment}}}_{\\text{progressive alignment}} + \\underbrace{\\beta \\Omega(S)}_{\\text{constraint}} \\tag{1} 维度差异和特征失配问题： 将领域特征分别经$\\bf P_{s}$、$\\bf P_{t}$投影到同一维度，并通过共享字典$\\bf B$来学的一个新的特征空间。 \\underset{ {\\bf{b}}_{s},{\\bf{s}}_{s}}{\\text{min}} \\quad \\sum_{i=1}^{n_{s}}{ \\Vert{ {\\bf{x}}_{s,i} - \\sum_{j=1}^{k}{\\bf{b}}_{s,j}{\\bf{s}}_{s,j}^{j}}\\Vert_{2}^{2} +\\beta \\sum_{i=1}^{n_{s}}{\\Vert{ {\\bf{s}}_{s,i}}}\\Vert_{1} } \\\\ \\text{s.t.} \\Vert{ {\\bf{b}}_{s,j}}\\Vert^{2} \\le c,\\quad \\forall j \\tag{2} \\underset{\\bf P,B,S_{s},S_{t}}{\\text{min}} \\quad \\Vert{\\bf{P_{s}{X}_{s}}- BS_{s}}\\Vert_{F}^{2} +\\Vert{\\bf{P_{t}{X}_{t}}- BS_{t}}\\Vert_{F}^{2} + \\beta \\sum_{i=1}^{n}{\\Vert{ {\\bf{s}}_{i}}}\\Vert_{1} \\\\ \\text{s.t.} \\quad \\Vert{ {\\bf{b}}_{j}}\\Vert^{2} \\le c,\\quad \\forall j \\tag{3} 然而对于异构领域适应来说，投影降维的信息损失是十分致命的。通过$\\bf PXHX^{\\top}P^{\\top} = \\bf{I}$使投影后的交叉领域数据保留协方差，即特征维度之间保持线性无关。并添加约束$\\Vert{\\bf{P}}\\Vert_{F}^{2}$防止过拟合。 \\underset{\\bf P,B,S}{\\text{min}} \\quad \\Vert{\\bf{P{X}}- BS}\\Vert_{F}^{2} + \\beta \\sum_{i=1}^{n}{\\Vert{ {\\bf{s}}_{i}}}\\Vert_{1} + \\gamma \\Vert{ {\\bf{P}}}\\Vert_{F}^{2} \\\\ \\text{s.t.} \\quad {\\bf PXHX^{\\top}P^{\\top}} = {\\bf{I}} , \\,\\, \\Vert{ {\\bf{b}}_{j}}\\Vert^{2} \\le c,\\quad \\forall j \\tag{4} 分布差异问题： 采用MMD来度量领域特征分布差异，可直接用学到的新特征$\\bf S$代替经过映射$\\phi(\\cdot)$后的特征。 \\underset{ {\\bf S}_{s},{\\bf S}_{t}}{\\text{min}} \\quad \\left| \\left| \\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}{ {\\bf S}_{s,j}-\\frac{1}{n_t}\\sum_{i=1}^{n_{t}} { {\\bf S}_{t,j}}} \\right| \\right| = \\underset{\\bf S}{\\text{min}} \\quad \\text{tr}({\\bf{SMS}^{\\top}}) \\tag{5} 局部一致问题： 新特征空间中一个样本的标签应该与其$k$近邻趋向一致。用$\\bf W$表示邻接矩阵，注意其保留的是原始特征空间的数据关系。 \\frac{1}{2}\\sum_{j,l=1}^{n}\\Vert{ {\\bf s}_{j}-{\\bf s}_{l}} \\Vert_{2}^{2} {\\bf W}_{jl} \\\\ \\tag{6} {\\bf W}_{ij}= \\begin{cases} \\text{cosine}({\\bf x}_{i},{\\bf x}_{j}), \\,\\,\\text{if}\\,\\, {\\bf x}_{i}\\in \\mathcal{N}_{k}({\\bf x}_{j}) \\\\ 0,\\qquad \\qquad \\quad \\,\\,\\,\\, \\text{otherwise} \\end{cases} 展开后可用拉普拉斯矩阵表示： \\sum_{j=1}^{n}{ {\\bf s}_{j}{\\bf s}_{j}^{\\top}{\\bf D}_{jj}} - \\sum_{j,l=1}^{n} { {\\bf s}_{j}{\\bf s}_{l}^{\\top}{\\bf W}_{jl}} = \\text{tr}({\\bf SLS}^\\top) \\tag{7} 最终目标函数： \\underset{\\bf P,B,S}{\\text{min}} \\Vert{ {\\bf PX-BS}}\\Vert_{F}^{2} + \\text{tr}({\\bf S}(\\alpha_{1} {\\bf M} + \\alpha_{2} {\\bf L)}{\\bf S}^{\\top}) + \\beta \\sum _{i=1}^{n}{\\Vert{ {\\bf s}_{i}\\Vert}_{1}} + \\gamma \\Vert { {\\bf P}}\\Vert_{F}^2 \\\\ \\text{s.t.} \\quad {\\bf PXHX^{\\top}P^{\\top}} = {\\bf{I}} , \\,\\, \\Vert{ {\\bf{b}}_{j}}\\Vert^{2} \\le c,\\quad \\forall j \\tag{8} 3 思考 字典学习的特征太浅显？ 拉普拉斯矩阵那些事： 三种形式 Simple Laplacian：$L=D-W$ Symmetric normalized Laplacian：${\\displaystyle L^{\\text{sym}}:=D^{-{\\frac {1}{2}}}LD^{-{\\frac {1}{2}}}=I-D^{-{\\frac {1}{2}}}WD^{-{\\frac {1}{2}}}}$ Random walk normalized Laplacian：${\\displaystyle L^{\\text{rw}}:=D^{-1}L=I-D^{-1}A}$ 邻接矩阵 第一步找近邻：一般采用K邻近法，找出同一类的K个样本。又分两种是否保留相似度的方法：一种是只需要邻近就保留，另一种是两者互为邻近才保留。一般采用前者。 第二步求相似度： 二值：邻近即为1，非邻近即为0。过于单一。 余弦相似度：$cosine(\\mathbf x,\\mathbf x’)$。一般用这个。 径向基(高斯)核函数： $K({\\mathbf {x}},{\\mathbf {x’}})=\\exp \\left(-{\\frac {||{\\mathbf {x}}-{\\mathbf {x’}}||_{2}^{2}}{2\\sigma ^{2}}}\\right)$。计算复杂且引入了新参数，一般不采用。 瑞利熵？ 度量学习？","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"http://yvanzh.top/tags/Transfer-Learning/"},{"name":"Heterogeneous Domain Adaptation","slug":"Heterogeneous-Domain-Adaptation","permalink":"http://yvanzh.top/tags/Heterogeneous-Domain-Adaptation/"}]},{"title":"Deep Transfer Low-Rank Coding for Cross-Domain Learning","slug":"Paper-Notes-1","date":"2018-11-14T10:01:33.000Z","updated":"2020-08-29T02:28:17.119Z","comments":true,"path":"2018/11/14/Paper-Notes-1/","link":"","permalink":"http://yvanzh.top/2018/11/14/Paper-Notes-1/","excerpt":"","text":"1 简介文章来源：2018-TNNLS 寻找领域不变特征一般分为三类： 子空间学习 非线性投影 [X] 字典学习 稀疏约束 [X] 低秩约束 局部约束 为了实现两个领域的迁移需要减小其分布差异： [X] 边缘分布$P(X_{s})$与$P(X_{t})$ [X] 条件分布$P(Y_{s}|X_{s})$与$P(Y_{t}|X_{t})$ 2 详情 从稀疏编码谈起： \\underset{D,Z}{\\text{min}} \\quad \\lVert {X-DZ}\\rVert_{F}^{2} + \\mathcal{N}(Z) \\\\ \\text{s.t.} \\,\\, \\lVert{ {d}_{i}^j}\\rVert_{2}^2 \\le 1 \\quad \\forall i,j \\tag{1} 减小边缘分布差异： 由于光照，角度等问题。直接通过字典学习得到新特征太浅显。$\\Rightarrow$使用CNN提取抽象特征（参数共享）。 对于存在较大分布差异情况，学习的一个字典之后新特征空间中$P(Z_{s})$与$P(Z_{t})$的差异依然大。$\\Rightarrow$通过task-specific全连接层学习多层字典来覆盖两个领域共同特征，逐步减小分布差异。 task-specific全连接层的神经元个数需要根据秩固定。 $\\Rightarrow$通过对$Z$的低秩约束来获取有识别力的新特征。（还能防止过拟合） 只需约束最后一层的$Z_{k}$就能使中间层都能低秩学习： \\left. \\begin{aligned} Z_{k-1} = D_{k}Z_{k} \\Rightarrow \\text{rank}(Z_{k-1}) = \\text{rank}(D_{k}Z_{k}) \\\\ \\text{rank}(D_{k}Z_{k}) \\le \\text{min}(\\text{rank}(Z_{k}),\\text{rank}(D_{k})) \\end{aligned} \\right\\} \\Rightarrow \\text{rank}(Z_{k-1}) \\le \\text{rank}(Z_{k}) \\\\ \\Downarrow \\\\ \\text{rank}(Z_{1}) \\le \\text{rank}(Z_{2}) \\le \\cdots \\text{rank}(Z_{k}) \\tag{2} $\\Rightarrow$改进结果：\\underset{D_{1}\\dots,D_{2},Z_{k}}{\\text{min}} \\quad \\lVert{X-D_{1}D_{2}\\dots D_{k}Z_{k}}\\rVert_{F}^{2} + \\text{rank}(Z_{k}) \\\\ s.t. \\,\\, \\lVert{ {d}_{i}^j}\\rVert_{2}^2 \\le 1 \\quad \\forall i,j \\tag{3} 减小条件分布差异： 半监督知识适应($Z_{k}=[Z_{k}^s,Z_{k}^t]$)： 从传统的MMD谈起： \\begin{aligned} \\mathcal{M}(Z_{k}) &= \\left| \\left| {\\frac{1}{m_{s}} \\sum_{i=1}^{m_{s}} {z_{k,i}} - \\frac{1}{m_{t}} \\sum_{j=m_{s}+1}^{m}{z_{k,j}} } \\right| \\right| _{2}^2 \\\\ &= \\sum_{i=1}^m \\sum_{j=1}^m {z_{k,i}^\\top z_{k,j}W_{ij}=\\text{tr}(Z_{k}WZ_{k}^\\top)} \\end{aligned} \\tag{4} 改进： 传统的MMD只能减小边缘分布差异$\\Rightarrow$采用类间MMD，可减小条件分布差异。 但目标域几乎无标签可用$\\Rightarrow$对目标域样本添加软标签（样本属于每个类的概率）。 \\mathcal{C}(Z_{k}) = \\sum_{c=1}^C \\left| \\left|{\\frac{1}{m_{s}^c} \\sum_{i=1}^{m_{s}^c} {z_{k,i}^{s}} - \\frac{1}{m_{t}^c} \\sum_{j=1}^{m_{t}}{p_{c,j}z_{k,j}^{t}} } \\right| \\right|_{2}^2 = \\sum_{c=1}^C \\text{tr}(Z_{k}W^{(c)}Z_{k}^\\top) \\tag{5} “end-to-end”： 添加softmax层，计算交叉熵损失： \\mathcal{J}(Z_{K},\\Theta,Y) = -\\frac{1}{m}\\sum _{i=1}^m \\sum_{c=1}^C y_{c,i}\\text{log}\\frac{e^{\\theta_{c}^\\top}z_{k,i}}{\\sum_{u=1}^C e^{\\theta_{u}^\\top}z_{k,i}} \\tag{6} 非线性化： 非线性的数据表示可有效减少统计和感知冗余，同时可提高神经网络训练速度，一举两得。使用ReLU这个非线性的激活函数： Z_{i} \\approx f(D_{i+1}Z_{i+1}) \\tag{7} 低秩约束： 希望同类样本的被一个基底张成。那么$Z_{k}$的真实秩即类的总数$C$。将$Z_{k}\\approx AB$作为低秩约束，其中$A\\in \\mathbb{R}^{d_{k}\\times C}$，$B\\in \\mathbb{R}^{C \\times n}$。 最终目标函数: \\begin{aligned} \\mathcal{L} = &\\mathcal{L}(Z_{k},\\Theta,Y) + \\lambda\\Vert{X-D_{1}f(D_{2}f(\\cdots f(D_{k}Z_{k}) ))}\\Vert_{F}^2 \\\\ &+\\alpha\\sum_{c=0}^C\\text{tr}(Z_{k}W^{(c)}Z_{k}^{\\top})+\\beta\\Vert{Z_{k}-AB}\\Vert_{F}^2 \\end{aligned} \\tag{8} image 3 思考 深度学习与迁移学习结合起来，充分发挥各类神经网络的优势。CNN提取抽象特征，NN非线性拟合，GAN拟合分布，RNN序列模型，等等。 是否还需考虑保持特征空间的局部结构或谱结构，使用图拉普拉斯？ 是否可以引入流形学习的方法，假设两个领域的数据是采样于一个高维流形，每类数据分布紧凑。那么学习一个特征映射并通过它得到一个新的特征空间。 低秩约束是否太强。能否充分利用好软标签即样本标签的概率来挖掘类间的关系。 能否引入文本信息，做更广泛的迁移？ 如果使用BN层会不会效果更好？","categories":[{"name":"Paper Notes","slug":"Paper-Notes","permalink":"http://yvanzh.top/categories/Paper-Notes/"}],"tags":[{"name":"Cross-Domain","slug":"Cross-Domain","permalink":"http://yvanzh.top/tags/Cross-Domain/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"http://yvanzh.top/tags/Transfer-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yvanzh.top/tags/Deep-Learning/"}]},{"title":"机器学习数学基础","slug":"ML-Math","date":"2018-11-08T14:33:33.000Z","updated":"2020-08-29T02:28:17.118Z","comments":true,"path":"2018/11/08/ML-Math/","link":"","permalink":"http://yvanzh.top/2018/11/08/ML-Math/","excerpt":"问渠那得清如许？为有源头活水来。","text":"问渠那得清如许？为有源头活水来。 1 信息论1.1 信息熵(Entropy)信息熵的本质是香农信息量$log\\frac{1}{p}$的期望。 离散： H(p) = E[\\log{\\frac{1}{p}}] = \\sum{p_{i} \\cdot log\\frac{1}{p_{i}}}连续： H(p) = E[\\log{\\frac{1}{p}}] = \\int{p(x) \\cdot log\\frac{1}{p(x)}}dx香农信息量的意义：一个事件出现的概率越低，对其编码的长度越大。（huffman编码的思想。） 熵的意义：最短平均编码长度。（因为我们用真实分布$p_i$来编码，编码方案是最优的。） 1.2 交叉熵(Cross Entropy)交叉熵的本质是对估计分布的编码长度。todo 离散: H(p,q) = E_{p}[-\\log{q}] = \\sum_{i} {p_i \\cdot \\log{\\frac{1}{q_i}}}连续： H(p,q) = E_{p}[-\\log{q}] = \\int{p(x) \\cdot \\log{\\frac{1}{q(x)}} dx}交叉熵的意义：估计平均编码长度。（用一个估计分布$q_i$来编码一个真实分布$p_i$，编码不一定是最优的。） 可证明$H(p,q)\\le H(p)$，当且仅当$p=q$时等号成立。 1.3 相对熵(Relative Entropy)又称KL散度或信息增益，用来衡量来个分布之间的差异，具有不对称性。 D_{KL} = H(p,q) - H(p)相对熵的意义：估计平均编码长度相较于最短平均编码长度的冗余。 2 概率论2.1 最大似然估计(MLE)max \\space p(x_0 \\mid \\theta)2.2 贝叶斯估计(BE)p()2.3 最大后验估计(MAP)max \\space p(\\theta \\mid x_0) = \\frac{p(x_0 \\mid \\theta) \\cdot p(\\theta)} {p(x_0)}因$p(x_0)$，已知。故等价于： max \\space p(x_0 \\mid \\theta) \\cdot p(\\theta)3 矩阵分析3.1 特征值分解3.2 奇异值分解3.4 中心矩阵给定m维数据的n个样本，用$m \\times n$矩阵${X=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots ,\\mathbf{x}_{n}]}$。其表示样本均值为： $$\\overline { {\\mathbf {x} } }={\\frac {1}{n}}\\sum _{ {j=1} }^{n}{\\mathbf {x}}_{j}$$ 其中${\\mathbf {x} _{j}}$表示$X$矩阵的第$j$列。 $n \\times n$的中心矩阵可表示为： C_n = I_n - \\frac{1}{n} \\phi $I_{n}$为单位矩阵， $\\phi$表示全为1的矩阵。 中心矩阵的性质： 对称半正定； 幂等； 奇异； 有n-1个值为1的特征值，有一个值为0的特征值； 当n为1时，是零空间； 是投影矩阵，也就是说，把n维空间投影到n-1维子空间。 中心矩阵的特性： 给定$n$维的列向量$v$，若$u$中每个元素都是列向量$v$的所有元素的均值，有：C_{n}v = v - u 散度矩阵$S$可表示为： {S=\\sum _{j=1}^{n}(\\mathbf {x} _{j}-{\\overline {\\mathbf {x} }})(\\mathbf {x} _{j}-{\\overline {\\mathbf {x} }})^{T}=\\sum _{j=1}^{n}(\\mathbf {x} _{j}-{\\overline {\\mathbf {x} }})\\otimes (\\mathbf {x} _{j}-{\\overline {\\mathbf {x} }})=\\left(\\sum _{j=1}^{n}\\mathbf {x} _{j}\\mathbf {x} _{j}^{T}\\right)-n{\\overline {\\mathbf {x} }}{\\overline {\\mathbf {x} }}^{T}} 或 S= XC_{n}(XC_{n})^{T} = XC_{n}C_{n}X^{T}= XC_{n}X^{T} 协方差矩阵$\\Sigma$可表示为：\\Sigma = \\frac{1}{n-1}S = \\frac{1}{n-1}XC_{n}X^T 3.5 三个矩阵 3.6 拉普拉斯矩阵(Graph Laplacians)一般的$L = D - \\frac{S^T+S}{2}$，其中$D$为对角矩阵，元素为$\\sum(s_{ij}+s_{ji})/2$。 性质：对称，半正定。 对于特征向量$\\bf x$有： {\\bf x^\\top Lx} = \\frac{1}{2}\\sum_{j,l=1}^{n}({ {\\bf x}_{j}-{\\bf x}_{l}} )^{2} {\\bf S}_{jl} \\\\ 对于特征矩阵$\\bf X$有： \\text{tr}({\\bf XLX}^\\top) = \\frac{1}{2}\\sum_{j,l=1}^{n}\\Vert{ {\\bf x}_{j}-{\\bf x}_{l}} \\Vert_{2}^{2} {\\bf S}_{jl} \\\\","categories":[{"name":"Math","slug":"Math","permalink":"http://yvanzh.top/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"http://yvanzh.top/tags/Math/"}]},{"title":"Hexo 之公式渲染","slug":"Hexo-MathJax","date":"2018-11-08T14:33:33.000Z","updated":"2020-08-29T02:28:17.117Z","comments":true,"path":"2018/11/08/Hexo-MathJax/","link":"","permalink":"http://yvanzh.top/2018/11/08/Hexo-MathJax/","excerpt":"不写数学公式跟咸鱼有什么区别。","text":"不写数学公式跟咸鱼有什么区别。 1 公式渲染{{}}或{% %}会导致解析Bug。(尤其在使用数学公式时需注意) 官方给出的解决方案为： {% raw %} {{Balabala}} or {% Balabala %} {% endraw %} 然而这样十分不利于博客的迁移。 解决解决方案： 更换默认的渲染引擎：npm uninstall hexo-renderer-marked --save npm install hexo-renderer-kramed --save 修改\\node_modules\\kramed\\lib\\rules\\inline.js路径中的代码如下：var inline = { //escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_&gt;])/, escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/, autolink: /^&lt;([^ &gt;]+(@|:\\/)[^ &gt;]+)&gt;/, url: noop, html: /^&lt;!--[\\s\\S]*?--&gt;|^&lt;(\\w+(?!:\\/|[^\\w\\s@]*@)\\b)*?(?:&quot;[^&quot;]*&quot;|&#39;[^&#39;]*&#39;|[^&#39;&quot;&gt;])*?&gt;([\\s\\S]*?)?&lt;\\/\\1&gt;|^&lt;(\\w+(?!:\\/|[^\\w\\s@]*@)\\b)(?:&quot;[^&quot;]*&quot;|&#39;[^&#39;]*&#39;|[^&#39;&quot;&gt;])*?&gt;/, link: /^!?\\[(inside)\\]\\(href\\)/, reflink: /^!?\\[(inside)\\]\\s*\\[([^\\]]*)\\]/, nolink: /^!?\\[((?:\\[[^\\]]*\\]|[^\\[\\]])*)\\]/, reffn: /^!?\\[\\^(inside)\\]/, strong: /^__([\\s\\S]+?)__(?!_)|^\\*\\*([\\s\\S]+?)\\*\\*(?!\\*)/, //em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, code: /^(`+)\\s*([\\s\\S]*?[^`])\\s*\\1(?!`)/, br: /^ {2,}\\n(?!\\s*$)/, del: noop, text: /^[\\s\\S]+?(?=[\\\\&lt;!\\[_*`$]| {2,}\\n|$)/, math: /^\\$\\$\\s*([\\s\\S]*?[^\\$])\\s*\\$\\$(?!\\$)/, }; 在使用双花括号{ {和{ \\{时中间添加一个空格即可。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/tags/Hexo/"}]},{"title":"Git 手册","slug":"Git-Handbook","date":"2018-11-04T14:33:33.000Z","updated":"2020-08-29T02:28:17.108Z","comments":true,"path":"2018/11/04/Git-Handbook/","link":"","permalink":"http://yvanzh.top/2018/11/04/Git-Handbook/","excerpt":"穿越过去和未来！","text":"穿越过去和未来！ 1 前言 Git一款强大且应用广泛的分布式版本控制软件。 本手册参考的是莫烦教学和Git-Tutrial。笔者边学习边做笔记加深印象，也方便以后查阅。 2 基本操作 查看提交(commit)记录： 详细：git log。 简洁：git log --oneline。 查看文件状态： 详细：git status。 简洁：git status -s。 添加修改： 对所有文件：add .。 对单个文件：add file_name。 提交修改：git commit -m &quot;comments&quot;。 添加并提交修改：git commit -am &quot;comments&quot;。(仅对已被添加到管理库的文件有效) 查看文件的不同： $ git diff：查看这次还没add(unstaged)的修改部分。 $ git diff --cached：如果已经add了这次修改,文件变成了可提交状态(staged),我们可以在diff中添加参数--cached来查看修改。 $ git diff head：查看add过(staged)和没add(unstaged)的修改。 将本地记录发布：git push。 3 回到从前3.1 针对整个版本库 把这次的修改加入添加到上次的commit中，而不产生新的commit(但版本号会改变)： 不修改comment：git commit --amend -no-edit。 修改comment：git commit --amend -m &quot;commets&quot;。 把经过add后(staged)文件回退到add之前(modified)：git reset file_name。 版本跳跃： 当前版本的第前n个版本： 每个上角标代表回退一个版本：git reset --hard HEAD^。 回退n个版本：git reset --hard HEAD~n。(当n取0时可回退到当前版本最初的unmodified状态) 指定版本号的版本：git reset --hard HEAD 3f33f33。 通过git log获取当前版本及之前的版本号。 通过git reflog获取HEAD的指向记录，包含了所有版本的版本号。 指定HEAD记录的版本：git reset --hard HEAD@{1}。(需先在git reflog中找到所求版本的HEAD记录) 3.2 针对单个文件回退单个文件到指定版本号：git checkout 3f33f33 -- file。 4 分支管理 新建分支： 新建不切换到该分支：git branch branch_name。 新建且切换到该分支：git checkout -b branch_name。 删除指定分支：git branch -d branch_name 查看所有分支：git branch。(*号表示当前所处分支) 切换分支：git checkout branch_name。(HEAD指向该分支) 合并分支： merge： fast-forward：git merge branch_name。 no-fast-forward：git merge --no-ff &quot;comments&quot; branch_name。 rebase:git rebase branch_name。 merge冲突：通过人工修改来解决冲突，之后可使用git commit -am &quot;comments&quot;添加并提交。 rebase冲突：通过人工修改来解决冲突，之后可使用git add file_name和git rebase --continue添加并提交。 临时修改： 暂存修改：git stash。 其他任务：一般新建分支来完成其他任务，然后合并。 恢复暂存： 查看缓存：git stash list。 恢复缓存：git stash pop。","categories":[{"name":"Git","slug":"Git","permalink":"http://yvanzh.top/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yvanzh.top/tags/Git/"}]},{"title":"PyTorch 快速入门","slug":"PyTorch-QuickStart","date":"2018-10-29T04:01:33.000Z","updated":"2020-08-29T02:28:17.121Z","comments":true,"path":"2018/10/29/PyTorch-QuickStart/","link":"","permalink":"http://yvanzh.top/2018/10/29/PyTorch-QuickStart/","excerpt":"一只菜鸟的PyTorch笔记！","text":"一只菜鸟的PyTorch笔记！ 1 前言如今主流的学习框架有两个。其一是Google推出的TensorFlow，其二是由Facebook推出的PyTorch。相较之下后者更容易上手，且其动态图的设计也十分利于了解和调节神经网络结构，这也是其异军突起的原因。 笔者建议使用Jupyter Lab来学习。因其即时交互和支持Markdown等优点，许多Tutorial都是用采用的.ipynb来记录的。初次学习需要对Python有基础的了解。此文参考的是PyTorch官方教程。此外，多多查阅PyTorch官方文档会让你更快地上手。 所依赖的环境如下： Python 3.6.6 PyTorch 0.4.1 TorchVision 0.2.1 2 构建一个神经网络构建一个神经网络的主要步骤为： 定义网络结构。 定义损失函数。 定义优化器。 2.1 网络结构定义网络时，需要继承父类nn.Module，调用父类的初始化函数__init__，并实现它的forward函数，把网络中具有可学习参数的层放在初始化函数__init__中。如果某一层(如ReLU)不具有可学习的参数，则既可以放在初始化函数中，也可以不放，建议是不放在其中，而在forward中使用nn.functional代替。 只要在nn.Module的子类中定义了forward函数，backward函数就会自动被实现(利用autograd)。在forward函数中可使用任何tensor支持的函数，还可以使用if和for循环，print、log等Python语法，写法和标准的Python写法一致。 定义一个简单的卷积神经网络LeNet，如下： import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): # 定义一个子类Net，并继承父类nn.module。 def __init__(self): # 定义初始化方法__init__。 super(Net, self).__init__() # 等价于nn.Module.__init__(self) # 调用父类的初始化方法__init__。 # 卷积层 self.conv1 = nn.Conv2d(1, 6, 5) # 输入通道为1，输出通道为6（即分别与六个滤波器做卷积），滤波器大小为5*5) self.conv2 = nn.Conv2d(6, 16, 5) # conv2d表示输入为二维的卷积层。注意：torch中不支持一维的卷积运算。 # 全连接层/仿射层，y = Wx + b。 self.fc1 = nn.Linear(16*5*5, 120) # 定义fc1（fullconnect）全连接函数1为线性函数：y = Wx + b，并将16*5*5个节点连接到120个节点上。 self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 定义前向传播。 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 输入x经过卷积conv1之后，经过激活函数ReLU，使用2x2的窗口进行最大池化Max_pool2d，然后更新到x。 x = F.max_pool2d(F.relu(self.conv2(x)), 2) # 输入x经过卷积conv2之后，经过激活函数ReLU，使用2*2的窗口进行最大池化Max_pool2d,然后更新到x。 x = x.view(x.size()[0], -1) # view函数将池化后的[in_chanel,6,5,5]四维张量x,变形成根据in_chanel大小的[x.size()[0],-1]二维形式，其中-1表示自适应。总特征数并不改变，为接下来的全连接作准备。 x = F.relu(self.fc1(x)) # 输入x经过全连接fc1后，经过激活函数ReLU更新到x。 x = F.relu(self.fc2(x)) # 输入x经过全连接fc2后，经过激活函数ReLU更新到x。 x = self.fc3(x) # 输入x经过全连接fc3后，更新到x。 return x # 返回值x。 net = Net() # 创建一个Net的实例net。 print(net) &quot;&quot;&quot;Out: Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) &quot;&quot;&quot; 网络的可学习参数通过net.parameters()返回： params = list(net.parameters()) print(len(params)) &quot;&quot;&quot;Out: 10 &quot;&quot;&quot; net.named_parameters可同时返回可学习的参数及名称： for name,parameters in net.named_parameters(): # 可将其看作字典 print(name,&#39;:&#39;,parameters.size()) &quot;&quot;&quot;Out: conv1.weight : torch.Size([6, 1, 5, 5]) conv1.bias : torch.Size([6]) conv2.weight : torch.Size([16, 6, 5, 5]) conv2.bias : torch.Size([16]) fc1.weight : torch.Size([120, 400]) fc1.bias : torch.Size([120]) fc2.weight : torch.Size([84, 120]) fc2.bias : torch.Size([84]) fc3.weight : torch.Size([10, 84]) fc3.bias : torch.Size([10]) &quot;&quot;&quot; 需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用input.unsqueeze(0)将batch_size设为1。例如nn.Conv2d输入必须是4维张量，形如$nSamples \\times nChannels \\times Height \\times Width$。可将nSample设为1，即$1 \\times nChannels \\times Height \\times Width$。 2.2 损失函数输入和输出： input = torch.randn(1, 1, 32, 32) # 输入一个随机生成的样本。 out = net(input) # 输出 input.size(),out.size() #输入和输出的都是Tensor。 &quot;&quot;&quot;Out: (torch.Size([1, 1, 32, 32]), torch.Size([1, 10])) &quot;&quot;&quot; 前向传播，计算损失： output = net(input) target = torch.randn(10) # a dummy target, for example.随机生成一个目标，可理解为label值。 target = target.view(1, -1) # 转换为[1, n]的二维张量，-1表示自适应。 criterion = nn.MSELoss() # 定义均方误差损失函数。 loss = criterion(output, target) loss # lost是个scalar &quot;&quot;&quot;Out: tensor(0.7358, grad_fn=&lt;MseLossBackward&gt;) &quot;&quot;&quot; 反向传播，计算梯度： net.zero_grad() # 把net中所有可学习参数的梯度清零 print(&#39;反向传播之前 conv1.bias的梯度&#39;) print(net.conv1.bias.grad) loss.backward() print(&#39;反向传播之后 conv1.bias的梯度&#39;) print(net.conv1.bias.grad) &quot;&quot;&quot;Out: 反向传播之前 conv1.bias的梯度 None 反向传播之后 conv1.bias的梯度 tensor([-0.0030, -0.0066, -0.0036, 0.0317, 0.0085, 0.0023]) &quot;&quot;&quot; 2.3 优化器在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下： Weight = Weight - LearningRate * Gradient手动实现如下： learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) # inplace 减法 torch.optim中实现了深度学习中绝大多数的优化方法，例如RMSProp、Adam、SGD等，更便于使用，因此大多数时候并不需要手动写上述代码。 调用优化器进行操作如下： import torch.optim as optim #新建一个优化器，指定要调整的参数和学习率 optimizer = optim.SGD(net.parameters(), lr = 0.01) # 在训练过程中 # 先梯度清零(与net.zero_grad()效果一样) optimizer.zero_grad() # 前向传播，计算损失。 output = net(input) loss = criterion(output, target) #反向传播，计算梯度。 loss.backward() #更新参数。 optimizer.step() 就这样这样反复迭代下去，直到收敛，即可得到最终的模型。之后便是测试过程了。 3 小试牛刀：CIFAR-10分类CIFAR-10是一个常用的彩色图片数据集，它有10个类别: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。每张图片都是3×32×32，也即3-通道彩色图片，分辨率为32×32。 通过百度网盘下载到本地会更有效率。 实现对CIFAR-10分类的步骤如下： 数据处理。 构建网络。 训练网络。 测试网络。 3.1 数据处理加载并预处理CIFAR-10数据集，如下： import torch import torchvision as tv import torchvision.transforms as transforms from torchvision.transforms import ToPILImage # 第一次运行程序torchvision会自动下载CIFAR-10数据集。 # 大约163M，需花费一定的时间。 # 如果已经下载有CIFAR-10，可通过root参数指定。 # 定义对数据的预处理方式 transform = transforms.Compose([ transforms.ToTensor(), # 把一个取值范围是[0,255]的PIL.Image或者shape为(H,W,C)的numpy.ndarray，转换成形状为[C,H,W]，取值范围是[0,1.0]的torch.FloadTensor。 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化。Normalize(mean,std)：这里有三个通道，每个通道减去均值后除以标准差。 ]) # 训练集 trainset = tv.datasets.CIFAR10( root = &#39;H:/Data/CIFAR-10/&#39;, train = True, download = True, transform = transform) #################参数说明################# # root : cifar-10-batches-py 的根目录。 # train : True = 训练集； False = 测试集 。 # download : True = 从互联上下载数据，并将其放在root目录下。如果数据集已经下载，什么都不干。 # transform : A function/transform = 将PIL(python image library) image进行转换。 # target_transform : A function/transform = 对target（即标签）进行转换。 ######################################### trainloader = torch.utils.data.DataLoader( #数据加载器。组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。 trainset, batch_size = 4, shuffle = True, num_workers = 2) #################参数说明################# # dataset (Dataset) – 加载数据的数据集。 # batch_size (int, optional) – 每个batch加载多少个样本。(默认: 1) # shuffle (bool, optional) – 设置为True时会在每个epoch（整个训练集被遍历的次数）重新打乱数据。(默认: False) # sampler (Sampler, optional) – 定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。 # num_workers (int, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载。(默认: 0) # collate_fn (callable, optional) – 合并一个示例列表以形成一个Mini-batch。 # pin_memory (bool, optional) – 如果为True，数据加载器将把张量复制到CUDA固定内存中，然后返回它们。 # drop_last (bool, optional) – 如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小。(默认: False) ######################################### # 测试集 testset = tv.datasets.CIFAR10( &#39;H:/Data/CIFAR-10/&#39;, train = False, download = True, transform = transform) testloader = torch.utils.data.DataLoader( testset, batch_size = 4, shuffle = False, num_workers = 2) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;,&#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) # classes是一个元组，对应dataset的标签。 Dataset对象是一个数据集，可以按下标访问，返回形如(data, label)的数据。可尝试输出一张图片及其标签： (data, label) = trainset[100] print(classes[label]) # 打印对应label的标签。 show = ToPILImage() # 可以把Tensor转成PIL.Image，方便可视化 show((data + 1) / 2).resize((100, 100)) # (data + 1) / 2是为了还原被归一化的数据。 Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代。可尝试输出一个batch的图片及其标签： dataiter = iter(trainloader) # 因trainloader的shuffle参数设置为True，所以每次迭代时每个batch的内容被打乱。 images, labels = dataiter.next() # 返回4张图片及标签 print(&#39; &#39;.join(&#39;%11s&#39;%classes[labels[j]] for j in range(4))) show(tv.utils.make_grid((images+1)/2)).resize((400,100)) 3.2 构建网络拷贝前文中构建的LeNet网络，因CIFAR-10是3通道彩图，修改self.conv1第一个参数为3通道即可。 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(x.size()[0], -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() use_cuda = False # 控制全局是否使用GPU。 if use_cuda: net = net.cuda() print(net) # 损失函数和优化器。 from torch import optim criterion = nn.CrossEntropyLoss() # 交叉熵损失函数 optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 随机梯度下降优化器。 ############ 优化tricks之动量法参数momentum ############ # 一般，神经网络在更新权值时，采用公式为： w = w - learning_rate * dw # 引入momentum后，采用公式为： w = momentum * w - learning_rate * dw # 其在平坦的区域收敛会加快。 ###################################################### &quot;&quot;&quot;Out: Net( (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) &quot;&quot;&quot; 3.3 训练网络使用处理好的数据分批训练： torch.set_num_threads(8) # 获得用于并行化CPU操作的OpenMP线程数 for epoch in range(2): #训练两个epoch。即两次遍历训练集。 running_loss = 0.0 for i, data in enumerate(trainloader, 0): # 输入数据 inputs, labels = data # 数据加载器生成的data为一个list，其包含两个元素，第一个是inputs值，第二个是labels值 if use_cuda: # 是否使用GPU。 inputs, labels = inputs.cuda(), labels.cuda() # 梯度清零 optimizer.zero_grad() # forward + backward outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() # 更新参数 optimizer.step() # 打印log信息 # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]，否则返回的是一个tensor running_loss += loss.item() if i % 2000 == 1999: # 每2000个batch打印一下训练状态 print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch+1, i+1, running_loss / 2000)) # 每两千次训练的平均损失。 running_loss = 0.0 print(&#39;Finished Training&#39;) &quot;&quot;&quot;Out: [1, 2000] loss: 2.205 [1, 4000] loss: 1.810 [1, 6000] loss: 1.674 [1, 8000] loss: 1.564 [1, 10000] loss: 1.480 [1, 12000] loss: 1.469 [2, 2000] loss: 1.385 [2, 4000] loss: 1.365 [2, 6000] loss: 1.350 [2, 8000] loss: 1.289 [2, 10000] loss: 1.289 [2, 12000] loss: 1.270 Finished Training &quot;&quot;&quot; 3.4 测试网络首先尝试一个batch中四张图片： dataiter = iter(testloader) images, labels = dataiter.next() # 一个batch返回4张图片 print(&#39;实际的label: &#39; &#39;\\n&#39; , &#39; &#39;.join(&#39;%10s&#39; % classes[labels[j]] for j in range(4))) if use_cuda: # 是否使用GPU。 images, labels = images.cuda(), labels.cuda() show(tv.utils.make_grid((images.to(&#39;cpu&#39;)+1) / 2)).resize((400,100)) # 用.to(&#39;cpu&#39;)将cuda Tensor转换为普通的Tensor来画图 &quot;&quot;&quot;Out: 实际的label: cat ship ship plane &quot;&quot;&quot; 然后查看测试结果： # 计算图片在每个类别上的分数 outputs = net(images) # 得分最高的那个类 _, predicted = torch.max(outputs.data, 1) # torch.max(Tensor, dimension(optional))输出Tensor中指定维度最大值及其索引 print(&#39;预测结果: &#39; &#39;\\n&#39;, &#39; &#39;.join(&#39;%10s&#39;% classes[predicted[j]] for j in range(4))) &quot;&quot;&quot;Out: 预测结果: frog car ship plane &quot;&quot;&quot; 计算全局准确率： correct = 0 # 预测正确的图片数 total = 0 # 总共的图片数 # 由于测试的时候不需要求导，可以暂时关闭autograd，提高速度，节约内存。 with torch.no_grad(): for data in testloader: images, labels = data if use_cuda: # 是否使用GPU。 images, labels = images.cuda(), labels.cuda() outputs = net(images) _, predicted = torch.max(outputs, 1) total += labels.size(0) correct += (predicted == labels).sum() # (predicted == labels)是一个Tensor，为真时值为1，假时值为0。 print(&#39;10000张测试集中的准确率为: %d %%&#39; % (100 * correct / total)) &quot;&quot;&quot;Out: 10000张测试集中的准确率为: 55 % &quot;&quot;&quot; 比随机猜测的准确率百分之十要好，说明算法是有效的。 3.5 GPU加速如使用GPU加速，则所需添加环境： CUDA Toolkit V9.0 cuDNN v7.3 具体可参考笔者的前篇博文。 在上述的代码中，只需设置use_cuda = Ture即可。如自行设置GPU加速,需注意： device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # device命名。如果多个CPU有待补充，TODO。 net.to(device) # 将网络存入显存 inputs = inputs.to(device) # 将训练时的输入存入显存 labels = labels.to(device) # 将标签存入显存 images = images.to(device) # 将测试时的输入存入显存 4 后记恭喜你已经初步掌握了PyTorch，之后便是独自修行的过程了。望诸位早日成为一名合格的“炼丹师”！","categories":[{"name":"PyTorch","slug":"PyTorch","permalink":"http://yvanzh.top/categories/PyTorch/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yvanzh.top/tags/Deep-Learning/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://yvanzh.top/tags/PyTorch/"}]},{"title":"Win10 系统下配置 TensorFlow-GPU 环境","slug":"Win10-TensorFlow-GPU","date":"2018-10-05T13:11:11.000Z","updated":"2020-08-29T02:28:17.121Z","comments":true,"path":"2018/10/05/Win10-TensorFlow-GPU/","link":"","permalink":"http://yvanzh.top/2018/10/05/Win10-TensorFlow-GPU/","excerpt":"让你手中的煤气灶疯狂输出！","text":"让你手中的煤气灶疯狂输出！ 1 前言笔者经过两个月的学习，总算是对机器学习的知识框架以及应用领域有了大体的了解，研读文献也渐入佳境。 “纸上得来终觉浅，绝知此事要躬行。”笔者准备开始用实践检验真理，奈何没有趁手的电脑，之前的Coding计划被多次推延。 今天发现偶然发现古董笔记本的显卡居然支持CUDA。于是一番捣腾，踩过无数坑之后终究还是修成正果。 笔者的笔记本电脑信息如下： 型号：Thinkpad E540 系统：Windows 10 显卡：NVIDIA GeForce 840M 内存：8 GB 安装后的环境如下： Python 3.6.6 CUDA Toolkit v9.0 cuDNN v7.3 TensorFlow 1.11.0 PyTorch 0.4.1 2 准备工作2.1 显卡是否支持CUDA？可浏览CUDA官网查看显卡是否达到所需的算力(Compute Capability)要求。 2.2 Python环境根据TensorFlow官网给出的支持信息选择可支持的Python版本。笔者使用的是Python3.6.6版本。 安装Python的两种方法： 直接下载并安装Python对应版本。 安装Anaconda“全家桶”，创建Python对应版本的虚拟环境。 在Windows系统下笔者推荐后者。原因如下： Anaconda的base环境集成了多种常用库，省去了自行安装的繁琐过程。 conda可以创建多个独立的虚拟环境，并且能管理使用pip安装的库。 尽管如此在安装TensorFlow-GPU库的时候笔者还是推荐使用pip。笔者曾尝试使用conda进行安装，结果出现了错误。毕竟conda只是由社区维护的，没有受到TensorFlow官方团队的关注。而原生的pip相较之下就属于“亲儿子”了。 3 配置流程3.1 安装CUDA Toolkit和cuDNN首先根据TensorFlow官网给出的支持信息查看目前TensorFlow支持的CUDA Toolkit与cuDNN的版本。笔者使用的是CUDA Toolkit v9.0以及cuDNN v7.3版本。 下载CUDA Toolkit安装包。安装时注意： 选择自定义安装模式，并勾选所有选项。因为CUDA Toolkit安装包内含有与之相匹配的显卡驱动程序。 下载对应的cuDNN压缩包。将解压的三个文件夹合并到CUDA Toolkit的根目录的三个同名文件夹下。笔者路径为：C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 3.2 安装TensorFlow-GPU 安装好Anaconda后，创建一个新的虚拟环境。打开Anaconda Prompt并输入： conda create -n py36 python=3.6 其中py36为虚拟环境名，python=3.6为指定的版本。 当系统询问是否安装时输入y。 激活新的虚拟环境。输入： conda activate py36 命令行的(base)变成了(py36)。 在新的虚拟环境中使用pip安装TensorFlow-GPU库。输入： pip install --ignore-installed --upgrade tensorflow-gpu 等待安装完成之后，进行测试。输入python以打开解释器，并输入以下： import tensorflow as tf tf.test.gpu_device_name() 返回结果的最后一行代码如下： physical_device_desc: &quot;device: 0, name: GeForce 840M, pci bus id: 0000:01:00.0, compute capability: 5.0&quot; Tips 使用Anaconda的用户在创建好新环境之后会遇到一个问题——Anaconda自带的Jupyter Notebook或Jupyter Lab并没有把新环境添加到Kernel中去。当然，可以在每个新建的环境中都安装Jupyter，可行但是不优雅。那么解决方法如下： 在Anacodna Prompt中激活创建的环境py36: conda activate py36 安装插件ipykernel： pip install ipykernel 添加内核： python -m ipykernel install --name py36 --display-name &quot;py36&quot; --name参数后的py36是内核名称，用于系统保存。而--display-name参数后的 “Py36” 是在Jupyter Notebook网页中选择或切换内核时所显示的。建议二者设置为一样的，方便以后删除Kernel。 如果非要设置一个--display-name，然而一段时间后又忘了当时--name是什么了。那么查看所有Kernel,可在CMD中输入: jupyter kernelspec list 删除内核时，可在CMD中输入： jupyter kernelspec remove py36 4 后记 2018年10月23日更新！ 笔者最近在又学习使用PyTorch（TensorFlow还没玩好，逃~）。根据已经安装的CUDA Toolkit v9.0,在Pytorch官网可找到对应的pip的安装方法，同前文描述一样，先创建新的虚拟环境，然后安装PyTorch，最后可以添加Kernel。 进行测试： cuda = t.cuda.is_available() print(cuda) 返回结果为True则成功。 最新的PyTorch 1.0预览版已经发布啦，然而Windows系统暂时还不能玩，有点遗憾。笔者安装的是PyTorch 0.4.1版本。","categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yvanzh.top/categories/TensorFlow/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yvanzh.top/tags/Deep-Learning/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yvanzh.top/tags/TensorFlow/"}]},{"title":"使用 Markdown 优雅地写作","slug":"Markdown-Blog","date":"2018-08-19T11:01:33.000Z","updated":"2020-08-29T02:28:17.119Z","comments":true,"path":"2018/08/19/Markdown-Blog/","link":"","permalink":"http://yvanzh.top/2018/08/19/Markdown-Blog/","excerpt":"抛开排版的烦恼，优雅地记录世界！ image","text":"抛开排版的烦恼，优雅地记录世界！ image 1 前言第一次接触Markdown是在考完研之后。那段时间，笔者正在家中学习计算机网络的视频课。因为其知识结构比较繁杂，手写记录起来也比较累，于是笔者想偷个懒，把课件上的知识点梳理好，存放在电脑中，以便不时翻阅温习。 后来成绩公布，笔者落榜，笔记也就不了了之。但是笔者依然对这个大宝贝情有独钟。想来，如今用其来写博客也是一件美妙的事情。 本文主要是把Mardown语法以及在Hexo框架下写作的一些技巧记录下来与诸位分享。 2 Mardown语法2.1 标题标题最低为六级标题，如： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 2.2 段落段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个及以上空格加上回车。 2.2 区块引用只在第一行使用或者在段落的每行符号&gt;，还可使用多个嵌套引用。如： &gt; 区块引用 &gt;&gt; 嵌套引用 效果如下： 区块引用 嵌套引用 2.3 代码区块代码区块表示有两种方式： 在每行加上4个空格或者一个制表符。 在代码前后行分别添加三个`符号。如需要，可在第一行的符号后输入对应的代码类型来实现代码高亮。 2.4 强调在强调内容两侧分别加上*或者_。如： *斜体*，_斜体_ **粗体**，__粗体__ 效果： 斜体，斜体粗体，粗体 2.5 列表2.5.1 无序列表无序列表的标记方式是使用-、+、或·。如： - 第一项 - 第二项 - 第三项 注意：标记后面最少有一个空格。若不在引用区块中，必须和前方段落之间存在空行。 效果如下： 第一项 第二项 第三项 2.5.2 有序列表有序列表的标记方式是将上述的符号换成数字,并辅以.。如： 1. 第一项 2. 第二项 3. 第三项 效果如下： 第一项 第二项 第三项 2.5.3 列表嵌套列表嵌套的方式是在所需嵌套的列表项前添加四个空格或一个制表符。无序列表和有序列表可以相互嵌套。如： - 第一项 1. 嵌套有序第一项 - 嵌套无序第一项 - 嵌套无序第二项 - 嵌套无序第三项 2. 嵌套有序第二项 3. 嵌套有序第三项 - 第二项 - 第三项 效果如下： 第一项 嵌套有序第一项 嵌套无序第一项 嵌套无序第二项 嵌套无序第三项 嵌套有序第二项 嵌套有序第三项 第二项 第三项 2.6 分割线分割线最常使用就是三个或以上-，还可以使用*或_。 2.7 生成链接链接的生成需使用[]()符号。如： [YvanZh的博客](http://yvanzh.top/) 效果如下： YvanZh的博客 2.8 插入图片添加图片的形式和链接相似，使用![]()符号即可。 2.9 反斜杠反斜杠\\相当于反转义作用。使符号成为普通符号。 2.10 标记块标记块使用符号`。如： `ctrl+a` 效果如下： ctrl+a 2.11 表格使用|来分隔表格，使用前需空行。使用|-|分隔表头和表身。如： |姓名|性别|年龄| |---|---|---| |YvanZh|男|11| 效果如下： 姓名 性别 年龄 YvanZh 男 11 更多说明： |、-、:之间的多余空格会被忽略，不影响布局。 默认居左对齐。 -:表示内容和标题栏居右对齐，:-表示内容和标题栏居左对齐，:-:表示内容和标题栏居中对齐。 内容和|之间的多余空格会被忽略，每行第一个|和最后一个|可以省略，-的数量至少有一个。 3 Hexo写作技巧3.1 主页文章摘要 使用&lt;!--more--&gt;，可使得在此之前的内容作为摘要出现在主页中，同时也会出现在文章详情中。 在文章中的front-matter中添加description：，并提供文章摘录。则首页中会显示文章的摘要内容，而文章详情后不会再显示。比如： 2019年4月6日：更换indigo主题后色块不可用。 ## 3.2 `Next`主题自带引用块 {% cq %} 我是引用内容。 {% endcq %} ``` 效果如下： {% cq %} 我是引用的内容。 {% endcq %} ## 3.3 `Next`主题自带色块 主题共自带六种色块，分别为：default、success、danger、primary、info、warning。使用色块的方式为： ``` {% note 色块类型 %} 我是色块的内容。 {% endnote %} ``` 1. default色块 {% note default %} 我是default色块的内容。 {% endnote %} 1. success色块 {% note success %} 我是success色块的内容。 {% endnote %} 1. danger色块 {% note danger %} 我是danger色块的内容。 {% endnote %} 1. primary色块 {% note primary %} 我是primary色块的内容。 {% endnote %} 1. info色块 {% note info %} 我是info色块的内容。 {% endnote %} 1. warning色块 {% note warning %} 我是warning色块的内容。 {% endnote %} ## 3.4 `Next`主题自带Tabs 需在主题配置文件`_config.yml`中，将`Tabs Tag`相关设置改为`true`。 Tabs显示效果类似于选项卡。使用Tabs的方式为： ``` {% tabs 选项卡, 2 %} **这是选项卡 1** 哇，你找到我了！φ(≧ω≦*)♪～ **这是选项卡 2** **这是选项卡 3** 哇，你找到我了！φ(≧ω≦*)♪～ {% endtabs %} ``` 上方的源码中, 数字`2`表示初始显示的是第二个选项卡。非必须，若数值为`-1`则隐藏选项卡内容。 效果如下： {% tabs 选项卡, 2 %} **这是选项卡 1** 哇，你找到我了！φ(≧ω≦*)♪～ **这是选项卡 2** **这是选项卡 3** 哇，你找到我了！φ(≧ω≦*)♪～ {% endtabs %} ```","categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://yvanzh.top/categories/Markdown/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://yvanzh.top/tags/Markdown/"}]},{"title":"通过 GitHub&Conding+Hexo 搭建个人博客","slug":"GitHub-Conding-Hexo","date":"2018-08-15T02:01:23.000Z","updated":"2020-08-29T02:28:17.108Z","comments":true,"path":"2018/08/15/GitHub-Conding-Hexo/","link":"","permalink":"http://yvanzh.top/2018/08/15/GitHub-Conding-Hexo/","excerpt":"打开音乐播放器，系好安全带，开车！","text":"打开音乐播放器，系好安全带，开车！ 1 前言近日正值暑期，小伙伴们都在家里空调西瓜绿豆冰沙，而笔者这个准研究生却提前来到学校啃“西瓜书”。奈何才疏学浅，只得遍寻网络究其根本。期间拜读了各路大牛的优秀博文，惊叹其写作功力深厚之余，不免为其几年如一日的坚持而动容。于是笔者暗自摩拳擦掌，从而有了此间小筑与此篇小文。 在本文中笔者将会简要地复现搭建此博客的过程。既可方便来日再造轮子，又能给有缘人提供些许指引。 2 博客搭建之初步笔者使用的是Window 10操作系统，所以Linux/Mac OS用户还请酌情参考。 主要步骤： Git Node.js Hexo 推送网站 绑定域名 2.1 Git 一款强大且应用广泛的分布式版本控制软件。 2.1.1 安装与验证下载并安装Git，完成后可在CMD中输入git version进行验证。 2.1.2 账号绑定将Git与Github账号进行绑定。 完善用户信息。在鼠标右键的菜单栏中选择Git Bash here，输入： git config --global user.name &quot;GitHub的用户名&quot; git config --global user.email &quot;GitHub的注册邮箱&quot; 配置SSH密钥。继续输入： ssh-keygen -t rsa -C &quot;GitHub的注册邮箱&quot; 然后按三次回车键，即不设置密码，并生成SSH密钥文件。 之后在C:\\Users\\UserName\\.ssh文件夹中找到生成的文件id_rsa.pub，复制其中的内容。 最后在进入GitHub_setting_key中点击New SSH key，粘贴至Key中，完成添加。 验证绑定结果。继续输入： ssh git@github.com 提示是否通过SSH密钥匹配登陆。输入： yes 提示登录成功。 如需继续绑定Coding账户，可直接将之前生成的公钥添加到Coding账号中。验证绑定结果，输入： ssh -T git@git.coding.net 如需更多了解，可查阅Git官方文档。 2.2 Node.js 一个能够在服务器端运行JavaScript的，开源且跨平台的JavaScript运行环境。 下载并安装Node.js，完成后可在CMD中输入npm -v进行验证。 如需更多了解，可查阅Node.js官方文档。 2.3 Hexo Hexo是一个快速、简洁且高效的博客框架。Hexo使用Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装步骤： 在电脑中创建一个文件夹Hexo。Hexo框架以及之后发布的网页文件都将保存在其中。 按住shift键，鼠标右击文件夹Hexo，点击菜单栏中的在此处打开Powershell窗口。 安装Hexo。在Powershell窗口输入： npm install -g hexo-cli 初始化博客。继续输入： hexo init blog 构建网站雏形。继续输入： cd blog hexo g 开启本地服务器。继续输入： hexo s 此时在浏览器中访问localhost:4000，即可预览博客网站。通过Ctrl+C关闭本地预览。 安装Git部署插件。继续输入： npm install hexo-deployer-git --save 如需更多了解，可查阅Hexo官方文档。 2.4 推送网站 分别在GitHub和Coding中新建仓库YvanZh.github.io和YvanZh.coding.me。（GitHub的仓库名须为xxx.github.io，Coding的仓库名请随意发挥。） 在安装好Hexo框架的根目录blog下，打开站点配置文件_config.yml。 配置部署位置。于最后几段代码中找到deploy，对其进行修改： deploy: type: git repo: github: git@github.com:YvanZh/YvanZh.github.io.git coding: git@git.coding.net:YvanZh/YvanZh.coding.me.git branch: master 此处两个大坑需要特别留意： 冒号后面必须添加一个空格。 仓库名后面必须添加.git。 清除缓存文件，生成静态文件并部署到网站。继续输入： hexo clean hexo g -d 至此，可以在浏览器中输入仓库路径YvanZh.github.io或YvanZh.coding.me来在线访问的博客了！ 2.5 绑定域名 购买域名。笔者是在aliyun上购买.top域名，首年5元。 配置域名解析。此时GitHub&amp;Coding双重配置的好处就体现了：我们可以将境外的访问解析到YvanZh.github.io，将国内的访问解析到YvanZh.coding.me，从而提高访问速度。 记录类型 主机记录 记录值 解析线路 CNAME @ YvanZh.github.io 境外 CNAME @ YvanZh.coding.me 默认 进入GitHub仓库YvanZh.github.io的Setting页面，修改Custom domain项为所购买的域名Yvanzh.top。 进入Coding仓库YvanZh.coding.me的页面： 点击侧边栏代码，出现下拉菜单。 点击下拉菜单中的Pages服务，出现对应的主页面。 点击主页面中出现的自动部署Pages服务，等待部署完毕。 填入所购买的域名YvanZh.top并点击绑定新域名。 打开文件资源管理器: 在blog\\source目录下新建一个CNAME.txt文件。 写入YvanZh.top，保存并退出。 删除其文件扩展名.txt。 PS：配置域名解析以及绑定域名后，均需一段时间方能生效。所以短时间无法访问网站是正常的情况，还请耐心等待。可以尝试在CMD输入： ping yvanzh.top ping yvanzh.github.io ping yvanzh.coding.me 从而判断问题出在域名解析、GitHubPage部署、CodingPage部署三者中的何处。 2018年12月9日更新：开启GitHub Page与Coding Page的强制HTPPS服务，小绿锁成就达成~ 3 博客搭建之个性化初步的搭建的任务圆满完成，后续就是进行添砖加瓦的个性化设置了。 Tips： 在进行个性化配置之前，先备份一下blog文件夹，以防万一。 耐心与细心。因为有些设置比较繁杂，一次不成功还请多尝试，不要轻易放弃。 速度与美观。不要过于追求酷炫而使网页变得笨重，更不要忘记搭建博客的初衷。 强烈安利VS Code，果然开源就是力量。源代码管理功能可以使得修改代码事半功倍。 每配置好一小处，可以来一套‘素质三连’查看效果： hexo cl # 清除缓存文件。 hexo g -d # 生成静态网页文件并部署。 hexo s # 开启本地服务器。浏览器访问localhost：4000即可进行预览；Ctrl+C即可关闭预览。 笔者使用的是NeXT主题，可参考Hexo+NeXT主题优化一文。 4 博客迁移 傻瓜式： 在所要迁移的设备中安装Git以及Node.js并设置好环境变量。将原Hexo根目录文件夹复制到新设备中，并Git bash here： npm install -g hexo-cli npm install hexo-deployer-git --save 小机灵大法：Hexo 之博客备份 5 结语笔者从无到有，历时五日终尝胜果。期间踩坑无数，更添脖颈酸痛。水平有限，万望海涵，如有纰漏，敬请斧正。 愿不忘初心，坚持写作，与诸位共勉！ 6 后记研究生三年，说长不长，说短不短。但每当想到计算机领域的爆炸发展，以及走在前列的那些高平台、高起点的竞争对手们，唯有努力提升自我价值！ 定个目标：周内看文献写笔记，周末写博客做总结。把学习与生活都记录下来。不奢独上高楼，但求不会秃头。 挖个小坑：今年七夕一个人过无疑了，明年总该有小姐姐陪了吧。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yvanzh.top/tags/Hexo/"}]}]}